{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies\n",
    "\n",
    "Install dependencies not available on Google Collab.\n",
    "Collab provides numpy, pandas, sklearn, tensorflow, scipy, etc. (see requirements.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pinard\n",
    "!pip install scikeras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark details\n",
    "\n",
    "The results aggregate the combination of the following trainings configurations:\n",
    "- estimation configuration: [regression, classification]\n",
    "- datasets configurations: [Single Train, Cross validation with 5 folds and 2 repeats, Augmented Single Train]\n",
    "- preprocessing configuration: [flat spectrum, savgol, haar, [small set], [big_set]]\n",
    "- models: \n",
    "   - for all configuration: BACON, BACON-VG, DECON, PLS(components from 1 to 100), XGBoost, LW-PLS\n",
    "   - for single train + small_set : Stack > [ BACON, BACON-VG, DECON, PLS(components from 1 to 100), XGBoost, LW-PLS,\n",
    "   f_PLSRegression,f_AdaBoostRegressor,f_BaggingRegressor,f_ExtraTreesRegressor, f_GradientBoostingRegressor,f_RandomForestRegressor,\n",
    "   f_ARDRegression,f_BayesianRidge,f_ElasticNet,f_ElasticNetCV,f_HuberRegressor, f_LarsCV,f_LassoCV,f_Lasso,f_LassoLars,f_LassoLarsCV,\n",
    "   f_LassoLarsIC,f_LinearRegression,f_OrthogonalMatchingPursuit,f_OrthogonalMatchingPursuitCV, f_PassiveAggressiveRegressor,f_RANSACRegressor,\n",
    "   f_Ridge,f_RidgeCV,f_SGDRegressor,f_TheilSenRegressor,f_GaussianProcessRegressor,f_KNeighborsRegressor, f_Pipeline,f_MLPRegressor,f_LinearSVR,\n",
    "   f_NuSVR,f_SVR,f_DecisionTreeRegressor,f_ExtraTreeRegressor,f_KernelRidge,f_XGBRegressor]\n",
    "\n",
    "We perform training in 2 steps, (1) data transformation and (2) training because the sklearn pipeline does not use test data natively.\n",
    "To change with pinard update in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from contextlib import redirect_stdout\n",
    "# import joblib\n",
    "# import pickle\n",
    "\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.metrics \\\n",
    "    import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error,\\\n",
    "        r2_score, explained_variance_score, mean_squared_log_error, median_absolute_error\n",
    "from sklearn.pipeline import Pipeline\n",
    "import tensorflow as tf\n",
    "\n",
    "from data import load_data\n",
    "from preprocessings import preprocessing_list, transform_test_data, ml_transformer_pipeline\n",
    "from regressors import nn_list, ml_list, get_keras_model\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "\n",
    "def log_run(dataset_name, model_name, path, estimator, y_valid, y_pred):\n",
    "    # compute scores\n",
    "    RMSE = str(mean_squared_error(y_valid, y_pred, squared=True))\n",
    "    target_RMSE = str(float(os.path.split(path)[-1].split('_')[-1].split(\"RMSE\")[-1]))\n",
    "    print(RMSE, \" vs \", target_RMSE)\n",
    "\n",
    "    datasheet = {\n",
    "        \"model\":model_name, \n",
    "        \"dataset\":dataset_name,\n",
    "        \"target_RMSE\":target_RMSE,\n",
    "        \"RMSE\":RMSE,\n",
    "        \"MAPE\":str(mean_absolute_percentage_error(y_valid, y_pred)),\n",
    "        \"R2\":str(r2_score(y_valid, y_pred)),\n",
    "        \"MAE\":str(mean_absolute_error(y_valid, y_pred)),\n",
    "        \"MSE\":str(mean_squared_error(y_valid, y_pred, squared=False)),\n",
    "        \"MedAE\":str(median_absolute_error(y_valid, y_pred)),\n",
    "        \"EVS\":str(explained_variance_score(y_valid, y_pred)),\n",
    "        \"MSLE\":str(mean_squared_log_error(y_valid, y_pred)),\n",
    "    }\n",
    "\n",
    "    ### Save data\n",
    "    folder = \"results/\" + dataset_name\n",
    "    if not os.path.isdir(folder):\n",
    "        os.makedirs(folder)\n",
    "\n",
    "    canon_name = folder + \"/\" + model_name\n",
    "\n",
    "    ## save predictions\n",
    "    np.savetxt(canon_name + '.csv', np.column_stack((y_valid, y_pred)))\n",
    "\n",
    "    ## save main metrics globally\n",
    "    result_file = open(folder + \"/_runs.txt\", \"a\")\n",
    "    log = RMSE + \"  ---  \" + model_name + ' '*10 + datetime.datetime.now().strftime(\"%Y-%m-%d  %H:%M:%S\") + '\\n'\n",
    "    result_file.write(log)\n",
    "    result_file.close()\n",
    "\n",
    "    ## save pipeline\n",
    "    # joblib.dump(estimator, canon_name + '.pkl')\n",
    "\n",
    "    return datasheet\n",
    "\n",
    "\n",
    "def evaluate_pipeline(dataset_name, model_name, path, pipeline, y_scaler, X_train, y_train, X_valid, y_valid):  \n",
    "    # Fit estimator\n",
    "    estimator = TransformedTargetRegressor(regressor = pipeline, transformer = y_scaler)\n",
    "    estimator.fit(X_train, y_train)  \n",
    "    # Evaluate estimator\n",
    "    y_pred = estimator.predict(X_valid)\n",
    "    print(y_pred[0:10])\n",
    "    print(y_valid[0:10])\n",
    "    return log_run(dataset_name, model_name, path, estimator, y_valid, y_pred)\n",
    "\n",
    "\n",
    "def benchmark_dataset(path, SEED):\n",
    "    results = {}\n",
    "    dataset_name = ('_').join(os.path.split(path)[-1].split('_')[:-1])\n",
    "    print(\"=\"*10, str(dataset_name).upper(), \"=\"*10)\n",
    "\n",
    "\n",
    "    X_train, y_train, X_valid, y_valid = load_data(path)\n",
    "    print(\"Data >\", X_train.shape, y_train.shape, X_valid.shape, y_valid.shape)\n",
    "\n",
    "    ### First becnhmark in a single run\n",
    "    X_test, y_test = X_valid, y_valid\n",
    "    \n",
    "    for preprocessing in preprocessing_list():\n",
    "        # Only necessary for Kerasregressor and LWPLS in order to provide validation data to the model - should be replace with pinard 2.0\n",
    "        X_test_pp, y_test_pp, transformer_pipeline, y_scaler = transform_test_data(preprocessing, X_train, y_train, X_test, y_test)\n",
    "        ##\n",
    "\n",
    "        # ##### DEEP LEARNING #####\n",
    "        # for model_desc in nn_list():\n",
    "        #     model_name = model_desc.__name__ + \"-\" + preprocessing.__name__ + \"-\" + str(SEED)\n",
    "        #     print(\"-\", model_name, end=\" \")\n",
    "\n",
    "        #     # Get model\n",
    "        #     regressor = get_keras_model(dataset_name + '_' + model_name, model_desc, 5, 500, X_test_pp, y_test_pp, verbose=0, seed=SEED)\n",
    "        #     pipeline = Pipeline([\n",
    "        #         ('transformation', transformer_pipeline), \n",
    "        #         (model_name, regressor)\n",
    "        #     ])\n",
    "\n",
    "        #     datasheet = evaluate_pipeline(dataset_name, model_name, path, pipeline, y_scaler, X_train, y_train, X_valid, y_valid)           \n",
    "        #     results[model_name] = datasheet\n",
    "        \n",
    "        ##### MACHINE LEARNING #####\n",
    "        for regressor, mdl_name in ml_list(SEED, X_test_pp, y_test_pp):\n",
    "            model_name = mdl_name + \"-\" + preprocessing.__name__ + \"-\" + str(SEED)\n",
    "            pipeline = Pipeline([\n",
    "                ('transformation', ml_transformer_pipeline(preprocessing)), \n",
    "                (model_name, regressor)\n",
    "            ])\n",
    "\n",
    "            print(\"-\", model_name, end=\" \")\n",
    "            datasheet = evaluate_pipeline(dataset_name, model_name, path, pipeline, y_scaler, X_train, y_train, X_valid, y_valid)\n",
    "            results[model_name] = datasheet\n",
    "        \n",
    "        break\n",
    "\n",
    "    results = sorted(results.items(), key=lambda k_v: k_v[1]['RMSE'])\n",
    "    with open(\"results/\" + dataset_name + '_global_results.json', 'w') as fp:\n",
    "        json.dump(results, fp, indent=4)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== ALPINE_CALPINE_424_MURGUZUR ==========\n",
      "Data > (272, 2151) (272,) (152, 2151) (152,)\n",
      "- LWPLS_2_0.25-id_preprocessing-31441 "
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "y_true and y_pred have different number of output (1!=2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15396/2755572239.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfolder\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfolder_list\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mbenchmark_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfolder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSEED\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15396/2671753759.py\u001b[0m in \u001b[0;36mbenchmark_dataset\u001b[1;34m(path, SEED)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"-\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 120\u001b[1;33m             \u001b[0mdatasheet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluate_pipeline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_scaler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_valid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    121\u001b[0m             \u001b[0mresults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatasheet\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15396/2671753759.py\u001b[0m in \u001b[0;36mevaluate_pipeline\u001b[1;34m(dataset_name, model_name, path, pipeline, y_scaler, X_train, y_train, X_valid, y_valid)\u001b[0m\n\u001b[0;32m     74\u001b[0m     \u001b[1;31m# Evaluate estimator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mlog_run\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15396/2671753759.py\u001b[0m in \u001b[0;36mlog_run\u001b[1;34m(dataset_name, model_name, path, estimator, y_valid, y_pred)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mlog_run\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[1;31m# compute scores\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m     \u001b[0mRMSE\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmean_squared_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_valid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msquared\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m     \u001b[0mtarget_RMSE\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'_'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"RMSE\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mRMSE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\" vs \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_RMSE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Workspace\\ML\\pynirsENV\\lib\\site-packages\\sklearn\\metrics\\_regression.py\u001b[0m in \u001b[0;36mmean_squared_error\u001b[1;34m(y_true, y_pred, sample_weight, multioutput, squared)\u001b[0m\n\u001b[0;32m    422\u001b[0m     \"\"\"\n\u001b[0;32m    423\u001b[0m     y_type, y_true, y_pred, multioutput = _check_reg_targets(\n\u001b[1;32m--> 424\u001b[1;33m         \u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmultioutput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    425\u001b[0m     )\n\u001b[0;32m    426\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Workspace\\ML\\pynirsENV\\lib\\site-packages\\sklearn\\metrics\\_regression.py\u001b[0m in \u001b[0;36m_check_reg_targets\u001b[1;34m(y_true, y_pred, multioutput, dtype)\u001b[0m\n\u001b[0;32m    100\u001b[0m         raise ValueError(\n\u001b[0;32m    101\u001b[0m             \"y_true and y_pred have different number of output ({0}!={1})\".format(\n\u001b[1;32m--> 102\u001b[1;33m                 \u001b[0my_true\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m             )\n\u001b[0;32m    104\u001b[0m         )\n",
      "\u001b[1;31mValueError\u001b[0m: y_true and y_pred have different number of output (1!=2)"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "rootdir = Path('data/regression')\n",
    "folder_list = [f for f in rootdir.glob('**/*') if f.is_dir()]\n",
    "\n",
    "SEED = ord('D') + 31373\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "for folder in folder_list:\n",
    "    benchmark_dataset(folder, SEED)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import cuda \n",
    "device = cuda.get_current_device()\n",
    "device.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ord() expected a character, but string of length 4 found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15396/3746417142.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mord\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"test\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: ord() expected a character, but string of length 4 found"
     ]
    }
   ],
   "source": [
    "ord(\"test\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.9 ('pynirsENV')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7b09f6e5407ec4329146609a0cb08cbbe4720f97bb26598a93c421b663bd10d2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
