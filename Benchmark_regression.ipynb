{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies\n",
    "\n",
    "Install dependencies not available on Google Collab.\n",
    "Collab provides numpy, pandas, sklearn, tensorflow, scipy, etc. (see requirements.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pinard in d:\\workspace\\ml\\pynirsenv\\lib\\site-packages (0.9.5)\n",
      "Requirement already satisfied: pandas in d:\\workspace\\ml\\pynirsenv\\lib\\site-packages (from pinard) (1.3.4)\n",
      "Requirement already satisfied: scipy in d:\\workspace\\ml\\pynirsenv\\lib\\site-packages (from pinard) (1.7.3)\n",
      "Requirement already satisfied: PyWavelets in d:\\workspace\\ml\\pynirsenv\\lib\\site-packages (from pinard) (1.2.0)\n",
      "Requirement already satisfied: scikit-learn in d:\\workspace\\ml\\pynirsenv\\lib\\site-packages (from pinard) (1.0.1)\n",
      "Requirement already satisfied: numpy in d:\\workspace\\ml\\pynirsenv\\lib\\site-packages (from pinard) (1.21.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ip (d:\\workspace\\ml\\pynirsenv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (d:\\workspace\\ml\\pynirsenv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (d:\\workspace\\ml\\pynirsenv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (d:\\workspace\\ml\\pynirsenv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (d:\\workspace\\ml\\pynirsenv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (d:\\workspace\\ml\\pynirsenv\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in d:\\workspace\\ml\\pynirsenv\\lib\\site-packages (from pinard) (2.9.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in d:\\workspace\\ml\\pynirsenv\\lib\\site-packages (from pandas->pinard) (2021.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in d:\\workspace\\ml\\pynirsenv\\lib\\site-packages (from pandas->pinard) (2.8.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in d:\\workspace\\ml\\pynirsenv\\lib\\site-packages (from scikit-learn->pinard) (3.0.0)\n",
      "Requirement already satisfied: joblib>=0.11 in d:\\workspace\\ml\\pynirsenv\\lib\\site-packages (from scikit-learn->pinard) (1.1.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in d:\\workspace\\ml\\pynirsenv\\lib\\site-packages (from tensorflow->pinard) (2.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in d:\\workspace\\ml\\pynirsenv\\lib\\site-packages (from tensorflow->pinard) (4.4.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in d:\\workspace\\ml\\pynirsenv\\lib\\site-packages (from tensorflow->pinard) (1.1.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in d:\\workspace\\ml\\pynirsenv\\lib\\site-packages (from tensorflow->pinard) (3.1.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in d:\\workspace\\ml\\pynirsenv\\lib\\site-packages (from tensorflow->pinard) (1.1.2)\n",
      "Requirement already satisfied: keras<2.10.0,>=2.9.0rc0 in d:\\workspace\\ml\\pynirsenv\\lib\\site-packages (from tensorflow->pinard) (2.9.0)\n",
      "Requirement already satisfied: flatbuffers<2,>=1.12 in d:\\workspace\\ml\\pynirsenv\\lib\\site-packages (from tensorflow->pinard) (1.12)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in d:\\workspace\\ml\\pynirsenv\\lib\\site-packages (from tensorflow->pinard) (1.12.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in d:\\workspace\\ml\\pynirsenv\\lib\\site-packages (from tensorflow->pinard) (1.1.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in d:\\workspace\\ml\\pynirsenv\\lib\\site-packages (from tensorflow->pinard) (3.3.0)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in d:\\workspace\\ml\\pynirsenv\\lib\\site-packages (from tensorflow->pinard) (3.19.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in d:\\workspace\\ml\\pynirsenv\\lib\\site-packages (from tensorflow->pinard) (1.42.0)\n",
      "Requirement already satisfied: tensorboard<2.10,>=2.9 in d:\\workspace\\ml\\pynirsenv\\lib\\site-packages (from tensorflow->pinard) (2.9.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in d:\\workspace\\ml\\pynirsenv\\lib\\site-packages (from tensorflow->pinard) (1.6.3)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in d:\\workspace\\ml\\pynirsenv\\lib\\site-packages (from tensorflow->pinard) (0.2.0)\n",
      "Requirement already satisfied: six>=1.12.0 in d:\\workspace\\ml\\pynirsenv\\lib\\site-packages (from tensorflow->pinard) (1.15.0)\n",
      "Requirement already satisfied: packaging in d:\\workspace\\ml\\pynirsenv\\lib\\site-packages (from tensorflow->pinard) (21.3)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in d:\\workspace\\ml\\pynirsenv\\lib\\site-packages (from tensorflow->pinard) (0.26.0)\n",
      "Requirement already satisfied: setuptools in d:\\workspace\\ml\\pynirsenv\\lib\\site-packages (from tensorflow->pinard) (47.1.0)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in d:\\workspace\\ml\\pynirsenv\\lib\\site-packages (from tensorflow->pinard) (0.4.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in d:\\workspace\\ml\\pynirsenv\\lib\\site-packages (from tensorflow->pinard) (14.0.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in d:\\workspace\\ml\\pynirsenv\\lib\\site-packages (from astunparse>=1.6.0->tensorflow->pinard) (0.37.0)\n",
      "Requirement already satisfied: cached-property in d:\\workspace\\ml\\pynirsenv\\lib\\site-packages (from h5py>=2.9.0->tensorflow->pinard) (1.5.2)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in d:\\workspace\\ml\\pynirsenv\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow->pinard) (1.8.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in d:\\workspace\\ml\\pynirsenv\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow->pinard) (0.4.6)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in d:\\workspace\\ml\\pynirsenv\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow->pinard) (2.0.2)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in d:\\workspace\\ml\\pynirsenv\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow->pinard) (1.35.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in d:\\workspace\\ml\\pynirsenv\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow->pinard) (2.26.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in d:\\workspace\\ml\\pynirsenv\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow->pinard) (0.6.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in d:\\workspace\\ml\\pynirsenv\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow->pinard) (3.3.6)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in d:\\workspace\\ml\\pynirsenv\\lib\\site-packages (from packaging->tensorflow->pinard) (3.0.6)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in d:\\workspace\\ml\\pynirsenv\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow->pinard) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in d:\\workspace\\ml\\pynirsenv\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow->pinard) (4.2.4)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in d:\\workspace\\ml\\pynirsenv\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow->pinard) (4.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in d:\\workspace\\ml\\pynirsenv\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow->pinard) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in d:\\workspace\\ml\\pynirsenv\\lib\\site-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow->pinard) (5.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\workspace\\ml\\pynirsenv\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow->pinard) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in d:\\workspace\\ml\\pynirsenv\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow->pinard) (2.0.9)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in d:\\workspace\\ml\\pynirsenv\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow->pinard) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\workspace\\ml\\pynirsenv\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow->pinard) (3.3)\n",
      "Requirement already satisfied: zipp>=0.5 in d:\\workspace\\ml\\pynirsenv\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow->pinard) (3.6.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in d:\\workspace\\ml\\pynirsenv\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow->pinard) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in d:\\workspace\\ml\\pynirsenv\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow->pinard) (3.1.1)\n",
      "Requirement already satisfied: scikeras in d:\\workspace\\ml\\pynirsenv\\lib\\site-packages (0.9.0)\n",
      "Requirement already satisfied: packaging>=0.21 in d:\\workspace\\ml\\pynirsenv\\lib\\site-packages (from scikeras) (21.3)\n",
      "Requirement already satisfied: importlib-metadata>=3 in d:\\workspace\\ml\\pynirsenv\\lib\\site-packages (from scikeras) (5.0.0)\n",
      "Requirement already satisfied: scikit-learn>=1.0.0 in d:\\workspace\\ml\\pynirsenv\\lib\\site-packages (from scikeras) (1.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in d:\\workspace\\ml\\pynirsenv\\lib\\site-packages (from importlib-metadata>=3->scikeras) (4.4.0)\n",
      "Requirement already satisfied: zipp>=0.5 in d:\\workspace\\ml\\pynirsenv\\lib\\site-packages (from importlib-metadata>=3->scikeras) (3.6.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in d:\\workspace\\ml\\pynirsenv\\lib\\site-packages (from packaging>=0.21->scikeras) (3.0.6)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in d:\\workspace\\ml\\pynirsenv\\lib\\site-packages (from scikit-learn>=1.0.0->scikeras) (3.0.0)\n",
      "Requirement already satisfied: joblib>=0.11 in d:\\workspace\\ml\\pynirsenv\\lib\\site-packages (from scikit-learn>=1.0.0->scikeras) (1.1.0)\n",
      "Requirement already satisfied: numpy>=1.14.6 in d:\\workspace\\ml\\pynirsenv\\lib\\site-packages (from scikit-learn>=1.0.0->scikeras) (1.21.6)\n",
      "Requirement already satisfied: scipy>=1.1.0 in d:\\workspace\\ml\\pynirsenv\\lib\\site-packages (from scikit-learn>=1.0.0->scikeras) (1.7.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ip (d:\\workspace\\ml\\pynirsenv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (d:\\workspace\\ml\\pynirsenv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (d:\\workspace\\ml\\pynirsenv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (d:\\workspace\\ml\\pynirsenv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (d:\\workspace\\ml\\pynirsenv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (d:\\workspace\\ml\\pynirsenv\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install pinard\n",
    "!pip install scikeras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark details\n",
    "\n",
    "The results aggregate the combination of the following trainings configurations:\n",
    "- estimation configuration: [regression, classification]\n",
    "- datasets configurations: [Single Train, Cross validation with 5 folds and 2 repeats, Augmented Single Train]\n",
    "- preprocessing configuration: [flat spectrum, savgol, haar, [small set], [big_set]]\n",
    "- models: \n",
    "   - for all configuration: BACON, BACON-VG, DECON, PLS(components from 1 to 100), XGBoost, LW-PLS\n",
    "   - for single train + small_set : Stack > [ BACON, BACON-VG, DECON, PLS(components from 1 to 100), XGBoost, LW-PLS,\n",
    "   f_PLSRegression,f_AdaBoostRegressor,f_BaggingRegressor,f_ExtraTreesRegressor, f_GradientBoostingRegressor,f_RandomForestRegressor,\n",
    "   f_ARDRegression,f_BayesianRidge,f_ElasticNet,f_ElasticNetCV,f_HuberRegressor, f_LarsCV,f_LassoCV,f_Lasso,f_LassoLars,f_LassoLarsCV,\n",
    "   f_LassoLarsIC,f_LinearRegression,f_OrthogonalMatchingPursuit,f_OrthogonalMatchingPursuitCV, f_PassiveAggressiveRegressor,f_RANSACRegressor,\n",
    "   f_Ridge,f_RidgeCV,f_SGDRegressor,f_TheilSenRegressor,f_GaussianProcessRegressor,f_KNeighborsRegressor, f_Pipeline,f_MLPRegressor,f_LinearSVR,\n",
    "   f_NuSVR,f_SVR,f_DecisionTreeRegressor,f_ExtraTreeRegressor,f_KernelRidge,f_XGBRegressor]\n",
    "\n",
    "We perform training in 2 steps, (1) data transformation and (2) training because the sklearn pipeline does not use test data natively.\n",
    "To change with pinard update in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### FAST GPU RESET ####\n",
    "from numba import cuda \n",
    "device = cuda.get_current_device()\n",
    "device.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "\n",
    "from contextlib import redirect_stdout\n",
    "# import joblib\n",
    "# import pickle\n",
    "\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.metrics \\\n",
    "    import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error,\\\n",
    "        r2_score, explained_variance_score, mean_squared_log_error, median_absolute_error\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "import tensorflow as tf\n",
    "\n",
    "from data import load_data\n",
    "from preprocessings import transform_test_data\n",
    "from regressors import nn_list, ml_list, get_keras_model\n",
    "from pinard import augmentation, sklearn\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "\n",
    "def get_datasheet(dataset_name, model_name, path, SEED, y_valid, y_pred):\n",
    "    return {\n",
    "        \"model\":model_name, \n",
    "        \"dataset\":dataset_name,\n",
    "        \"seed\":str(SEED),\n",
    "        \"targetRMSE\":str(float(os.path.split(path)[-1].split('_')[-1].split(\"RMSE\")[-1])),\n",
    "        \"RMSE\":str(mean_squared_error(y_valid, y_pred, squared=False)),\n",
    "        \"MAPE\":str(mean_absolute_percentage_error(y_valid, y_pred)),\n",
    "        \"R2\":str(r2_score(y_valid, y_pred)),\n",
    "        \"MAE\":str(mean_absolute_error(y_valid, y_pred)),\n",
    "        \"MSE\":str(mean_squared_error(y_valid, y_pred, squared=True)),\n",
    "        \"MedAE\":str(median_absolute_error(y_valid, y_pred)),\n",
    "        \"EVS\":str(explained_variance_score(y_valid, y_pred)),\n",
    "        # \"MSLE\":str(mean_squared_log_error(y_valid, y_pred)),\n",
    "        \"run\":datetime.datetime.now().strftime(\"%Y-%m-%d  %H:%M:%S\")\n",
    "    }\n",
    "\n",
    "def log_run(dataset_name, model_name, path, SEED, y_valid, y_pred, elapsed_time):\n",
    "    datasheet = get_datasheet(dataset_name, model_name, path, SEED, y_valid, y_pred)\n",
    "    ### Save data\n",
    "    folder = \"results/\" + dataset_name\n",
    "    if not os.path.isdir(folder):\n",
    "        os.makedirs(folder)\n",
    "\n",
    "    canon_name = folder + \"/\" + model_name\n",
    "\n",
    "        ## save predictions\n",
    "    np.savetxt(canon_name + '.csv', np.column_stack((y_valid, y_pred)))\n",
    "\n",
    "    ## save main metrics globally\n",
    "    result_file = open(folder + \"/_runs.txt\", \"a\")\n",
    "    log = datasheet[\"RMSE\"] + \"  ---  \" + model_name + \" in \" + time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time)) \\\n",
    "        + ' ('+ datetime.datetime.now().strftime(\"%Y-%m-%d_%H:%M:%S\") + ')\\n'\n",
    "    result_file.write(log)\n",
    "    result_file.close()\n",
    "\n",
    "    ## save pipeline\n",
    "    # joblib.dump(estimator, canon_name + '.pkl')\n",
    "\n",
    "    return datasheet\n",
    "\n",
    "current_estimator = None\n",
    "current_X_test = None\n",
    "current_y_test = None\n",
    "current_path = None\n",
    "\n",
    "def callback_predict(epoch, val_loss):\n",
    "    if current_estimator is None:\n",
    "        return\n",
    "    \n",
    "    y_pred = current_estimator.predict(current_X_test)\n",
    "    res = get_datasheet(\"\", \"\", current_path, -1, current_y_test, y_pred)\n",
    "    print('Epoch:', epoch,'> RMSE:', res['RMSE'], '(', res['targetRMSE'], ') - RÂ²:', res['R2'], ' val_loss', val_loss)\n",
    "\n",
    "def evaluate_pipeline(desc, model_name, data, transformers):\n",
    "    print(\"<\", model_name, \">\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Unpack args\n",
    "    X_train, y_train, X_valid, y_valid = data\n",
    "    dataset_name, path, global_result_file, results, SEED = desc\n",
    "    global current_path\n",
    "    current_path = path\n",
    "    y_scaler, transformer_pipeline, regressor = transformers\n",
    "\n",
    "    # Construct pipeline\n",
    "    pipeline = Pipeline([\n",
    "        ('transformation', transformer_pipeline), \n",
    "        (model_name, regressor)\n",
    "    ])\n",
    "\n",
    "    # Fit estimator\n",
    "    estimator = TransformedTargetRegressor(regressor = pipeline, transformer = y_scaler)\n",
    "    global current_estimator\n",
    "    current_estimator = estimator\n",
    "    global current_X_test\n",
    "    current_X_test = X_valid\n",
    "    global current_y_test\n",
    "    current_y_test = y_valid\n",
    "    estimator.fit(X_train, y_train)\n",
    "    # Evaluate estimator\n",
    "    y_pred = estimator.predict(X_valid)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    datasheet = log_run(dataset_name, model_name, path, SEED, y_valid, y_pred, elapsed_time)\n",
    "    datasheet[\"training_time\"] = time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time))\n",
    "    results[model_name] = datasheet\n",
    "\n",
    "    # Save results\n",
    "    results = OrderedDict(sorted(results.items(), key=lambda k_v: float(k_v[1]['RMSE'])))\n",
    "    with open(global_result_file, 'w') as fp:\n",
    "        json.dump(results, fp, indent=4)\n",
    "\n",
    "    print(datasheet[\"RMSE\"], \" (\", datasheet[\"targetRMSE\"], \") in\", datasheet[\"training_time\"])\n",
    "    return y_pred\n",
    "\n",
    "\n",
    "def benchmark_dataset(path, SEED, preprocessing_list, batch_size=50, augment=False):\n",
    "    dataset_name = ('_').join(os.path.split(path)[-1].split('_')[:-1])\n",
    "    print(\"=\"*10, str(dataset_name).upper(), end=\" \")\n",
    "    global_result_file = \"results/\" + dataset_name + '_results.json'\n",
    "    results = {}\n",
    "    if os.path.isfile(global_result_file):\n",
    "        with open(global_result_file) as json_file:\n",
    "            results = json.load(json_file)\n",
    "   \n",
    "    desc = (dataset_name, path, global_result_file, results, SEED)\n",
    "\n",
    "    X, y, X_valid, y_valid = load_data(path)\n",
    "    print(X.shape, y.shape, X_valid.shape, y_valid.shape, \"=\"*10)\n",
    "    \n",
    "\n",
    "    #########################\n",
    "    ### SINGLE RUN TRAINING\n",
    "    X_train, y_train, X_test, y_test = X, y, X_valid, y_valid\n",
    "\n",
    "    if(augment):\n",
    "        augmentation_pipeline = sklearn.SampleAugmentation([\n",
    "            (2, 'rot_tr', augmentation.Rotate_Translate()),\n",
    "            (1, 'rd_mult', augmentation.Random_X_Operation()),\n",
    "            (1, 'simpl', augmentation.Random_Spline_Addition())\n",
    "        ])\n",
    "        print(X_train.shape, y_train.shape)\n",
    "        X_train, y_train = augmentation_pipeline.transform(X_train, y_train)\n",
    "        print(\"augmented to:\", X_train.shape, y_train.shape)\n",
    "\n",
    "    data = (X_train, y_train, X_valid, y_valid)\n",
    "    for preprocessing in preprocessing_list:            \n",
    "        ##### DEEP LEARNING #####\n",
    "        X_test_pp, y_test_pp, transformer_pipeline, y_scaler = transform_test_data(preprocessing, X_train, y_train, X_test, y_test, type=\"augmentation\")\n",
    "        for model_desc in nn_list():\n",
    "            model_name = model_desc.__name__ + \"-\" + preprocessing.__name__ + \"-\" + str(SEED)\n",
    "            if os.path.isfile(  \"results/\" + dataset_name + \"/\" + model_name + '.csv'):\n",
    "                # print(\"Skipping\", model_name)\n",
    "                continue\n",
    "            \n",
    "            # batch_size = 3000\n",
    "            # if preprocessing.__name__ == \"dumb_set\":\n",
    "            #     batch_size = 3\n",
    "            regressor = get_keras_model(dataset_name + '_' + model_name, model_desc, 4096, batch_size, X_test_pp, y_test_pp, transfer=True, callback_func=callback_predict, verbose=0, seed=SEED)\n",
    "            transformers = (y_scaler, transformer_pipeline, regressor)\n",
    "            evaluate_pipeline(desc, model_name, data, transformers)\n",
    "        \n",
    "        # ##### MACHINE LEARNING #####\n",
    "        # X_test_pp, y_test_pp, transformer_pipeline, y_scaler = transform_test_data(preprocessing, X_train, y_train, X_test, y_test, type=\"union\")\n",
    "        # for regressor, mdl_name in ml_list(SEED, X_test_pp, y_test_pp):\n",
    "        #     model_name = mdl_name + \"-\" + preprocessing.__name__ + \"-\" + str(SEED)\n",
    "        #     if os.path.isfile(  \"results/\" + dataset_name + \"/\" + model_name + '.csv'):\n",
    "        #        # print(\"Skipping\", model_name)\n",
    "        #         continue\n",
    "        #     transformers = (y_scaler, transformer_pipeline, regressor)\n",
    "        #     evaluate_pipeline(desc, model_name, data, transformers)\n",
    "\n",
    "    #########################\n",
    "\n",
    "\n",
    "\n",
    "    #########################\n",
    "    # ### CROSS VALIDATION TRAINING\n",
    "    # cv_predictions = {}\n",
    "    # for preprocessing in preprocessing_list():\n",
    "    #     fold = RepeatedKFold(n_splits=5, n_repeats=2, random_state=SEED)\n",
    "    #     fold_index = 0\n",
    "    #     for train_index, test_index in fold.split(X):\n",
    "    #         X_train, y_train, X_test, y_test = X[train_index], y[train_index], X[test_index], y[test_index]\n",
    "    #         data = (X_train, y_train, X_valid, y_valid)\n",
    "            \n",
    "    #         ##### DEEP LEARNING #####\n",
    "    #         X_test_pp, y_test_pp, transformer_pipeline, y_scaler = transform_test_data(preprocessing, X_train, y_train, X_test, y_test, type=\"augmentation\")\n",
    "    #         for model_desc in nn_list():\n",
    "    #             model_name = model_desc.__name__ + \"-\" + preprocessing.__name__  + \"-\" + str(SEED)\n",
    "    #             fold_name = model_name + \"-F\" + str(fold_index)\n",
    "    #             if os.path.isfile(  \"results/\" + dataset_name + \"/\" + fold_name + '.csv'):\n",
    "    #                # print(\"Skipping\", model_name)\n",
    "    #                 continue\n",
    "    #             regressor = get_keras_model(dataset_name + '_' + fold_name, model_desc, 7500, 750, X_test_pp, y_test_pp, verbose=0, seed=SEED)\n",
    "    #             y_pred = evaluate_pipeline(desc, fold_name, data, (y_scaler, transformer_pipeline, regressor))\n",
    "    #             cv_predictions[model_name] = cv_predictions[model_name] + y_pred if model_name in cv_predictions else y_pred\n",
    "            \n",
    "    #         ##### MACHINE LEARNING #####\n",
    "    #         X_test_pp, y_test_pp, transformer_pipeline, y_scaler = transform_test_data(preprocessing, X_train, y_train, X_test, y_test, type=\"union\")\n",
    "    #         for regressor, mdl_name in ml_list(SEED, X_test_pp, y_test_pp):\n",
    "    #             model_name = mdl_name + \"-\" + preprocessing.__name__ + \"-\" + str(SEED)\n",
    "    #             fold_name = model_name + \"-F\" + str(fold_index)\n",
    "    #             if os.path.isfile(  \"results/\" + dataset_name + \"/\" + fold_name + '.csv'):\n",
    "    #              #  print(\"Skipping\", model_name)\n",
    "    #                 continue\n",
    "    #             y_pred = evaluate_pipeline(desc, fold_name, data, (y_scaler, transformer_pipeline, regressor))\n",
    "    #             cv_predictions[model_name] = cv_predictions[model_name] + y_pred if model_name in cv_predictions else y_pred\n",
    "                \n",
    "    #         fold_index +=1\n",
    "\n",
    "    # for key, val in cv_predictions.items():\n",
    "    #     y_pred = val / fold.get_n_splits()\n",
    "    #     datasheet = get_datasheet(dataset_name, key, path, SEED, y_valid, y_pred)\n",
    "    #     results[key +\"_CV\"] = datasheet\n",
    "    #\n",
    "    # results = OrderedDict(sorted(results.items(), key=lambda k_v: float(k_v[1]['RMSE'])))\n",
    "    # with open(global_result_file, 'w') as fp:\n",
    "    #     json.dump(results, fp, indent=4)\n",
    "\n",
    "    # #########################\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "========== LUCAS_SOCGRASSLAND_4096_NOCITA (2867, 4200) (2867,) (1229, 4200) (1229,) ==========\n",
      "--- Trainable: 120945 - untrainable: 0.0 > 120945.0\n",
      "< transformer_nirs-decon_set-31441 >\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 57). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "Graph execution error:\n\nDetected at node 'model_6/multi_head_attention_22/softmax_22/Softmax' defined at (most recent call last):\n    File \"C:\\Users\\grego\\AppData\\Local\\Programs\\Python\\Python37\\lib\\runpy.py\", line 193, in _run_module_as_main\n      \"__main__\", mod_spec)\n    File \"C:\\Users\\grego\\AppData\\Local\\Programs\\Python\\Python37\\lib\\runpy.py\", line 85, in _run_code\n      exec(code, run_globals)\n    File \"d:\\Workspace\\ML\\pynirsENV\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n      app.launch_new_instance()\n    File \"d:\\Workspace\\ML\\pynirsENV\\lib\\site-packages\\traitlets\\config\\application.py\", line 846, in launch_instance\n      app.start()\n    File \"d:\\Workspace\\ML\\pynirsENV\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 677, in start\n      self.io_loop.start()\n    File \"d:\\Workspace\\ML\\pynirsENV\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n      self.asyncio_loop.run_forever()\n    File \"C:\\Users\\grego\\AppData\\Local\\Programs\\Python\\Python37\\lib\\asyncio\\base_events.py\", line 541, in run_forever\n      self._run_once()\n    File \"C:\\Users\\grego\\AppData\\Local\\Programs\\Python\\Python37\\lib\\asyncio\\base_events.py\", line 1786, in _run_once\n      handle._run()\n    File \"C:\\Users\\grego\\AppData\\Local\\Programs\\Python\\Python37\\lib\\asyncio\\events.py\", line 88, in _run\n      self._context.run(self._callback, *self._args)\n    File \"d:\\Workspace\\ML\\pynirsENV\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 457, in dispatch_queue\n      await self.process_one()\n    File \"d:\\Workspace\\ML\\pynirsENV\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 446, in process_one\n      await dispatch(*args)\n    File \"d:\\Workspace\\ML\\pynirsENV\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 353, in dispatch_shell\n      await result\n    File \"d:\\Workspace\\ML\\pynirsENV\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 648, in execute_request\n      reply_content = await reply_content\n    File \"d:\\Workspace\\ML\\pynirsENV\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 353, in do_execute\n      res = shell.run_cell(code, store_history=store_history, silent=silent)\n    File \"d:\\Workspace\\ML\\pynirsENV\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 533, in run_cell\n      return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n    File \"d:\\Workspace\\ML\\pynirsENV\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2915, in run_cell\n      raw_cell, store_history, silent, shell_futures)\n    File \"d:\\Workspace\\ML\\pynirsENV\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2960, in _run_cell\n      return runner(coro)\n    File \"d:\\Workspace\\ML\\pynirsENV\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 78, in _pseudo_sync_runner\n      coro.send(None)\n    File \"d:\\Workspace\\ML\\pynirsENV\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3186, in run_cell_async\n      interactivity=interactivity, compiler=compiler, result=result)\n    File \"d:\\Workspace\\ML\\pynirsENV\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3377, in run_ast_nodes\n      if (await self.run_code(code, result,  async_=asy)):\n    File \"d:\\Workspace\\ML\\pynirsENV\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3457, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\grego\\AppData\\Local\\Temp/ipykernel_27656/1583894892.py\", line 19, in <module>\n      benchmark_dataset(\"data/regression/LUCAS_SOCgrassland_4096_Nocita_RMSE7.2\", SEED, preprocessing_list(), 100, augment=False)\n    File \"C:\\Users\\grego\\AppData\\Local\\Temp/ipykernel_27656/2898103413.py\", line 169, in benchmark_dataset\n      evaluate_pipeline(desc, model_name, data, transformers)\n    File \"C:\\Users\\grego\\AppData\\Local\\Temp/ipykernel_27656/2898103413.py\", line 108, in evaluate_pipeline\n      estimator.fit(X_train, y_train)\n    File \"d:\\Workspace\\ML\\pynirsENV\\lib\\site-packages\\sklearn\\compose\\_target.py\", line 246, in fit\n      self.regressor_.fit(X, y_trans, **fit_params)\n    File \"d:\\Workspace\\ML\\pynirsENV\\lib\\site-packages\\sklearn\\pipeline.py\", line 394, in fit\n      self._final_estimator.fit(Xt, y, **fit_params_last_step)\n    File \"d:\\Workspace\\ML\\pynirsENV\\lib\\site-packages\\scikeras\\wrappers.py\", line 767, in fit\n      **kwargs,\n    File \"d:\\Workspace\\ML\\pynirsENV\\lib\\site-packages\\scikeras\\wrappers.py\", line 938, in _fit\n      **kwargs,\n    File \"d:\\Workspace\\ML\\pynirsENV\\lib\\site-packages\\scikeras\\wrappers.py\", line 526, in _fit_keras_model\n      hist = self.model_.fit(x=X, y=y, **fit_args)\n    File \"d:\\Workspace\\ML\\pynirsENV\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"d:\\Workspace\\ML\\pynirsENV\\lib\\site-packages\\keras\\engine\\training.py\", line 1409, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"d:\\Workspace\\ML\\pynirsENV\\lib\\site-packages\\keras\\engine\\training.py\", line 1051, in train_function\n      return step_function(self, iterator)\n    File \"d:\\Workspace\\ML\\pynirsENV\\lib\\site-packages\\keras\\engine\\training.py\", line 1040, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"d:\\Workspace\\ML\\pynirsENV\\lib\\site-packages\\keras\\engine\\training.py\", line 1030, in run_step\n      outputs = model.train_step(data)\n    File \"d:\\Workspace\\ML\\pynirsENV\\lib\\site-packages\\keras\\engine\\training.py\", line 889, in train_step\n      y_pred = self(x, training=True)\n    File \"d:\\Workspace\\ML\\pynirsENV\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"d:\\Workspace\\ML\\pynirsENV\\lib\\site-packages\\keras\\engine\\training.py\", line 490, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"d:\\Workspace\\ML\\pynirsENV\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"d:\\Workspace\\ML\\pynirsENV\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1014, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"d:\\Workspace\\ML\\pynirsENV\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"d:\\Workspace\\ML\\pynirsENV\\lib\\site-packages\\keras\\engine\\functional.py\", line 459, in call\n      inputs, training=training, mask=mask)\n    File \"d:\\Workspace\\ML\\pynirsENV\\lib\\site-packages\\keras\\engine\\functional.py\", line 596, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"d:\\Workspace\\ML\\pynirsENV\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"d:\\Workspace\\ML\\pynirsENV\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1014, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"d:\\Workspace\\ML\\pynirsENV\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"d:\\Workspace\\ML\\pynirsENV\\lib\\site-packages\\keras\\layers\\attention\\multi_head_attention.py\", line 510, in call\n      query, key, value, attention_mask, training)\n    File \"d:\\Workspace\\ML\\pynirsENV\\lib\\site-packages\\keras\\layers\\attention\\multi_head_attention.py\", line 474, in _compute_attention\n      attention_scores = self._masked_softmax(attention_scores, attention_mask)\n    File \"d:\\Workspace\\ML\\pynirsENV\\lib\\site-packages\\keras\\layers\\attention\\multi_head_attention.py\", line 438, in _masked_softmax\n      return self._softmax(attention_scores, attention_mask)\n    File \"d:\\Workspace\\ML\\pynirsENV\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"d:\\Workspace\\ML\\pynirsENV\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1014, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"d:\\Workspace\\ML\\pynirsENV\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"d:\\Workspace\\ML\\pynirsENV\\lib\\site-packages\\keras\\layers\\activation\\softmax.py\", line 98, in call\n      return backend.softmax(inputs, axis=self.axis[0])\n    File \"d:\\Workspace\\ML\\pynirsENV\\lib\\site-packages\\keras\\backend.py\", line 5039, in softmax\n      return tf.nn.softmax(x, axis=axis)\nNode: 'model_6/multi_head_attention_22/softmax_22/Softmax'\nOOM when allocating tensor with shape[100,2,4198,4198] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node model_6/multi_head_attention_22/softmax_22/Softmax}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_16698836]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_27656/1583894892.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;31m# benchmark_dataset(\"data/regression/ALPINE_Calpine_424_Murguzur_RMSE1.36\", SEED, preprocessing_list(), 200)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;31m# benchmark_dataset(\"data/regression/Cassava_TBC_3556_Davrieux_RMSE1.02\", SEED, preprocessing_list(), augment=True)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[0mbenchmark_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"data/regression/LUCAS_SOCgrassland_4096_Nocita_RMSE7.2\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSEED\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreprocessing_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maugment\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[0mbenchmark_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"data/regression/ALPINE_Calpine_424_Murguzur_RMSE1.36\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSEED\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreprocessing_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maugment\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[0mbenchmark_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"data/regression/Cassava_TBC_3556_Davrieux_RMSE1.02\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSEED\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreprocessing_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maugment\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_27656/2898103413.py\u001b[0m in \u001b[0;36mbenchmark_dataset\u001b[1;34m(path, SEED, preprocessing_list, batch_size, augment)\u001b[0m\n\u001b[0;32m    167\u001b[0m             \u001b[0mregressor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_keras_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset_name\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'_'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_desc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4096\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test_pp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test_pp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransfer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback_func\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallback_predict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mSEED\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m             \u001b[0mtransformers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0my_scaler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransformer_pipeline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mregressor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 169\u001b[1;33m             \u001b[0mevaluate_pipeline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdesc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    170\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m         \u001b[1;31m# ##### MACHINE LEARNING #####\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_27656/2898103413.py\u001b[0m in \u001b[0;36mevaluate_pipeline\u001b[1;34m(desc, model_name, data, transformers)\u001b[0m\n\u001b[0;32m    106\u001b[0m     \u001b[1;32mglobal\u001b[0m \u001b[0mcurrent_y_test\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[0mcurrent_y_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_valid\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m     \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m     \u001b[1;31m# Evaluate estimator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Workspace\\ML\\pynirsENV\\lib\\site-packages\\sklearn\\compose\\_target.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    244\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregressor_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregressor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    245\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 246\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregressor_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_trans\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    247\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    248\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregressor_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"feature_names_in_\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Workspace\\ML\\pynirsENV\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    392\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m\"passthrough\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    393\u001b[0m                 \u001b[0mfit_params_last_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_params_steps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 394\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params_last_step\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    395\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    396\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Workspace\\ML\\pynirsENV\\lib\\site-packages\\scikeras\\wrappers.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, **kwargs)\u001b[0m\n\u001b[0;32m    765\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    766\u001b[0m             \u001b[0mwarm_start\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarm_start\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 767\u001b[1;33m             \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    768\u001b[0m         )\n\u001b[0;32m    769\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Workspace\\ML\\pynirsENV\\lib\\site-packages\\scikeras\\wrappers.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, y, sample_weight, warm_start, epochs, initial_epoch, **kwargs)\u001b[0m\n\u001b[0;32m    936\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    937\u001b[0m             \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 938\u001b[1;33m             \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    939\u001b[0m         )\n\u001b[0;32m    940\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Workspace\\ML\\pynirsENV\\lib\\site-packages\\scikeras\\wrappers.py\u001b[0m in \u001b[0;36m_fit_keras_model\u001b[1;34m(self, X, y, sample_weight, warm_start, epochs, initial_epoch, **kwargs)\u001b[0m\n\u001b[0;32m    524\u001b[0m                 \u001b[0mhist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    525\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 526\u001b[1;33m             \u001b[0mhist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    527\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    528\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mwarm_start\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"history_\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0minitial_epoch\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Workspace\\ML\\pynirsENV\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Workspace\\ML\\pynirsENV\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 55\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     56\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: Graph execution error:\n\nDetected at node 'model_6/multi_head_attention_22/softmax_22/Softmax' defined at (most recent call last):\n    File \"C:\\Users\\grego\\AppData\\Local\\Programs\\Python\\Python37\\lib\\runpy.py\", line 193, in _run_module_as_main\n      \"__main__\", mod_spec)\n    File \"C:\\Users\\grego\\AppData\\Local\\Programs\\Python\\Python37\\lib\\runpy.py\", line 85, in _run_code\n      exec(code, run_globals)\n    File \"d:\\Workspace\\ML\\pynirsENV\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n      app.launch_new_instance()\n    File \"d:\\Workspace\\ML\\pynirsENV\\lib\\site-packages\\traitlets\\config\\application.py\", line 846, in launch_instance\n      app.start()\n    File \"d:\\Workspace\\ML\\pynirsENV\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 677, in start\n      self.io_loop.start()\n    File \"d:\\Workspace\\ML\\pynirsENV\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n      self.asyncio_loop.run_forever()\n    File \"C:\\Users\\grego\\AppData\\Local\\Programs\\Python\\Python37\\lib\\asyncio\\base_events.py\", line 541, in run_forever\n      self._run_once()\n    File \"C:\\Users\\grego\\AppData\\Local\\Programs\\Python\\Python37\\lib\\asyncio\\base_events.py\", line 1786, in _run_once\n      handle._run()\n    File \"C:\\Users\\grego\\AppData\\Local\\Programs\\Python\\Python37\\lib\\asyncio\\events.py\", line 88, in _run\n      self._context.run(self._callback, *self._args)\n    File \"d:\\Workspace\\ML\\pynirsENV\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 457, in dispatch_queue\n      await self.process_one()\n    File \"d:\\Workspace\\ML\\pynirsENV\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 446, in process_one\n      await dispatch(*args)\n    File \"d:\\Workspace\\ML\\pynirsENV\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 353, in dispatch_shell\n      await result\n    File \"d:\\Workspace\\ML\\pynirsENV\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 648, in execute_request\n      reply_content = await reply_content\n    File \"d:\\Workspace\\ML\\pynirsENV\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 353, in do_execute\n      res = shell.run_cell(code, store_history=store_history, silent=silent)\n    File \"d:\\Workspace\\ML\\pynirsENV\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 533, in run_cell\n      return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n    File \"d:\\Workspace\\ML\\pynirsENV\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2915, in run_cell\n      raw_cell, store_history, silent, shell_futures)\n    File \"d:\\Workspace\\ML\\pynirsENV\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2960, in _run_cell\n      return runner(coro)\n    File \"d:\\Workspace\\ML\\pynirsENV\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 78, in _pseudo_sync_runner\n      coro.send(None)\n    File \"d:\\Workspace\\ML\\pynirsENV\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3186, in run_cell_async\n      interactivity=interactivity, compiler=compiler, result=result)\n    File \"d:\\Workspace\\ML\\pynirsENV\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3377, in run_ast_nodes\n      if (await self.run_code(code, result,  async_=asy)):\n    File \"d:\\Workspace\\ML\\pynirsENV\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3457, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\grego\\AppData\\Local\\Temp/ipykernel_27656/1583894892.py\", line 19, in <module>\n      benchmark_dataset(\"data/regression/LUCAS_SOCgrassland_4096_Nocita_RMSE7.2\", SEED, preprocessing_list(), 100, augment=False)\n    File \"C:\\Users\\grego\\AppData\\Local\\Temp/ipykernel_27656/2898103413.py\", line 169, in benchmark_dataset\n      evaluate_pipeline(desc, model_name, data, transformers)\n    File \"C:\\Users\\grego\\AppData\\Local\\Temp/ipykernel_27656/2898103413.py\", line 108, in evaluate_pipeline\n      estimator.fit(X_train, y_train)\n    File \"d:\\Workspace\\ML\\pynirsENV\\lib\\site-packages\\sklearn\\compose\\_target.py\", line 246, in fit\n      self.regressor_.fit(X, y_trans, **fit_params)\n    File \"d:\\Workspace\\ML\\pynirsENV\\lib\\site-packages\\sklearn\\pipeline.py\", line 394, in fit\n      self._final_estimator.fit(Xt, y, **fit_params_last_step)\n    File \"d:\\Workspace\\ML\\pynirsENV\\lib\\site-packages\\scikeras\\wrappers.py\", line 767, in fit\n      **kwargs,\n    File \"d:\\Workspace\\ML\\pynirsENV\\lib\\site-packages\\scikeras\\wrappers.py\", line 938, in _fit\n      **kwargs,\n    File \"d:\\Workspace\\ML\\pynirsENV\\lib\\site-packages\\scikeras\\wrappers.py\", line 526, in _fit_keras_model\n      hist = self.model_.fit(x=X, y=y, **fit_args)\n    File \"d:\\Workspace\\ML\\pynirsENV\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"d:\\Workspace\\ML\\pynirsENV\\lib\\site-packages\\keras\\engine\\training.py\", line 1409, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"d:\\Workspace\\ML\\pynirsENV\\lib\\site-packages\\keras\\engine\\training.py\", line 1051, in train_function\n      return step_function(self, iterator)\n    File \"d:\\Workspace\\ML\\pynirsENV\\lib\\site-packages\\keras\\engine\\training.py\", line 1040, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"d:\\Workspace\\ML\\pynirsENV\\lib\\site-packages\\keras\\engine\\training.py\", line 1030, in run_step\n      outputs = model.train_step(data)\n    File \"d:\\Workspace\\ML\\pynirsENV\\lib\\site-packages\\keras\\engine\\training.py\", line 889, in train_step\n      y_pred = self(x, training=True)\n    File \"d:\\Workspace\\ML\\pynirsENV\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"d:\\Workspace\\ML\\pynirsENV\\lib\\site-packages\\keras\\engine\\training.py\", line 490, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"d:\\Workspace\\ML\\pynirsENV\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"d:\\Workspace\\ML\\pynirsENV\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1014, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"d:\\Workspace\\ML\\pynirsENV\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"d:\\Workspace\\ML\\pynirsENV\\lib\\site-packages\\keras\\engine\\functional.py\", line 459, in call\n      inputs, training=training, mask=mask)\n    File \"d:\\Workspace\\ML\\pynirsENV\\lib\\site-packages\\keras\\engine\\functional.py\", line 596, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"d:\\Workspace\\ML\\pynirsENV\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"d:\\Workspace\\ML\\pynirsENV\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1014, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"d:\\Workspace\\ML\\pynirsENV\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"d:\\Workspace\\ML\\pynirsENV\\lib\\site-packages\\keras\\layers\\attention\\multi_head_attention.py\", line 510, in call\n      query, key, value, attention_mask, training)\n    File \"d:\\Workspace\\ML\\pynirsENV\\lib\\site-packages\\keras\\layers\\attention\\multi_head_attention.py\", line 474, in _compute_attention\n      attention_scores = self._masked_softmax(attention_scores, attention_mask)\n    File \"d:\\Workspace\\ML\\pynirsENV\\lib\\site-packages\\keras\\layers\\attention\\multi_head_attention.py\", line 438, in _masked_softmax\n      return self._softmax(attention_scores, attention_mask)\n    File \"d:\\Workspace\\ML\\pynirsENV\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"d:\\Workspace\\ML\\pynirsENV\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1014, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"d:\\Workspace\\ML\\pynirsENV\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"d:\\Workspace\\ML\\pynirsENV\\lib\\site-packages\\keras\\layers\\activation\\softmax.py\", line 98, in call\n      return backend.softmax(inputs, axis=self.axis[0])\n    File \"d:\\Workspace\\ML\\pynirsENV\\lib\\site-packages\\keras\\backend.py\", line 5039, in softmax\n      return tf.nn.softmax(x, axis=axis)\nNode: 'model_6/multi_head_attention_22/softmax_22/Softmax'\nOOM when allocating tensor with shape[100,2,4198,4198] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node model_6/multi_head_attention_22/softmax_22/Softmax}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_16698836]"
     ]
    }
   ],
   "source": [
    "## Browse path and launch benchmark for every folders\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pathlib import Path\n",
    "from preprocessings import preprocessing_list\n",
    "\n",
    "rootdir = Path('data/regression')\n",
    "folder_list = [f for f in rootdir.glob('**/*') if f.is_dir()]\n",
    "\n",
    "SEED = ord('D') + 31373\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# (preprocessing_list, nn_run, nn_cv, ml_single, ml_cv)\n",
    "\n",
    "# benchmark_dataset(\"data/regression/ALPINE_Calpine_424_Murguzur_RMSE1.36\", SEED, preprocessing_list(), 200)\n",
    "# benchmark_dataset(\"data/regression/Cassava_TBC_3556_Davrieux_RMSE1.02\", SEED, preprocessing_list(), augment=True)\n",
    "benchmark_dataset(\"data/regression/LUCAS_SOCgrassland_4096_Nocita_RMSE7.2\", SEED, preprocessing_list(), 20, augment=False)\n",
    "benchmark_dataset(\"data/regression/ALPINE_Calpine_424_Murguzur_RMSE1.36\", SEED, preprocessing_list(), 100, augment=False)\n",
    "benchmark_dataset(\"data/regression/Cassava_TBC_3556_Davrieux_RMSE1.02\", SEED, preprocessing_list(), 20, augment=False)\n",
    "benchmark_dataset(\"data/regression/Meat_FatE1_215_Borggaard_RMSE2.33\", SEED, preprocessing_list(), 100, augment=False)\n",
    "\n",
    "# for folder in folder_list:\n",
    "    # # print(ord(str(folder)[17]), ord('A'), ord('M'))\n",
    "    # if ord(str(folder)[16]) < ord(\"L\") or ord(str(folder)[16]) > ord(\"M\"):\n",
    "    #     continue\n",
    "    # benchmark_dataset(folder, SEED, preprocessing_list(), 20, augment=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pynirsENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7b09f6e5407ec4329146609a0cb08cbbe4720f97bb26598a93c421b663bd10d2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
