{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies\n",
    "\n",
    "Install dependencies not available on Google Collab.\n",
    "Collab provides numpy, pandas, sklearn, tensorflow, scipy, etc. (see requirements.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pinard\n",
    "!pip install scikeras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark details\n",
    "\n",
    "The results aggregate the combination of the following trainings configurations:\n",
    "- estimation configuration: [regression, classification]\n",
    "- datasets configurations: [Single Train, Cross validation with 5 folds and 2 repeats, Augmented Single Train]\n",
    "- preprocessing configuration: [flat spectrum, savgol, haar, [small set], [big_set]]\n",
    "- models: \n",
    "   - for all configuration: BACON, BACON-VG, DECON, PLS(components from 1 to 100), XGBoost, LW-PLS\n",
    "   - for single train + small_set : Stack > [ BACON, BACON-VG, DECON, PLS(components from 1 to 100), XGBoost, LW-PLS,\n",
    "   f_PLSRegression,f_AdaBoostRegressor,f_BaggingRegressor,f_ExtraTreesRegressor, f_GradientBoostingRegressor,f_RandomForestRegressor,\n",
    "   f_ARDRegression,f_BayesianRidge,f_ElasticNet,f_ElasticNetCV,f_HuberRegressor, f_LarsCV,f_LassoCV,f_Lasso,f_LassoLars,f_LassoLarsCV,\n",
    "   f_LassoLarsIC,f_LinearRegression,f_OrthogonalMatchingPursuit,f_OrthogonalMatchingPursuitCV, f_PassiveAggressiveRegressor,f_RANSACRegressor,\n",
    "   f_Ridge,f_RidgeCV,f_SGDRegressor,f_TheilSenRegressor,f_GaussianProcessRegressor,f_KNeighborsRegressor, f_Pipeline,f_MLPRegressor,f_LinearSVR,\n",
    "   f_NuSVR,f_SVR,f_DecisionTreeRegressor,f_ExtraTreeRegressor,f_KernelRidge,f_XGBRegressor]\n",
    "\n",
    "We perform training in 2 steps, (1) data transformation and (2) training because the sklearn pipeline does not use test data natively.\n",
    "To change with pinard update in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "\n",
    "from contextlib import redirect_stdout\n",
    "# import joblib\n",
    "# import pickle\n",
    "\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.metrics \\\n",
    "    import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error,\\\n",
    "        r2_score, explained_variance_score, mean_squared_log_error, median_absolute_error\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "import tensorflow as tf\n",
    "\n",
    "from data import load_data\n",
    "from preprocessings import preprocessing_list, transform_test_data\n",
    "from regressors import nn_list, ml_list, get_keras_model\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "\n",
    "def get_datasheet(dataset_name, model_name, path, SEED, y_valid, y_pred):\n",
    "    return {\n",
    "        \"model\":model_name, \n",
    "        \"dataset\":dataset_name,\n",
    "        \"seed\":str(SEED),\n",
    "        \"targetRMSE\":str(float(os.path.split(path)[-1].split('_')[-1].split(\"RMSE\")[-1])),\n",
    "        \"RMSE\":str(mean_squared_error(y_valid, y_pred, squared=True)),\n",
    "        \"MAPE\":str(mean_absolute_percentage_error(y_valid, y_pred)),\n",
    "        \"R2\":str(r2_score(y_valid, y_pred)),\n",
    "        \"MAE\":str(mean_absolute_error(y_valid, y_pred)),\n",
    "        \"MSE\":str(mean_squared_error(y_valid, y_pred, squared=False)),\n",
    "        \"MedAE\":str(median_absolute_error(y_valid, y_pred)),\n",
    "        \"EVS\":str(explained_variance_score(y_valid, y_pred)),\n",
    "        # \"MSLE\":str(mean_squared_log_error(y_valid, y_pred)),\n",
    "        \"run\":datetime.datetime.now().strftime(\"%Y-%m-%d  %H:%M:%S\")\n",
    "    }\n",
    "\n",
    "def log_run(dataset_name, model_name, path, SEED, y_valid, y_pred):\n",
    "    datasheet = get_datasheet(dataset_name, model_name, path, SEED, y_valid, y_pred)\n",
    "    ### Save data\n",
    "    folder = \"results/\" + dataset_name\n",
    "    if not os.path.isdir(folder):\n",
    "        os.makedirs(folder)\n",
    "\n",
    "    canon_name = folder + \"/\" + model_name\n",
    "\n",
    "        ## save predictions\n",
    "    np.savetxt(canon_name + '.csv', np.column_stack((y_valid, y_pred)))\n",
    "\n",
    "    ## save main metrics globally\n",
    "    result_file = open(folder + \"/_runs.txt\", \"a\")\n",
    "    log = datasheet[\"RMSE\"] + \"  ---  \" + model_name + ' '*10 + datetime.datetime.now().strftime(\"%Y-%m-%d  %H:%M:%S\") + '\\n'\n",
    "    result_file.write(log)\n",
    "    result_file.close()\n",
    "\n",
    "    ## save pipeline\n",
    "    # joblib.dump(estimator, canon_name + '.pkl')\n",
    "\n",
    "    return datasheet\n",
    "\n",
    "\n",
    "def evaluate_pipeline(desc, model_name, data, transformers):\n",
    "    print(\"<\", model_name, \">\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Unpack args\n",
    "    X_train, y_train, X_valid, y_valid = data\n",
    "    dataset_name, path, global_result_file, results, SEED = desc\n",
    "    y_scaler, transformer_pipeline, regressor = transformers\n",
    "\n",
    "    # Construct pipeline\n",
    "    pipeline = Pipeline([\n",
    "        ('transformation', transformer_pipeline), \n",
    "        (model_name, regressor)\n",
    "    ])\n",
    "\n",
    "    # Fit estimator\n",
    "    estimator = TransformedTargetRegressor(regressor = pipeline, transformer = y_scaler)\n",
    "    estimator.fit(X_train, y_train)  \n",
    "    # Evaluate estimator\n",
    "    y_pred = estimator.predict(X_valid)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    datasheet = log_run(dataset_name, model_name, path, SEED, y_valid, y_pred)\n",
    "    datasheet[\"training_time\"] = time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time))\n",
    "    results[model_name] = datasheet\n",
    "\n",
    "    # Save results\n",
    "    results = OrderedDict(sorted(results.items(), key=lambda k_v: float(k_v[1]['RMSE'])))\n",
    "    with open(global_result_file, 'w') as fp:\n",
    "        json.dump(results, fp, indent=4)\n",
    "\n",
    "    print(datasheet[\"RMSE\"], \" (\", datasheet[\"targetRMSE\"], \") in\", datasheet[\"training_time\"])\n",
    "    return y_pred\n",
    "\n",
    "\n",
    "def benchmark_dataset(path, SEED):\n",
    "    dataset_name = ('_').join(os.path.split(path)[-1].split('_')[:-1])\n",
    "    print(\"=\"*10, str(dataset_name).upper(), end=\" \")\n",
    "    global_result_file = \"results/\" + dataset_name + '_results.json'\n",
    "    results = {}\n",
    "    if os.path.isfile(global_result_file):\n",
    "        with open(global_result_file) as json_file:\n",
    "            results = json.load(json_file)\n",
    "   \n",
    "    desc = (dataset_name, path, global_result_file, results, SEED)\n",
    "\n",
    "    X, y, X_valid, y_valid = load_data(path)\n",
    "    print(X.shape, y.shape, X_valid.shape, y_valid.shape, \"=\"*10)\n",
    "    \n",
    "\n",
    "    #########################\n",
    "    ### SINGLE RUN TRAINING\n",
    "    X_train, y_train, X_test, y_test = X, y, X_valid, y_valid\n",
    "    data = (X_train, y_train, X_valid, y_valid)\n",
    "    for preprocessing in preprocessing_list():            \n",
    "        ##### DEEP LEARNING #####\n",
    "        X_test_pp, y_test_pp, transformer_pipeline, y_scaler = transform_test_data(preprocessing, X_train, y_train, X_test, y_test, type=\"augmentation\")\n",
    "        for model_desc in nn_list():\n",
    "            model_name = model_desc.__name__ + \"-\" + preprocessing.__name__ + \"-\" + str(SEED)\n",
    "            if os.path.isfile(  \"results/\" + dataset_name + \"/\" + model_name + '.csv'):\n",
    "                # print(\"Skipping\", model_name)\n",
    "                continue\n",
    "\n",
    "            regressor = get_keras_model(dataset_name + '_' + model_name, model_desc, 7500, 750, X_test_pp, y_test_pp, verbose=0, seed=SEED)\n",
    "            transformers = (y_scaler, transformer_pipeline, regressor)\n",
    "            evaluate_pipeline(desc, model_name, data, transformers)\n",
    "        \n",
    "        # ##### MACHINE LEARNING #####\n",
    "        # X_test_pp, y_test_pp, transformer_pipeline, y_scaler = transform_test_data(preprocessing, X_train, y_train, X_test, y_test, type=\"union\")\n",
    "        # for regressor, mdl_name in ml_list(SEED, X_test_pp, y_test_pp):\n",
    "        #     model_name = mdl_name + \"-\" + preprocessing.__name__ + \"-\" + str(SEED)\n",
    "        #     if os.path.isfile(  \"results/\" + dataset_name + \"/\" + model_name + '.csv'):\n",
    "        #        # print(\"Skipping\", model_name)\n",
    "        #         continue\n",
    "        #     transformers = (y_scaler, transformer_pipeline, regressor)\n",
    "        #     evaluate_pipeline(desc, model_name, data, transformers)\n",
    "\n",
    "    #########################\n",
    "\n",
    "\n",
    "\n",
    "    #########################\n",
    "    # ### CROSS VALIDATION TRAINING\n",
    "    # cv_predictions = {}\n",
    "    # for preprocessing in preprocessing_list():\n",
    "    #     fold = RepeatedKFold(n_splits=5, n_repeats=2, random_state=SEED)\n",
    "    #     fold_index = 0\n",
    "    #     for train_index, test_index in fold.split(X):\n",
    "    #         X_train, y_train, X_test, y_test = X[train_index], y[train_index], X[test_index], y[test_index]\n",
    "    #         data = (X_train, y_train, X_valid, y_valid)\n",
    "            \n",
    "    #         ##### DEEP LEARNING #####\n",
    "    #         X_test_pp, y_test_pp, transformer_pipeline, y_scaler = transform_test_data(preprocessing, X_train, y_train, X_test, y_test, type=\"augmentation\")\n",
    "    #         for model_desc in nn_list():\n",
    "    #             model_name = model_desc.__name__ + \"-\" + preprocessing.__name__  + \"-\" + str(SEED)\n",
    "    #             fold_name = model_name + \"-F\" + str(fold_index)\n",
    "    #             if os.path.isfile(  \"results/\" + dataset_name + \"/\" + fold_name + '.csv'):\n",
    "    #                # print(\"Skipping\", model_name)\n",
    "    #                 continue\n",
    "    #             regressor = get_keras_model(dataset_name + '_' + fold_name, model_desc, 7500, 750, X_test_pp, y_test_pp, verbose=0, seed=SEED)\n",
    "    #             y_pred = evaluate_pipeline(desc, fold_name, data, (y_scaler, transformer_pipeline, regressor))\n",
    "    #             cv_predictions[model_name] = cv_predictions[model_name] + y_pred if model_name in cv_predictions else y_pred\n",
    "            \n",
    "    #         ##### MACHINE LEARNING #####\n",
    "    #         X_test_pp, y_test_pp, transformer_pipeline, y_scaler = transform_test_data(preprocessing, X_train, y_train, X_test, y_test, type=\"union\")\n",
    "    #         for regressor, mdl_name in ml_list(SEED, X_test_pp, y_test_pp):\n",
    "    #             model_name = mdl_name + \"-\" + preprocessing.__name__ + \"-\" + str(SEED)\n",
    "    #             fold_name = model_name + \"-F\" + str(fold_index)\n",
    "    #             if os.path.isfile(  \"results/\" + dataset_name + \"/\" + fold_name + '.csv'):\n",
    "    #              #  print(\"Skipping\", model_name)\n",
    "    #                 continue\n",
    "    #             y_pred = evaluate_pipeline(desc, fold_name, data, (y_scaler, transformer_pipeline, regressor))\n",
    "    #             cv_predictions[model_name] = cv_predictions[model_name] + y_pred if model_name in cv_predictions else y_pred\n",
    "                \n",
    "    #         fold_index +=1\n",
    "\n",
    "    # for key, val in cv_predictions.items():\n",
    "    #     y_pred = val / fold.get_n_splits()\n",
    "    #     datasheet = get_datasheet(dataset_name, key, path, SEED, y_valid, y_pred)\n",
    "    #     results[key +\"_CV\"] = datasheet\n",
    "    #\n",
    "    # results = OrderedDict(sorted(results.items(), key=lambda k_v: float(k_v[1]['RMSE'])))\n",
    "    # with open(global_result_file, 'w') as fp:\n",
    "    #     json.dump(results, fp, indent=4)\n",
    "\n",
    "    # #########################\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "========== ALPINE_CALPINE_424_MURGUZUR (272, 2151) (272,) (152, 2151) (152,) ==========\n",
      "========== ALPINE_C_424_MURGUZUR (361, 2151) (361,) (63, 2151) (63,) ==========\n",
      "========== ALPINE_NALPINE_552_MURGUZUR (326, 2151) (326,) (37, 2151) (37,) ==========\n",
      "========== ALPINE_N_552_MURGUZUR (470, 2151) (470,) (82, 2151) (82,) ==========\n",
      "========== ALPINE_PALPINE_291_MURGUZUR (158, 2151) (158,) (133, 2151) (133,) ==========\n",
      "========== ALPINE_P_291_MURGUZUR (248, 2151) (248,) (43, 2151) (43,) ==========\n",
      "========== CASSAVA_TBC_3393_SANCHEZ "
     ]
    },
    {
     "ename": "WrongFormatError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mWrongFormatError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_21972/4186551710.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfolder\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfolder_list\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[0mbenchmark_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfolder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSEED\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_21972/1567229967.py\u001b[0m in \u001b[0;36mbenchmark_dataset\u001b[1;34m(path, SEED)\u001b[0m\n\u001b[0;32m    112\u001b[0m     \u001b[0mdesc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglobal_result_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSEED\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m     \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_valid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_valid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    115\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_valid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"=\"\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Workspace\\ML\\DECON\\data.py\u001b[0m in \u001b[0;36mload_data\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mprojdir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mfiles\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprojdir\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"*Xcal*\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"*Ycal*\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"*Xval*\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"*Yval*\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_hdr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_hdr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m';'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[0mX_valid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_valid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_hdr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_hdr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m';'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_valid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Workspace\\ML\\pynirsENV\\lib\\site-packages\\pinard\\utils.py\u001b[0m in \u001b[0;36mload_csv\u001b[1;34m(x_fname, y_fname, y_cols, sep, x_hdr, y_hdr, x_index_col, y_index_col, remove_na)\u001b[0m\n\u001b[0;32m     97\u001b[0m             \u001b[0my_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0my_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mWrongFormatError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m         \u001b[0my_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mWrongFormatError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## Browse path and launch benchmark for every folders\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "rootdir = Path('data/regression')\n",
    "folder_list = [f for f in rootdir.glob('**/*') if f.is_dir()]\n",
    "\n",
    "SEED = ord('D') + 31373\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "for folder in folder_list:\n",
    "    if \"ALPINE\" in str(folder):\n",
    "        continue\n",
    "    benchmark_dataset(folder, SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### FAST GPU RESET ####\n",
    "from numba import cuda \n",
    "device = cuda.get_current_device()\n",
    "device.reset()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.9 ('pynirsENV')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7b09f6e5407ec4329146609a0cb08cbbe4720f97bb26598a93c421b663bd10d2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
