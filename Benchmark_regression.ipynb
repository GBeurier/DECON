{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies\n",
    "\n",
    "Install dependencies not available on Google Collab.\n",
    "Collab provides numpy, pandas, sklearn, tensorflow, scipy, etc. (see requirements.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pinard\n",
    "!pip install scikeras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark details\n",
    "\n",
    "The results aggregate the combination of the following trainings configurations:\n",
    "- estimation configuration: [regression, classification]\n",
    "- datasets configurations: [Single Train, Cross validation with 5 folds and 2 repeats, Augmented Single Train]\n",
    "- preprocessing configuration: [flat spectrum, savgol, haar, [small set], [big_set]]\n",
    "- models: \n",
    "   - for all configuration: BACON, BACON-VG, DECON, PLS(components from 1 to 100), XGBoost, LW-PLS\n",
    "   - for single train + small_set : Stack > [ BACON, BACON-VG, DECON, PLS(components from 1 to 100), XGBoost, LW-PLS,\n",
    "   f_PLSRegression,f_AdaBoostRegressor,f_BaggingRegressor,f_ExtraTreesRegressor, f_GradientBoostingRegressor,f_RandomForestRegressor,\n",
    "   f_ARDRegression,f_BayesianRidge,f_ElasticNet,f_ElasticNetCV,f_HuberRegressor, f_LarsCV,f_LassoCV,f_Lasso,f_LassoLars,f_LassoLarsCV,\n",
    "   f_LassoLarsIC,f_LinearRegression,f_OrthogonalMatchingPursuit,f_OrthogonalMatchingPursuitCV, f_PassiveAggressiveRegressor,f_RANSACRegressor,\n",
    "   f_Ridge,f_RidgeCV,f_SGDRegressor,f_TheilSenRegressor,f_GaussianProcessRegressor,f_KNeighborsRegressor, f_Pipeline,f_MLPRegressor,f_LinearSVR,\n",
    "   f_NuSVR,f_SVR,f_DecisionTreeRegressor,f_ExtraTreeRegressor,f_KernelRidge,f_XGBRegressor]\n",
    "\n",
    "We perform training in 2 steps, (1) data transformation and (2) training because the sklearn pipeline does not use test data natively.\n",
    "To change with pinard update in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "\n",
    "from contextlib import redirect_stdout\n",
    "# import joblib\n",
    "# import pickle\n",
    "\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.metrics \\\n",
    "    import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error,\\\n",
    "        r2_score, explained_variance_score, mean_squared_log_error, median_absolute_error\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "import tensorflow as tf\n",
    "\n",
    "from data import load_data\n",
    "from preprocessings import preprocessing_list, transform_test_data\n",
    "from regressors import nn_list, ml_list, get_keras_model\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "\n",
    "def get_datasheet(dataset_name, model_name, path, SEED, y_valid, y_pred):\n",
    "    return {\n",
    "        \"model\":model_name, \n",
    "        \"dataset\":dataset_name,\n",
    "        \"seed\":str(SEED),\n",
    "        \"targetRMSE\":str(float(os.path.split(path)[-1].split('_')[-1].split(\"RMSE\")[-1])),\n",
    "        \"RMSE\":str(mean_squared_error(y_valid, y_pred, squared=True)),\n",
    "        \"MAPE\":str(mean_absolute_percentage_error(y_valid, y_pred)),\n",
    "        \"R2\":str(r2_score(y_valid, y_pred)),\n",
    "        \"MAE\":str(mean_absolute_error(y_valid, y_pred)),\n",
    "        \"MSE\":str(mean_squared_error(y_valid, y_pred, squared=False)),\n",
    "        \"MedAE\":str(median_absolute_error(y_valid, y_pred)),\n",
    "        \"EVS\":str(explained_variance_score(y_valid, y_pred)),\n",
    "        \"MSLE\":str(mean_squared_log_error(y_valid, y_pred)),\n",
    "        \"run\":datetime.datetime.now().strftime(\"%Y-%m-%d  %H:%M:%S\")\n",
    "    }\n",
    "\n",
    "def log_run(dataset_name, model_name, path, SEED, y_valid, y_pred):\n",
    "    datasheet = get_datasheet(dataset_name, model_name, path, SEED, y_valid, y_pred)\n",
    "    ### Save data\n",
    "    folder = \"results/\" + dataset_name\n",
    "    if not os.path.isdir(folder):\n",
    "        os.makedirs(folder)\n",
    "\n",
    "    canon_name = folder + \"/\" + model_name\n",
    "\n",
    "        ## save predictions\n",
    "    np.savetxt(canon_name + '.csv', np.column_stack((y_valid, y_pred)))\n",
    "\n",
    "    ## save main metrics globally\n",
    "    result_file = open(folder + \"/_runs.txt\", \"a\")\n",
    "    log = datasheet[\"RMSE\"] + \"  ---  \" + model_name + ' '*10 + datetime.datetime.now().strftime(\"%Y-%m-%d  %H:%M:%S\") + '\\n'\n",
    "    result_file.write(log)\n",
    "    result_file.close()\n",
    "\n",
    "    ## save pipeline\n",
    "    # joblib.dump(estimator, canon_name + '.pkl')\n",
    "\n",
    "    return datasheet\n",
    "\n",
    "\n",
    "def evaluate_pipeline(desc, model_name, data, transformers):\n",
    "    print(\"<\", model_name, \">\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Unpack args\n",
    "    X_train, y_train, X_valid, y_valid = data\n",
    "    dataset_name, path, global_result_file, results, SEED = desc\n",
    "    y_scaler, transformer_pipeline, regressor = transformers\n",
    "\n",
    "    # Construct pipeline\n",
    "    pipeline = Pipeline([\n",
    "        ('transformation', transformer_pipeline), \n",
    "        (model_name, regressor)\n",
    "    ])\n",
    "\n",
    "    # Fit estimator\n",
    "    estimator = TransformedTargetRegressor(regressor = pipeline, transformer = y_scaler)\n",
    "    estimator.fit(X_train, y_train)  \n",
    "    # Evaluate estimator\n",
    "    y_pred = estimator.predict(X_valid)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    datasheet = log_run(dataset_name, model_name, path, SEED, y_valid, y_pred)\n",
    "    datasheet[\"training_time\"] = time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time))\n",
    "    results[model_name] = datasheet\n",
    "\n",
    "    # Save results\n",
    "    results = OrderedDict(sorted(results.items(), key=lambda k_v: float(k_v[1]['RMSE'])))\n",
    "    with open(global_result_file, 'w') as fp:\n",
    "        json.dump(results, fp, indent=4)\n",
    "\n",
    "    print(datasheet[\"RMSE\"], \" (\", datasheet[\"targetRMSE\"], \") in\", datasheet[\"training_time\"])\n",
    "    return y_pred\n",
    "\n",
    "\n",
    "def benchmark_dataset(path, SEED):\n",
    "    dataset_name = ('_').join(os.path.split(path)[-1].split('_')[:-1])\n",
    "    global_result_file = \"results/\" + dataset_name + '_results.json'\n",
    "    results = {}\n",
    "    if os.path.isfile(global_result_file):\n",
    "        with open(global_result_file) as json_file:\n",
    "            results = json.load(json_file)\n",
    "   \n",
    "    desc = (dataset_name, path, global_result_file, results, SEED)\n",
    "\n",
    "    X, y, X_valid, y_valid = load_data(path)\n",
    "    print(\"=\"*10, str(dataset_name).upper(), X.shape, y.shape, X_valid.shape, y_valid.shape, \"=\"*10)\n",
    "    \n",
    "\n",
    "    #########################\n",
    "    ### SINGLE RUN TRAINING\n",
    "    X_train, y_train, X_test, y_test = X, y, X_valid, y_valid\n",
    "    data = (X_train, y_train, X_valid, y_valid)\n",
    "    for preprocessing in preprocessing_list():            \n",
    "        ##### DEEP LEARNING #####\n",
    "        X_test_pp, y_test_pp, transformer_pipeline, y_scaler = transform_test_data(preprocessing, X_train, y_train, X_test, y_test, type=\"augmentation\")\n",
    "        for model_desc in nn_list():\n",
    "            model_name = model_desc.__name__ + \"-\" + preprocessing.__name__ + \"-\" + str(SEED)\n",
    "            if os.path.isfile(  \"results/\" + dataset_name + \"/\" + model_name + '.csv'):\n",
    "                # print(\"Skipping\", model_name)\n",
    "                continue\n",
    "\n",
    "            regressor = get_keras_model(dataset_name + '_' + model_name, model_desc, 7500, 750, X_test_pp, y_test_pp, verbose=0, seed=SEED)\n",
    "            transformers = (y_scaler, transformer_pipeline, regressor)\n",
    "            evaluate_pipeline(desc, model_name, data, transformers)\n",
    "        \n",
    "        # ##### MACHINE LEARNING #####\n",
    "        # X_test_pp, y_test_pp, transformer_pipeline, y_scaler = transform_test_data(preprocessing, X_train, y_train, X_test, y_test, type=\"union\")\n",
    "        # for regressor, mdl_name in ml_list(SEED, X_test_pp, y_test_pp):\n",
    "        #     model_name = mdl_name + \"-\" + preprocessing.__name__ + \"-\" + str(SEED)\n",
    "        #     if os.path.isfile(  \"results/\" + dataset_name + \"/\" + model_name + '.csv'):\n",
    "        #        # print(\"Skipping\", model_name)\n",
    "        #         continue\n",
    "        #     transformers = (y_scaler, transformer_pipeline, regressor)\n",
    "        #     evaluate_pipeline(desc, model_name, data, transformers)\n",
    "\n",
    "    #########################\n",
    "\n",
    "\n",
    "\n",
    "    #########################\n",
    "    # ### CROSS VALIDATION TRAINING\n",
    "    # cv_predictions = {}\n",
    "    # for preprocessing in preprocessing_list():\n",
    "    #     fold = RepeatedKFold(n_splits=5, n_repeats=2, random_state=SEED)\n",
    "    #     fold_index = 0\n",
    "    #     for train_index, test_index in fold.split(X):\n",
    "    #         X_train, y_train, X_test, y_test = X[train_index], y[train_index], X[test_index], y[test_index]\n",
    "    #         data = (X_train, y_train, X_valid, y_valid)\n",
    "            \n",
    "    #         ##### DEEP LEARNING #####\n",
    "    #         X_test_pp, y_test_pp, transformer_pipeline, y_scaler = transform_test_data(preprocessing, X_train, y_train, X_test, y_test, type=\"augmentation\")\n",
    "    #         for model_desc in nn_list():\n",
    "    #             model_name = model_desc.__name__ + \"-\" + preprocessing.__name__  + \"-\" + str(SEED)\n",
    "    #             fold_name = model_name + \"-F\" + str(fold_index)\n",
    "    #             if os.path.isfile(  \"results/\" + dataset_name + \"/\" + fold_name + '.csv'):\n",
    "    #                # print(\"Skipping\", model_name)\n",
    "    #                 continue\n",
    "    #             regressor = get_keras_model(dataset_name + '_' + fold_name, model_desc, 7500, 750, X_test_pp, y_test_pp, verbose=0, seed=SEED)\n",
    "    #             y_pred = evaluate_pipeline(desc, fold_name, data, (y_scaler, transformer_pipeline, regressor))\n",
    "    #             cv_predictions[model_name] = cv_predictions[model_name] + y_pred if model_name in cv_predictions else y_pred\n",
    "            \n",
    "    #         ##### MACHINE LEARNING #####\n",
    "    #         X_test_pp, y_test_pp, transformer_pipeline, y_scaler = transform_test_data(preprocessing, X_train, y_train, X_test, y_test, type=\"union\")\n",
    "    #         for regressor, mdl_name in ml_list(SEED, X_test_pp, y_test_pp):\n",
    "    #             model_name = mdl_name + \"-\" + preprocessing.__name__ + \"-\" + str(SEED)\n",
    "    #             fold_name = model_name + \"-F\" + str(fold_index)\n",
    "    #             if os.path.isfile(  \"results/\" + dataset_name + \"/\" + fold_name + '.csv'):\n",
    "    #              #  print(\"Skipping\", model_name)\n",
    "    #                 continue\n",
    "    #             y_pred = evaluate_pipeline(desc, fold_name, data, (y_scaler, transformer_pipeline, regressor))\n",
    "    #             cv_predictions[model_name] = cv_predictions[model_name] + y_pred if model_name in cv_predictions else y_pred\n",
    "                \n",
    "    #         fold_index +=1\n",
    "\n",
    "    # for key, val in cv_predictions.items():\n",
    "    #     y_pred = val / fold.get_n_splits()\n",
    "    #     datasheet = get_datasheet(dataset_name, key, path, SEED, y_valid, y_pred)\n",
    "    #     results[key +\"_CV\"] = datasheet\n",
    "    #\n",
    "    # results = OrderedDict(sorted(results.items(), key=lambda k_v: float(k_v[1]['RMSE'])))\n",
    "    # with open(global_result_file, 'w') as fp:\n",
    "    #     json.dump(results, fp, indent=4)\n",
    "\n",
    "    # #########################\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Browse path and launch benchmark for every folders\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "rootdir = Path('data/regression')\n",
    "folder_list = [f for f in rootdir.glob('**/*') if f.is_dir()]\n",
    "\n",
    "SEED = ord('D') + 31373\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "for folder in folder_list:\n",
    "    benchmark_dataset(folder, SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### FAST GPU RESET ####\n",
    "from numba import cuda \n",
    "device = cuda.get_current_device()\n",
    "device.reset()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.9 ('pynirsENV')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7b09f6e5407ec4329146609a0cb08cbbe4720f97bb26598a93c421b663bd10d2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
