{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies\n",
    "\n",
    "Install dependencies not available on Google Collab.\n",
    "Collab provides numpy, pandas, sklearn, tensorflow, scipy, etc. (see requirements.txt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pinard\n",
    "%pip install scikeras"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark details\n",
    "\n",
    "The results aggregate the combination of the following trainings configurations:\n",
    "\n",
    "- estimation configuration: [regression, classification]\n",
    "- datasets configurations: [Single Train, Cross validation with 5 folds and 2 repeats, Augmented Single Train]\n",
    "- preprocessing configuration: [flat spectrum, savgol, haar, [small set], [big_set]]\n",
    "- models:\n",
    "  - for all configuration: BACON, BACON-VG, DECON, PLS(components from 1 to 100), XGBoost, LW-PLS\n",
    "  - for single train + small_set : Stack > [ BACON, BACON-VG, DECON, PLS(components from 1 to 100), XGBoost, LW-PLS,\n",
    "    f_PLSRegression,f_AdaBoostRegressor,f_BaggingRegressor,f_ExtraTreesRegressor, f_GradientBoostingRegressor,f_RandomForestRegressor,\n",
    "    f_ARDRegression,f_BayesianRidge,f_ElasticNet,f_ElasticNetCV,f_HuberRegressor, f_LarsCV,f_LassoCV,f_Lasso,f_LassoLars,f_LassoLarsCV,\n",
    "    f_LassoLarsIC,f_LinearRegression,f_OrthogonalMatchingPursuit,f_OrthogonalMatchingPursuitCV, f_PassiveAggressiveRegressor,f_RANSACRegressor,\n",
    "    f_Ridge,f_RidgeCV,f_SGDRegressor,f_TheilSenRegressor,f_GaussianProcessRegressor,f_KNeighborsRegressor, f_Pipeline,f_MLPRegressor,f_LinearSVR,\n",
    "    f_NuSVR,f_SVR,f_DecisionTreeRegressor,f_ExtraTreeRegressor,f_KernelRidge,f_XGBRegressor]\n",
    "\n",
    "We perform training in 2 steps, (1) data transformation and (2) training because the sklearn pipeline does not use test data natively.\n",
    "To change with pinard update in the future.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### FAST GPU RESET ####\n",
    "from numba import cuda\n",
    "device = cuda.get_current_device()\n",
    "device.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "adding folder data/_RefSet\\ALPINE_Calpine_424_Murguzur_RMSE1.36\n",
      "========== ALPINE_CALPINE_424_MURGUZUR_RMSE1.36 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'root - 2023-06-08 19:06:37,381 [WARNING] Delimiter correction for data\\_RefSet\\ALPINE_Calpine_424_Murguzur_RMSE1.36\\Ycal.csv.gz: \\n\n",
      "'root - 2023-06-08 19:06:37,385 [WARNING] No y file y_test - *Ytest* found for ALPINE_Calpine_424_Murguzur_RMSE1.36.\n",
      "'root - 2023-06-08 19:06:37,386 [WARNING] Delimiter correction for data\\_RefSet\\ALPINE_Calpine_424_Murguzur_RMSE1.36\\Yval.csv.gz: \\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== (272, 2151) (272, 1) (152, 2151) (152, 1)\n",
      "Finished ALPINE_Calpine_424_Murguzur_RMSE1.36 with 5 results\n",
      "adding folder data/_RefSet\\ALPINE_C_424_Murguzur_RMSE1.16\n",
      "========== ALPINE_C_424_MURGUZUR_RMSE1.16 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'root - 2023-06-08 19:06:38,451 [WARNING] Delimiter correction for data\\_RefSet\\ALPINE_C_424_Murguzur_RMSE1.16\\Ycal.csv.gz: \\n\n",
      "'root - 2023-06-08 19:06:38,455 [WARNING] No y file y_test - *Ytest* found for ALPINE_C_424_Murguzur_RMSE1.16.\n",
      "'root - 2023-06-08 19:06:38,457 [WARNING] Delimiter correction for data\\_RefSet\\ALPINE_C_424_Murguzur_RMSE1.16\\Yval.csv.gz: \\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== (361, 2151) (361, 1) (63, 2151) (63, 1)\n",
      "Finished ALPINE_C_424_Murguzur_RMSE1.16 with 5 results\n",
      "adding folder data/_RefSet\\ALPINE_Nalpine_552_Murguzur_RMSE0.38\n",
      "========== ALPINE_NALPINE_552_MURGUZUR_RMSE0.38 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'root - 2023-06-08 19:06:39,374 [WARNING] Delimiter correction for data\\_RefSet\\ALPINE_Nalpine_552_Murguzur_RMSE0.38\\Ycal.csv.gz: \\n\n",
      "'root - 2023-06-08 19:06:39,379 [WARNING] No y file y_test - *Ytest* found for ALPINE_Nalpine_552_Murguzur_RMSE0.38.\n",
      "'root - 2023-06-08 19:06:39,380 [WARNING] Delimiter correction for data\\_RefSet\\ALPINE_Nalpine_552_Murguzur_RMSE0.38\\Yval.csv.gz: \\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== (326, 2151) (326, 1) (37, 2151) (37, 1)\n",
      "Finished ALPINE_Nalpine_552_Murguzur_RMSE0.38 with 5 results\n",
      "adding folder data/_RefSet\\ALPINE_N_552_Murguzur_RMSE0.2\n",
      "========== ALPINE_N_552_MURGUZUR_RMSE0.2 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'root - 2023-06-08 19:06:40,614 [WARNING] Delimiter correction for data\\_RefSet\\ALPINE_N_552_Murguzur_RMSE0.2\\Ycal.csv.gz: \\n\n",
      "'root - 2023-06-08 19:06:40,619 [WARNING] No y file y_test - *Ytest* found for ALPINE_N_552_Murguzur_RMSE0.2.\n",
      "'root - 2023-06-08 19:06:40,620 [WARNING] Delimiter correction for data\\_RefSet\\ALPINE_N_552_Murguzur_RMSE0.2\\Yval.csv.gz: \\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== (470, 2151) (470, 1) (82, 2151) (82, 1)\n",
      "Finished ALPINE_N_552_Murguzur_RMSE0.2 with 5 results\n",
      "adding folder data/_RefSet\\ALPINE_Palpine_291_Murguzur_RMSE0.13\n",
      "========== ALPINE_PALPINE_291_MURGUZUR_RMSE0.13 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'root - 2023-06-08 19:06:41,442 [WARNING] Delimiter correction for data\\_RefSet\\ALPINE_Palpine_291_Murguzur_RMSE0.13\\Ycal.csv.gz: \\n\n",
      "'root - 2023-06-08 19:06:41,447 [WARNING] No y file y_test - *Ytest* found for ALPINE_Palpine_291_Murguzur_RMSE0.13.\n",
      "'root - 2023-06-08 19:06:41,448 [WARNING] Delimiter correction for data\\_RefSet\\ALPINE_Palpine_291_Murguzur_RMSE0.13\\Yval.csv.gz: \\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== (158, 2151) (158, 1) (133, 2151) (133, 1)\n",
      "Finished ALPINE_Palpine_291_Murguzur_RMSE0.13 with 5 results\n",
      "adding folder data/_RefSet\\ALPINE_P_291_Murguzur_RMSE0.05\n",
      "========== ALPINE_P_291_MURGUZUR_RMSE0.05 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'root - 2023-06-08 19:06:42,343 [WARNING] Delimiter correction for data\\_RefSet\\ALPINE_P_291_Murguzur_RMSE0.05\\Ycal.csv.gz: \\n\n",
      "'root - 2023-06-08 19:06:42,348 [WARNING] No y file y_test - *Ytest* found for ALPINE_P_291_Murguzur_RMSE0.05.\n",
      "'root - 2023-06-08 19:06:42,350 [WARNING] Delimiter correction for data\\_RefSet\\ALPINE_P_291_Murguzur_RMSE0.05\\Yval.csv.gz: \\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== (248, 2151) (248, 1) (43, 2151) (43, 1)\n",
      "Finished ALPINE_P_291_Murguzur_RMSE0.05 with 5 results\n",
      "adding folder data/_RefSet\\Cassava_TBC_3393_Sanchez_RMSE1.12\n",
      "========== CASSAVA_TBC_3393_SANCHEZ_RMSE1.12 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'root - 2023-06-08 19:06:44,458 [WARNING] Delimiter correction for data\\_RefSet\\Cassava_TBC_3393_Sanchez_RMSE1.12\\Ycal.csv.gz: \\n\n",
      "'root - 2023-06-08 19:06:44,467 [WARNING] No y file y_test - *Ytest* found for Cassava_TBC_3393_Sanchez_RMSE1.12.\n",
      "'root - 2023-06-08 19:06:44,468 [WARNING] Delimiter correction for data\\_RefSet\\Cassava_TBC_3393_Sanchez_RMSE1.12\\Yval.csv.gz: \\n\n",
      "'root - 2023-06-08 19:06:44,494 [WARNING] Removing rows: [   0    1    3    5    7   23   24   28   29   30   33   42   44   52\n",
      "   60   62   71   87   91   96  138  141  150  153  166  170  176  191\n",
      "  193  200  203  205  209  222  229  235  236  245  264  267  281  288\n",
      "  291  308  332  336  367  371  380  396  405  421  432  435  455  462\n",
      "  468  470  474  482  493  495  502  506  508  530  539  558  561  584\n",
      "  588  593  598  603  612  626  630  638  650  654  688  697  742  782\n",
      "  823  832  916  933  946 1002 1005 1011 1102 1128 1142 1146 1160 1220\n",
      " 1229 1239 1270 1282 1293 1325 1355 1361 1366 1368 1377 1381 1391 1393\n",
      " 1398 1399 1400 1403 1418 1434 1439 1441 1447 1457 1458 1468 1471 1480\n",
      " 1491 1509 1516 1526 1533 1536 1543 1549 1552 1554 1562 1568 1569 1574\n",
      " 1585 1592 1596 1607 1610 1614 1636 1639 1644 1648 1652 1654 1657 1670\n",
      " 1680 1681 1684 1701 1702 1707 1710 1713 1718 1719 1741 1749 1751 1755\n",
      " 1768 1778 1781 1788 1790 1793 1796 1802 1813 1831 1849 1859 1861 1877\n",
      " 1880 1887 1904 1909 1916 1920 1923 1934 1945 1948 1952 1970 1977 1995\n",
      " 1997 2004 2014] in Cassava_TBC_3393_Sanchez_RMSE1.12 X_train\n",
      "'root - 2023-06-08 19:06:44,509 [WARNING] Removing rows: [   3    6   10   47   53   55   65   85   88  110  116  135  153  157\n",
      "  174  175  188  212  254  260  273  298  304  319  333  344  354  357\n",
      "  366  367  369  384  405  419  432  437  462  463  469  477  479  484\n",
      "  487  531  558  572  576  586  600  604  625  651  660  667  674  683\n",
      "  715  753  759  786  802  816  830  846  859  860  864  866  876  893\n",
      "  906  908  935  951  962  990  991 1001 1011 1024 1028 1037 1049 1065\n",
      " 1077 1095 1105 1112 1150 1152 1233 1246 1252 1258 1263 1264 1284 1303\n",
      " 1310 1314 1337 1363] in Cassava_TBC_3393_Sanchez_RMSE1.12 X_val\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== (1823, 1050) (1823, 1) (1270, 1050) (1270, 1)\n",
      "Finished Cassava_TBC_3393_Sanchez_RMSE1.12 with 5 results\n",
      "adding folder data/_RefSet\\Cassava_TBC_3432_Shen_RMSE1.16\n",
      "========== CASSAVA_TBC_3432_SHEN_RMSE1.16 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'root - 2023-06-08 19:06:47,436 [WARNING] Delimiter correction for data\\_RefSet\\Cassava_TBC_3432_Shen_RMSE1.16\\Ycal.csv.gz: \\n\n",
      "'root - 2023-06-08 19:06:47,444 [WARNING] No y file y_test - *Ytest* found for Cassava_TBC_3432_Shen_RMSE1.16.\n",
      "'root - 2023-06-08 19:06:47,446 [WARNING] Delimiter correction for data\\_RefSet\\Cassava_TBC_3432_Shen_RMSE1.16\\Yval.csv.gz: \\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== (3093, 1050) (3093, 1) (732, 1050) (732, 1)\n",
      "Finished Cassava_TBC_3432_Shen_RMSE1.16 with 5 results\n",
      "adding folder data/_RefSet\\Cassava_TBC_3556_Davrieux_RMSE1.02\n",
      "========== CASSAVA_TBC_3556_DAVRIEUX_RMSE1.02 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'root - 2023-06-08 19:06:50,010 [WARNING] Delimiter correction for data\\_RefSet\\Cassava_TBC_3556_Davrieux_RMSE1.02\\Ycal.csv.gz: \\n\n",
      "'root - 2023-06-08 19:06:50,018 [WARNING] No y file y_test - *Ytest* found for Cassava_TBC_3556_Davrieux_RMSE1.02.\n",
      "'root - 2023-06-08 19:06:50,019 [WARNING] Delimiter correction for data\\_RefSet\\Cassava_TBC_3556_Davrieux_RMSE1.02\\Yval.csv.gz: \\n\n",
      "'root - 2023-06-08 19:06:50,037 [WARNING] Removing rows: [0] in Cassava_TBC_3556_Davrieux_RMSE1.02 X_val\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== (3368, 1050) (3368, 1) (461, 1050) (461, 1)\n",
      "Finished Cassava_TBC_3556_Davrieux_RMSE1.02 with 5 results\n",
      "adding folder data/_RefSet\\Cassava_TTC_3393_Sanchez_RMSE1.8\n",
      "========== CASSAVA_TTC_3393_SANCHEZ_RMSE1.8 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'root - 2023-06-08 19:06:52,200 [WARNING] Delimiter correction for data\\_RefSet\\Cassava_TTC_3393_Sanchez_RMSE1.8\\Ycal.csv.gz: \\n\n",
      "'root - 2023-06-08 19:06:52,209 [WARNING] No y file y_test - *Ytest* found for Cassava_TTC_3393_Sanchez_RMSE1.8.\n",
      "'root - 2023-06-08 19:06:52,210 [WARNING] Delimiter correction for data\\_RefSet\\Cassava_TTC_3393_Sanchez_RMSE1.8\\Yval.csv.gz: \\n\n",
      "'root - 2023-06-08 19:06:52,233 [WARNING] Removing rows: [   0 1388 1432 1499 1567 1723 1941] in Cassava_TTC_3393_Sanchez_RMSE1.8 X_train\n",
      "'root - 2023-06-08 19:06:52,247 [WARNING] Removing rows: [ 128  225  228  256  261  325  352  726  804  853  862 1026 1064 1098\n",
      " 1164 1198 1225 1314 1341] in Cassava_TTC_3393_Sanchez_RMSE1.8 X_val\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== (2015, 1050) (2015, 1) (1353, 1050) (1353, 1)\n",
      "Finished Cassava_TTC_3393_Sanchez_RMSE1.8 with 5 results\n",
      "adding folder data/_RefSet\\Cassava_TTC_3830_Davrieux_RMSE1.38\n",
      "========== CASSAVA_TTC_3830_DAVRIEUX_RMSE1.38 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'root - 2023-06-08 19:06:54,656 [WARNING] Delimiter correction for data\\_RefSet\\Cassava_TTC_3830_Davrieux_RMSE1.38\\Ycal.csv.gz: \\n\n",
      "'root - 2023-06-08 19:06:54,664 [WARNING] No y file y_test - *Ytest* found for Cassava_TTC_3830_Davrieux_RMSE1.38.\n",
      "'root - 2023-06-08 19:06:54,666 [WARNING] Delimiter correction for data\\_RefSet\\Cassava_TTC_3830_Davrieux_RMSE1.38\\Yval.csv.gz: \\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== (3368, 1050) (3368, 1) (462, 1050) (462, 1)\n",
      "RUN Cassava_TTC_3830_Davrieux_RMSE1.38 Transformer transf_set\n",
      "--- Trainable: 9405 - untrainable: 0.0 > 9405.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'absl - 2023-06-08 19:06:56,294 [WARNING] Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, query_layer_call_fn, query_layer_call_and_return_conditional_losses, key_layer_call_fn while saving (showing 5 of 14). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** (462, 1050) (462, 1) (462, 1)        \n",
      "Epoch: 0 > RMSE: 5.1518655  ( 1.5740196 | Transformer--NoSpl-NoCV-Fold_1(1)-NoAug-small_set-31441-23-06-08_17-55-33 ) - R²: -2.356924436000692  val_loss inf\n",
      "Best so far > 5.1518655 results\\Cassava_TTC_3830_Davrieux_RMSE1.38\\Transformer--NoSpl-NoCV-Fold_1(1)-NoAug-transf_set-31441-23-06-08_19-06-54\n",
      "********** (462, 1050) (462, 1) (462, 1)0 0.041572           \n",
      "Epoch: 1 > RMSE: 2.4289222  ( 1.5740196 | Transformer--NoSpl-NoCV-Fold_1(1)-NoAug-small_set-31441-23-06-08_17-55-33 ) - R²: 0.2538265905139906  val_loss 0.04157228022813797\n",
      "Best so far > 2.4289222 results\\Cassava_TTC_3830_Davrieux_RMSE1.38\\Transformer--NoSpl-NoCV-Fold_1(1)-NoAug-transf_set-31441-23-06-08_19-06-54\n",
      "********** (462, 1050) (462, 1) (462, 1) 0.009260           \n",
      "Epoch: 2 > RMSE: 2.0126436  ( 1.5740196 | Transformer--NoSpl-NoCV-Fold_1(1)-NoAug-small_set-31441-23-06-08_17-55-33 ) - R²: 0.4876739931183056  val_loss 0.009260456077754498\n",
      "Best so far > 2.0126436 results\\Cassava_TTC_3830_Davrieux_RMSE1.38\\Transformer--NoSpl-NoCV-Fold_1(1)-NoAug-transf_set-31441-23-06-08_19-06-54\n",
      "********** (462, 1050) (462, 1) (462, 1) 0.006354           \n",
      "Epoch: 5 > RMSE: 1.8388854  ( 1.5740196 | Transformer--NoSpl-NoCV-Fold_1(1)-NoAug-small_set-31441-23-06-08_17-55-33 ) - R²: 0.5723170811967303  val_loss 0.006354489363729954\n",
      "Best so far > 1.8388854 results\\Cassava_TTC_3830_Davrieux_RMSE1.38\\Transformer--NoSpl-NoCV-Fold_1(1)-NoAug-transf_set-31441-23-06-08_19-06-54\n",
      "********** (462, 1050) (462, 1) (462, 1) 0.005306           \n",
      "Epoch: 14 > RMSE: 1.6705995  ( 1.5740196 | Transformer--NoSpl-NoCV-Fold_1(1)-NoAug-small_set-31441-23-06-08_17-55-33 ) - R²: 0.6470142141709505  val_loss 0.005306303966790438\n",
      "Best so far > 1.6705995 results\\Cassava_TTC_3830_Davrieux_RMSE1.38\\Transformer--NoSpl-NoCV-Fold_1(1)-NoAug-transf_set-31441-23-06-08_19-06-54\n",
      "********** (462, 1050) (462, 1) (462, 1)0.004375            \n",
      "Epoch: 21 > RMSE: 1.6229465  ( 1.5740196 | Transformer--NoSpl-NoCV-Fold_1(1)-NoAug-small_set-31441-23-06-08_17-55-33 ) - R²: 0.6668644171546863  val_loss 0.004374578595161438\n",
      "Best so far > 1.6229465 results\\Cassava_TTC_3830_Davrieux_RMSE1.38\\Transformer--NoSpl-NoCV-Fold_1(1)-NoAug-transf_set-31441-23-06-08_19-06-54\n",
      "********** (462, 1050) (462, 1) (462, 1) 0.004132           \n",
      "Epoch: 22 > RMSE: 1.621501  ( 1.5740196 | Transformer--NoSpl-NoCV-Fold_1(1)-NoAug-small_set-31441-23-06-08_17-55-33 ) - R²: 0.6674576081637505  val_loss 0.00413171760737896\n",
      "Best so far > 1.621501 results\\Cassava_TTC_3830_Davrieux_RMSE1.38\\Transformer--NoSpl-NoCV-Fold_1(1)-NoAug-transf_set-31441-23-06-08_19-06-54\n",
      "********** (462, 1050) (462, 1) (462, 1)0.004123            \n",
      "Epoch: 25 > RMSE: 1.6195449  ( 1.5740196 | Transformer--NoSpl-NoCV-Fold_1(1)-NoAug-small_set-31441-23-06-08_17-55-33 ) - R²: 0.6682594133043708  val_loss 0.004123446065932512\n",
      "Best so far > 1.6195449 results\\Cassava_TTC_3830_Davrieux_RMSE1.38\\Transformer--NoSpl-NoCV-Fold_1(1)-NoAug-transf_set-31441-23-06-08_19-06-54\n",
      "********** (462, 1050) (462, 1) (462, 1) 0.004110           \n",
      "Epoch: 42 > RMSE: 1.5875833  ( 1.5740196 | Transformer--NoSpl-NoCV-Fold_1(1)-NoAug-small_set-31441-23-06-08_17-55-33 ) - R²: 0.6812239642640117  val_loss 0.00411014212295413\n",
      "Best so far > 1.5875833 results\\Cassava_TTC_3830_Davrieux_RMSE1.38\\Transformer--NoSpl-NoCV-Fold_1(1)-NoAug-transf_set-31441-23-06-08_19-06-54\n",
      "epoch 00064 lr 1e-06  -  0.004547 0.003950                   \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_42908/2084034515.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfolder\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfolders\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"adding folder\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfolder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 200\u001b[1;33m     \u001b[0mbenchmark_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfolder\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msplit_configs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv_configs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maugmentations\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreprocessings_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSEED\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    201\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Workspace\\ML\\DECON\\benchmark_loop.py\u001b[0m in \u001b[0;36mbenchmark_dataset\u001b[1;34m(dataset_list, split_configs, cv_configs, augmentations, preprocessings, models, SEED, bins, resampling, resample_size, weight_config, skip_existing)\u001b[0m\n\u001b[0;32m    553\u001b[0m                                 \u001b[1;31m# print(model)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    554\u001b[0m                                 \u001b[0mtransformers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0my_scaler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransformer_pipeline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mregressor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 555\u001b[1;33m                                 \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdatasheet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluate_pipeline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdesc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_RMSE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbest_current_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdiscretizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    556\u001b[0m                                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdatasheet\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"RMSE\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\" (\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_RMSE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\") in\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdatasheet\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"training_time\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# \"|\", best_current_model,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    557\u001b[0m                                 \u001b[1;32mif\u001b[0m \u001b[0mname_cv\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m\"NoCV\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Workspace\\ML\\DECON\\benchmark_loop.py\u001b[0m in \u001b[0;36mevaluate_pipeline\u001b[1;34m(desc, model_name, data, transformers, target_RMSE, best_current_model, discretizer)\u001b[0m\n\u001b[0;32m    267\u001b[0m     \u001b[0mcurrent_y_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_valid\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    268\u001b[0m     \u001b[1;31m# print(estimator)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 269\u001b[1;33m     \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    270\u001b[0m     \u001b[1;31m# save estimator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    271\u001b[0m     \u001b[1;31m# with open(\"estimator.pkl\", \"wb\") as f:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Workspace\\ML\\DECON_ENV\\lib\\site-packages\\sklearn\\compose\\_target.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    244\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregressor_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregressor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    245\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 246\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregressor_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_trans\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    247\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    248\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregressor_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"feature_names_in_\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Workspace\\ML\\DECON_ENV\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    392\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m\"passthrough\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    393\u001b[0m                 \u001b[0mfit_params_last_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_params_steps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 394\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params_last_step\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    395\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    396\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Workspace\\ML\\DECON_ENV\\lib\\site-packages\\scikeras\\wrappers.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, **kwargs)\u001b[0m\n\u001b[0;32m    765\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    766\u001b[0m             \u001b[0mwarm_start\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarm_start\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 767\u001b[1;33m             \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    768\u001b[0m         )\n\u001b[0;32m    769\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Workspace\\ML\\DECON_ENV\\lib\\site-packages\\scikeras\\wrappers.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, y, sample_weight, warm_start, epochs, initial_epoch, **kwargs)\u001b[0m\n\u001b[0;32m    936\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    937\u001b[0m             \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 938\u001b[1;33m             \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    939\u001b[0m         )\n\u001b[0;32m    940\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Workspace\\ML\\DECON_ENV\\lib\\site-packages\\scikeras\\wrappers.py\u001b[0m in \u001b[0;36m_fit_keras_model\u001b[1;34m(self, X, y, sample_weight, warm_start, epochs, initial_epoch, **kwargs)\u001b[0m\n\u001b[0;32m    524\u001b[0m                 \u001b[0mhist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    525\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 526\u001b[1;33m             \u001b[0mhist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    527\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    528\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mwarm_start\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"history_\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0minitial_epoch\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Workspace\\ML\\DECON_ENV\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Workspace\\ML\\DECON_ENV\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1562\u001b[0m                         ):\n\u001b[0;32m   1563\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1564\u001b[1;33m                             \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1565\u001b[0m                             \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1566\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Workspace\\ML\\DECON_ENV\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Workspace\\ML\\DECON_ENV\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    914\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 915\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Workspace\\ML\\DECON_ENV\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    945\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    946\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 947\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    948\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Workspace\\ML\\DECON_ENV\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2495\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m   2496\u001b[0m     return graph_function._call_flat(\n\u001b[1;32m-> 2497\u001b[1;33m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m   2498\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2499\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Workspace\\ML\\DECON_ENV\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1861\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1862\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1863\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1864\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1865\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Workspace\\ML\\DECON_ENV\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    502\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    503\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 504\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    505\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    506\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32md:\\Workspace\\ML\\DECON_ENV\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 55\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     56\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## Browse path and launch benchmark for every folders\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pathlib import Path\n",
    "from preprocessings import preprocessing_list\n",
    "\n",
    "from benchmark_loop import benchmark_dataset\n",
    "import regressors\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.get_logger().setLevel(\"ERROR\")\n",
    "tf.keras.mixed_precision.set_global_policy(\"mixed_float16\")\n",
    "\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(\n",
    "    level= logging.WARNING,\n",
    "    format=\"'%(name)s - %(asctime)s [%(levelname)s] %(message)s\",\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"debug.log\"),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "rootdir = Path('data/regression')\n",
    "folder_list = [f for f in rootdir.glob('**/*') if f.is_dir()]\n",
    "\n",
    "SEED = ord('D') + 31373\n",
    "\n",
    "# tf.keras.utils.set_random_seed(SEED)\n",
    "# tf.config.experimental.enable_op_determinism()\n",
    "\n",
    "\n",
    "import preprocessings\n",
    "import regressors\n",
    "import pinard.preprocessing as pp\n",
    "from pinard import augmentation, model_selection\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from xgboost import XGBRegressor\n",
    "import sys\n",
    "import os.path\n",
    "\n",
    "def str_to_class(classname):\n",
    "    return getattr(sys.modules['pinard.preprocessing'], classname)\n",
    "\n",
    "# print(str_to_class('SavitzkyGolay'))\n",
    "\n",
    "\n",
    "split_configs = [\n",
    "    None,\n",
    "    # {'test_size':None, 'method':\"random\", 'random_state':SEED},\n",
    "    # {'test_size':None, 'method':\"stratified\", 'random_state':SEED, 'n_bins':5},\n",
    "    # {'test_size':0.25, 'method':\"spxy\", 'random_state':SEED, 'metric':\"euclidean\", 'pca_components':250},\n",
    "]\n",
    "\n",
    "augmentations = [\n",
    "    None,\n",
    "    # [(6, augmentation.Rotate_Translate())],\n",
    "    # [(3, augmentation.Rotate_Translate()),(2, augmentation.Random_X_Operation()),(1, augmentation.Random_Spline_Addition())],\n",
    "    # [(3, augmentation.Rotate_Translate()),(2, augmentation.Random_X_Operation()),(2, augmentation.Random_Spline_Addition())],\n",
    "]\n",
    "\n",
    "preprocessings_list = [\n",
    "    # None,\n",
    "    # [(\"id\", pp.IdentityTransformer())],\n",
    "    # [(\"baseline\", pp.StandardNormalVariate())],\n",
    "    # [(\"savgol\", pp.SavitzkyGolay())],\n",
    "    # [(\"gaussian1\", pp.Gaussian(order=1, sigma=2))],\n",
    "    # [(\"gaussian2\", pp.Gaussian(order=2, sigma=1))],\n",
    "    # [(\"haar\", pp.Wavelet(\"haar\"))],\n",
    "    # [(\"coif3\", pp.Wavelet(\"coif3\"))],\n",
    "    # [(\"detrend\", pp.Detrend())],\n",
    "    # [(\"msc\", pp.MultiplicativeScatterCorrection(scale=False))],\n",
    "    # [(\"dv1\", pp.Derivate(1, 1))],\n",
    "    # [(\"dv2\", pp.Derivate(2, 1))],\n",
    "    # [(\"dv3\", pp.Derivate(2, 2))],\n",
    "    \n",
    "    (\"small_set\", preprocessings.small_set()),\n",
    "    (\"simple\", [(\"id\", pp.IdentityTransformer()), ('haar', pp.Haar()), ('savgol', pp.SavitzkyGolay())]),\n",
    "    (\"transf_set\", preprocessings.transf_set()),\n",
    "    (\"bacon_set\", preprocessings.bacon_set()),\n",
    "    (\"decon_set\", preprocessings.decon_set()),\n",
    "    # preprocessings.decon_set(),\n",
    "    # preprocessings.optimal_set_2D(),\n",
    "    # preprocessings.fat_set(),\n",
    "]\n",
    "\n",
    "# for p in preprocessings_list:\n",
    "#     if isinstance(p, tuple):\n",
    "#         h = str(hash(frozenset(p[1])))[0:5]\n",
    "#         print(h)\n",
    "#     else:\n",
    "#         h = str(hash(frozenset(p)))[0:5]\n",
    "#         print(h)\n",
    "\n",
    "\n",
    "cv_configs = [\n",
    "    None,\n",
    "    # {'n_splits':5, 'n_repeats':4},\n",
    "    # {'n_splits':4, 'n_repeats':2},\n",
    "    # {'n_splits':3, 'n_repeats':1},\n",
    "]\n",
    "\n",
    "# import os\n",
    "# folder = \"data/regression\"\n",
    "# folder = \"data/Paprica_2D\"\n",
    "folder = \"data/_RefSet\"\n",
    "\n",
    "def get_dataset_list(path, filter=\"\"):\n",
    "    datasets = []\n",
    "    for r, d, _ in os.walk(path):\n",
    "        for folder in d:\n",
    "            path = os.path.join(r, folder)\n",
    "            if os.path.isdir(path):\n",
    "                if str(path).lower()[13] >= filter:\n",
    "                    datasets.append(str(path))\n",
    "                # break\n",
    "    return datasets\n",
    "\n",
    "folders = get_dataset_list(folder)\n",
    "# folders = folders[0:1]\n",
    "# folders = [\"data/regression/Cassava_TBC_3556_Davrieux_RMSE1.02\"]\n",
    "\n",
    "len_cv_configs = 0\n",
    "for c in cv_configs:\n",
    "    if c == None:\n",
    "        len_cv_configs += 1\n",
    "    else:\n",
    "        len_cv_configs += (c['n_splits'] * c['n_repeats'])\n",
    "\n",
    "models = [\n",
    "    # (regressors.ML_Regressor(XGBRegressor), {\"n_estimators\":200, \"max_depth\":50, \"seed\":SEED}),\n",
    "    # (regressors.ML_Regressor(PLSRegression), {\"n_components\":50}),\n",
    "\n",
    "    (regressors.Transformer(), {'batch_size': 10, 'epoch': 1000, 'verbose': 0, 'patience': 100, 'optimizer': 'adam', 'loss': 'mse', \"min_lr\": 1e-6, \"max_lr\": 1e-2, \"cycle_length\": 32}),\n",
    "    # (regressors.Transformer(), {'batch_size':15, 'epoch':400, 'verbose':0, 'patience':60, 'optimizer':'adam', 'loss':'mse'}),\n",
    "    # (regressors.Transformer_VG(), {'batch_size':15, 'epoch':400, 'verbose':0, 'patience':30, 'optimizer':'adam', 'loss':'mse'}),\n",
    "    # (regressors.Transformer_NIRS(), {'batch_size':15, 'epoch':400, 'verbose':0, 'patience':30, 'optimizer':'adam', 'loss':'mse'}),\n",
    "    \n",
    "    \n",
    "    # (regressors.FFT_Conv(), {'batch_size':1, 'epoch':10000, 'verbose':0, 'patience':300, 'optimizer':'adam', 'loss':'mse'}),\n",
    "    # (regressors.ResNetV2(), {'batch_size':1, 'epoch':10000, 'verbose':0, 'patience':300, 'optimizer':'adam', 'loss':'mse'}),\n",
    "    # (regressors.XCeption1D(), {'batch_size':1, 'epoch':10000, 'verbose':0, 'patience':600, 'optimizer':'adam', 'loss':'mse'}),\n",
    "    \n",
    "    \n",
    "    # (regressors.Decon_SepPo(), {'batch_size':50, 'epoch':10000, 'verbose':0, 'patience':1000, 'optimizer':'adam', 'loss':'mse'}),\n",
    "    # (regressors.Decon(), {'batch_size':100, 'epoch':20000, 'verbose':0, 'patience':400, 'optimizer':'adam', 'loss':'mse', \"min_lr\": 1e-6, \"max_lr\": 1e-2, \"cycle_length\": 32}),\n",
    "    # (regressors.Decon_Sep(), {'batch_size':100, 'epoch':20, 'verbose':2, 'patience':500, 'optimizer':'adam', 'loss':'mse'}),\n",
    "    # (regressors.Decon_SepPo(), {'batch_size':100, 'epoch':20000, 'verbose':2, 'patience':500, 'optimizer':'adam', 'loss':'mse'}),\n",
    "    # (regressors.Decon_Sep_VG(), {'batch_size':100, 'epoch':20000, 'verbose':2, 'patience':500, 'optimizer':'adam', 'loss':'mse'}),\n",
    "    # (regressors.MLP(), {'batch_size':1000, 'epoch':20000, 'verbose':0, 'patience':2000, 'optimizer':'adam', 'loss':'mse'}),\n",
    "    # (regressors.Bacon(), {'batch_size':100, 'epoch':10, 'verbose':0, 'patience':10, 'optimizer':'adam', 'loss':'mse'}, True),\n",
    "]\n",
    "\n",
    "\n",
    "# from lwpls import LWPLS\n",
    "# from regressors import NonlinearPLSRegressor\n",
    "\n",
    "# for i in range(5,150,5):\n",
    "#     models. append(\n",
    "#         (regressors.ML_Regressor(NonlinearPLSRegressor, name=f\"NL_RBF_PLS_{i}\"), {\"n_components\":i, \"poly_degree\":2, \"gamma\":0.1})\n",
    "#     )\n",
    "#     models. append(\n",
    "#         (regressors.ML_Regressor(PLSRegression, name=f\"PLS_{i}\"), {\"n_components\":i})\n",
    "#     )\n",
    "\n",
    "# models.append(\n",
    "#     (regressors.ML_Regressor(LWPLS, name=f\"LWPLS_0-05_45\"), {\"max_component_number\":45, \"lambda_in_similarity\":0.05})\n",
    "# )\n",
    "\n",
    "# for i in range(10,100,50):\n",
    "#     models.append(\n",
    "#         (regressors.ML_Regressor(LWPLS, name=f\"LWPLS_0-1_{i}\"), {\"max_component_number\":i, \"lambda_in_similarity\":0.05})\n",
    "#     )\n",
    "    # models.append(\n",
    "    #     (regressors.ML_Regressor(LWPLS, name=f\"LWPLS_0-5_{i}\"), {\"max_component_number\":i, \"lambda_in_similarity\":0.5})\n",
    "    # )\n",
    "# models.append( (regressors.ML_Regressor(PLSRegression, name=f\"PLS_5\"), {\"n_components\":5}) )\n",
    "\n",
    "\n",
    "# for i in range(1,20,1):\n",
    "#     models.append( (regressors.ML_Regressor(PLSRegression, name=f\"PLS_{i}\"), {\"n_components\":i}) )\n",
    "# for i in range(21,120,3):\n",
    "#     models.append( (regressors.ML_Regressor(PLSRegression, name=f\"PLS_{i}\"), {\"n_components\":i}) )\n",
    "# for i in range(1,50,10):\n",
    "#     models.append( (regressors.ML_Regressor(XGBRegressor, name=f\"XGB_{i}\"), {\"n_estimators\":i, \"max_depth\":max(30, int(i/2)), \"seed\":SEED}) )\n",
    "\n",
    "# models.append((regressors.Decon(), {'batch_size':100, 'epoch':20000, 'verbose':0, 'patience':400, 'optimizer':'adam', 'loss':'mse'}))\n",
    "# benchmark_size = len(folders) * len(split_configs) * len_cv_configs * len(augmentations) * len(preprocessings_list) * len(models)\n",
    "# print(\"Benchmarking\", benchmark_size, \"runs.\")\n",
    "# benchmark_dataset(folders[:-1], split_configs, cv_configs, augmentations, preprocessings_list, models, SEED) #, resampling='resample', resample_size=2048) #bins=5)\n",
    "\n",
    "\n",
    "# benchmark_dataset([folders[12]], split_configs, cv_configs, augmentations, preprocessings_list, models, SEED)\n",
    "for folder in folders[:-1]:\n",
    "    print(\"adding folder\", folder)\n",
    "    benchmark_dataset([folder], split_configs, cv_configs, augmentations, preprocessings_list, models, SEED)\n",
    "\n",
    "\n",
    "# for folder in folders[:-1]:\n",
    "#     print(\"adding folder\", folder)\n",
    "#     benchmark_dataset([folders[i]], split_configs, cv_configs, augmentations, preprocessings_list, [(regressors.CONV_LSTM(), {'batch_size':10, 'epoch':1, 'verbose':0, 'patience':2000, 'optimizer':'adam', 'loss':'mse'}),], SEED)\n",
    "    \n",
    "\n",
    "# import threading\n",
    "# import queue\n",
    "\n",
    "# def worker(q):\n",
    "#     while True:\n",
    "#         item = q.get()\n",
    "#         if item is None:\n",
    "#             break\n",
    "#         # process item\n",
    "#         # print(f'Processing {item}')\n",
    "#         benchmark_dataset(*item)\n",
    "#         q.task_done()\n",
    "\n",
    "# q = queue.Queue()\n",
    "# num_worker_threads = 1\n",
    "# threads = []\n",
    "# for i in range(num_worker_threads):\n",
    "#     t = threading.Thread(target=worker, args=(q,))\n",
    "#     t.start()\n",
    "#     threads.append(t)\n",
    "\n",
    "# # add items to the queue\n",
    "# for folder in folders[:-1]:\n",
    "#     print(\"adding folder\", folder)\n",
    "#     q.put(([folder], split_configs, cv_configs, augmentations, preprocessings_list, models, SEED))\n",
    "\n",
    "# # block until all tasks are done\n",
    "# q.join()\n",
    "\n",
    "# # stop workers\n",
    "# for i in range(num_worker_threads):\n",
    "#     q.put(None)\n",
    "\n",
    "\n",
    "# for t in threads:\n",
    "#     t.join()\n",
    "\n",
    "\n",
    "print(\"Benchmarking finished.\")\n",
    "\n",
    "\n",
    "\n",
    "# benchmark_dataset(folders, split_configs, cv_configs, augmentations, preprocessings_list, models, SEED) #, resampling='resample', resample_size=2048) #bins=5)\n",
    "\n",
    "\n",
    "# benchmark_dataset(folders, split_configs, cv_configs, augmentations, preprocessings_list, models, SEED, resampling='crop', resample_size=2150)\n",
    "\n",
    "\n",
    "# for folder in folder_list:\n",
    "    # # print(ord(str(folder)[17]), ord('A'), ord('M'))\n",
    "    # if ord(str(folder)[16]) < ord(\"L\") or ord(str(folder)[16]) > ord(\"M\"):\n",
    "    #     continue\n",
    "    # benchmark_dataset(folder, SEED, preprocessing_list(), 20, augment=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# browse all folders in \"results\" folder\n",
    "# load the _runs.txt and print the last line\n",
    "import os\n",
    "for r, d, _ in os.walk(\"results\"):\n",
    "    for folder in d:\n",
    "        path = os.path.join(r, folder)\n",
    "        if os.path.isdir(path):\n",
    "            with open(os.path.join(path, \"_runs.txt\"), \"r\") as f:\n",
    "                lines = f.readlines()\n",
    "                print(folder, lines[-1].strip())\n",
    "                print([round(float(l.split(\"  \")[0]), 3) for l in lines])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import minmaxscaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import sklearn\n",
    "\n",
    "minMaxScaler = MinMaxScaler(feature_range=(0, 0.5))\n",
    "print(type(minMaxScaler), minMaxScaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "\n",
    "def generate_combinations(steps):\n",
    "    methods = []\n",
    "    for step in steps.values():\n",
    "        methods.append(step['methods'])\n",
    "    return list(product(*methods))\n",
    "\n",
    "\n",
    "pre_indexation_steps = {\n",
    "    \"step_1\": {\n",
    "        \"type\": \"filter\",\n",
    "        \"methods\": ['method_1', 'method_2'],  # List[List[(TransformerMixin, Dict)]]\n",
    "    },\n",
    "    \"step_2\": {\n",
    "        \"type\": \"augmentation\",\n",
    "        \"methods\": ['method_3', 'method_4'],  # List[List[(TransformerMixin, Dict)]]\n",
    "    },\n",
    "}\n",
    "\n",
    "combinations = generate_combinations(pre_indexation_steps)\n",
    "print(combinations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "from data import load_data\n",
    "from benchmark_loop import get_datasheet\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "import glob\n",
    "import json\n",
    "\n",
    "dataset = \"Paprica_XY2_eqGlu\"\n",
    "canon_folder = \"results/\" + dataset + \"/\"\n",
    "json_file = canon_folder[:-1] + \"_results.json\"\n",
    "# sort json_file dict by \"RMSE\" key\n",
    "with open(json_file, 'r') as f:\n",
    "    json_dict = json.load(f)\n",
    "    json_dict = {k: v for k, v in json_dict.items() if \"type\" in v}\n",
    "    json_dict = dict(\n",
    "        sorted(json_dict.items(), key=lambda item: float(item[1][\"RMSE\"])))\n",
    "\n",
    "    if len(json_dict) == 0:\n",
    "        print(\"no best model saved\")\n",
    "    else:\n",
    "        run = list(json_dict.values())[0]\n",
    "        print(json.dumps(run, indent=4))\n",
    "\n",
    "        X, y, X_valid, y_valid = load_data(run[\"path\"])\n",
    "        print(X.shape, y.shape, X_valid.shape, y_valid.shape)\n",
    "\n",
    "        canon_name = canon_folder + run[\"model\"]\n",
    "        print(canon_name)\n",
    "\n",
    "        transformer_pipeline = joblib.load(canon_name + \"_tf.pkl\")\n",
    "        y_scaler = joblib.load(canon_name + \"y_scaler.pkl\")\n",
    "        model = None\n",
    "\n",
    "        if run[\"type\"] == \"NN\":\n",
    "            regex = os.path.join(canon_name + '*' + '.h5')\n",
    "            weight_files = glob.glob(regex)\n",
    "            weight_file = max(weight_files, key=os.path.getctime)\n",
    "            model = tf.keras.models.load_model(weight_file)\n",
    "        else:\n",
    "            model = joblib.load(canon_name + \"_reg.pkl\")\n",
    "\n",
    "        X_valid = transformer_pipeline.transform(X_valid)\n",
    "        y_pred = model.predict(X_valid)\n",
    "        y_pred = y_scaler.inverse_transform(y_pred)\n",
    "\n",
    "        datasheet = get_datasheet(\n",
    "            \"dataset_name\", \"model_name\", \"path\", SEED, y_valid, y_pred)\n",
    "        print(json.dumps(datasheet, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from scipy import signal\n",
    "path = \"Data/Paprica_2D/Paprica_2D_XY_ag\"\n",
    "projdir = Path(path)\n",
    "# print(projdir.glob(\"*XCal*\"))\n",
    "# for t in projdir.glob(\"*XCal*\"):\n",
    "    # print(t)\n",
    "files = tuple(next(projdir.glob(n)) for n in [\"*Xcal*\", \"*Ycal*\"])\n",
    "print(files)\n",
    "\n",
    "# from data import load_multiple_data\n",
    "# load_multiple_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "import numpy as np\n",
    "y_train = np.array([0, 1, 2, 2.5, 3, 3.5, 6, 8, 20]).reshape(-1, 1)\n",
    "bins = 4\n",
    "discretizer = KBinsDiscretizer(\n",
    "    n_bins=bins, encode='onehot-dense', strategy='uniform')\n",
    "discretizer.fit(y_train)\n",
    "tt = discretizer.transform(y_train)\n",
    "print(tt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from numpy import genfromtxt\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import cross_validate, cross_val_predict\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def plot_regression_results(ax, y_true, y_pred, title, scores, elapsed_time):\n",
    "    \"\"\"Scatter plot of the predicted vs true targets.\"\"\"\n",
    "    ax.plot(\n",
    "        [y_true.min(), y_true.max()], [y_true.min(), y_true.max()], \"--r\", linewidth=2\n",
    "    )\n",
    "    ax.scatter(y_true, y_pred, alpha=0.2)\n",
    "\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "    ax.get_xaxis().tick_bottom()\n",
    "    ax.get_yaxis().tick_left()\n",
    "    ax.spines[\"left\"].set_position((\"outward\", 10))\n",
    "    ax.spines[\"bottom\"].set_position((\"outward\", 10))\n",
    "    ax.set_xlim([y_true.min(), y_true.max()])\n",
    "    ax.set_ylim([y_true.min(), y_true.max()])\n",
    "    ax.set_xlabel(\"Measured\")\n",
    "    ax.set_ylabel(\"Predicted\")\n",
    "    extra = plt.Rectangle(\n",
    "        (0, 0), 0, 0, fc=\"w\", fill=False, edgecolor=\"none\", linewidth=0\n",
    "    )\n",
    "    ax.legend([extra], [scores], loc=\"upper left\")\n",
    "    title = title + \"\\n Evaluation in {:.2f} seconds\".format(elapsed_time)\n",
    "    ax.set_title(title)\n",
    "\n",
    "\n",
    "def plot_data(d, filepath):\n",
    "    plt.scatter(d[:, 0], d[:, 1])\n",
    "    plt.xlabel('test')\n",
    "    plt.ylabel('predict')\n",
    "    plt.savefig(filepath + '.png')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "path = 'results'\n",
    "for root, dirs, files in os.walk(path):\n",
    "    for file in files:\n",
    "        if file.endswith('.csv'):\n",
    "            filepath = os.path.join(root, file)\n",
    "            df = pd.read_csv(filepath)\n",
    "            my_data = genfromtxt(filepath, delimiter=';')\n",
    "            # print(my_data)\n",
    "            plot_data(my_data, filepath.replace('csv', 'png'))\n",
    "        # if file.endswith('.json'):\n",
    "        #     print(file)\n",
    "        #     dataset = file.replace('.json','')\n",
    "        #     f = open(os.path.join(root, file))\n",
    "        #     data = json.load(f)\n",
    "        #     for key in data:\n",
    "        #         print(key)\n",
    "\n",
    "# returns JSON object as \n",
    "# a dictionary\n",
    "            # filepath = os.path.join(root, file)\n",
    "            # df = pd.read_csv(filepath)\n",
    "            # y_res = df.iloc[:,0]\n",
    "            # y_pred = df.iloc[:,1]\n",
    "            # fig, axs = plt.subplots(1,1, figsize=(10,10))\n",
    "            # axs = np.ravel(axs)\n",
    "\n",
    "            # plot_regression_results(\n",
    "            #     ax,\n",
    "            #     y,\n",
    "            #     y_pred,\n",
    "            #     name,\n",
    "            #     (r\"$R^2={:.2f} \\pm {:.2f}$\" + \"\\n\" + r\"$MAE={:.2f} \\pm {:.2f}$\").format(\n",
    "            #         np.mean(score[\"test_r2\"]),\n",
    "            #         np.std(score[\"test_r2\"]),\n",
    "            #         -np.mean(score[\"test_neg_mean_absolute_error\"]),\n",
    "            #         np.std(score[\"test_neg_mean_absolute_error\"]),\n",
    "            #     ),\n",
    "            #     elapsed_time,\n",
    "            # )\n",
    "\n",
    "\n",
    "# plt.suptitle(\"Single predictors versus stacked predictors\")\n",
    "# plt.tight_layout()\n",
    "# plt.subplots_adjust(top=0.9)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinard.utils import load_csv\n",
    "from benchmark_loop import transform_test_data\n",
    "import preprocessings\n",
    "import numpy as np\n",
    "\n",
    "dumb_set = preprocessings.dumb_and_dumber_set()\n",
    "Xfile = \"data/regression/ALPINE_Calpine_424_Murguzur_RMSE1.36/Xcal.csv.gz\"\n",
    "yfile = \"data/regression/ALPINE_Calpine_424_Murguzur_RMSE1.36/Ycal.csv.gz\"\n",
    "X_train, y_train = load_csv(Xfile, yfile, x_hdr=0, y_hdr=0, sep=\";\")\n",
    "X_train, y_train, X_test, y_test = X_train[0:\n",
    "                                           100], y_train[0:100], X_train[0:100], y_train[0:100]\n",
    "X_test_pp, y_test_pp, transformer_pipeline, y_scaler = transform_test_data(\n",
    "    dumb_set, X_train, y_train, X_test, y_test, type=\"augmentation\")\n",
    "\n",
    "print(X_test_pp.shape)\n",
    "sample = X_test_pp[0]\n",
    "print(len(dumb_set))\n",
    "ok = []\n",
    "for i in range(len(dumb_set)-1, -1, -1):\n",
    "    found = False\n",
    "    for j in range(i-1, -1, -1):\n",
    "        if np.allclose(sample[i], sample[j], rtol=10e-3, atol=10e-3):\n",
    "            found = True\n",
    "            break\n",
    "    if not found:\n",
    "        ok.append(dumb_set[i][0])\n",
    "\n",
    "print(len(ok))\n",
    "print(ok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "\n",
    "def count_columns(file):\n",
    "    with open(file, 'r') as f:\n",
    "        reader = csv.reader(f, delimiter=\";\")\n",
    "        header = next(reader)\n",
    "        header = next(reader)\n",
    "        return len(header), header\n",
    "\n",
    "\n",
    "def walk_directory(directory):\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith('.csv'):\n",
    "                path = os.path.join(root, file)\n",
    "                columns, header = count_columns(path)\n",
    "                if columns > 2100 and columns < 4000:\n",
    "                    print(header[0:5])\n",
    "                    print(f\"File: {path}, Columns: {columns}\")\n",
    "\n",
    "\n",
    "# Replace '.' with the directory path you want to traverse\n",
    "walk_directory('rawdata/regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "def json_to_csv(json_folder, csv_file):\n",
    "    csv_rows = []\n",
    "    for file in os.listdir(json_folder):\n",
    "        if file.endswith(\".json\"):\n",
    "            file_path = os.path.join(json_folder, file)\n",
    "            with open(file_path, \"r\") as f:\n",
    "                json_data = json.load(f)\n",
    "            for key, obj in json_data.items():\n",
    "                csv_row = {}\n",
    "                for key, value in obj.items():\n",
    "                    csv_row[key] = value\n",
    "                csv_rows.append(csv_row)\n",
    "\n",
    "    fieldnames = []\n",
    "    for row in csv_rows:\n",
    "        for key in row.keys():\n",
    "            if key not in fieldnames:\n",
    "                fieldnames.append(key)\n",
    "\n",
    "    with open(csv_file, \"w\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for row in csv_rows:\n",
    "            writer.writerow(row)\n",
    "\n",
    "\n",
    "json_folder = \"results\"\n",
    "csv_file = \"result.csv\"\n",
    "\n",
    "json_to_csv(json_folder, csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow version\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Check if TensorFlow can access the GPU\n",
    "print(\"Is GPU available:\", tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "# Run a simple computation on the GPU (if available) and time it\n",
    "with tf.device('/gpu:0'):\n",
    "    a = tf.random.normal([1000, 1000])\n",
    "    b = tf.random.normal([1000, 1000])\n",
    "    c = tf.matmul(a, b)\n",
    "\n",
    "# Print the result\n",
    "print(c)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyNIRS_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c3c3249a294db370905b327c25644ce18610bfebb370223cf3414a9c437db486"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
