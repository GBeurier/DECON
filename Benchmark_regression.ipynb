{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies\n",
    "\n",
    "Install dependencies not available on Google Collab.\n",
    "Collab provides numpy, pandas, sklearn, tensorflow, scipy, etc. (see requirements.txt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pinard\n",
    "%pip install scikeras"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark details\n",
    "\n",
    "The results aggregate the combination of the following trainings configurations:\n",
    "\n",
    "- estimation configuration: [regression, classification]\n",
    "- datasets configurations: [Single Train, Cross validation with 5 folds and 2 repeats, Augmented Single Train]\n",
    "- preprocessing configuration: [flat spectrum, savgol, haar, [small set], [big_set]]\n",
    "- models:\n",
    "  - for all configuration: BACON, BACON-VG, DECON, PLS(components from 1 to 100), XGBoost, LW-PLS\n",
    "  - for single train + small_set : Stack > [ BACON, BACON-VG, DECON, PLS(components from 1 to 100), XGBoost, LW-PLS,\n",
    "    f_PLSRegression,f_AdaBoostRegressor,f_BaggingRegressor,f_ExtraTreesRegressor, f_GradientBoostingRegressor,f_RandomForestRegressor,\n",
    "    f_ARDRegression,f_BayesianRidge,f_ElasticNet,f_ElasticNetCV,f_HuberRegressor, f_LarsCV,f_LassoCV,f_Lasso,f_LassoLars,f_LassoLarsCV,\n",
    "    f_LassoLarsIC,f_LinearRegression,f_OrthogonalMatchingPursuit,f_OrthogonalMatchingPursuitCV, f_PassiveAggressiveRegressor,f_RANSACRegressor,\n",
    "    f_Ridge,f_RidgeCV,f_SGDRegressor,f_TheilSenRegressor,f_GaussianProcessRegressor,f_KNeighborsRegressor, f_Pipeline,f_MLPRegressor,f_LinearSVR,\n",
    "    f_NuSVR,f_SVR,f_DecisionTreeRegressor,f_ExtraTreeRegressor,f_KernelRidge,f_XGBRegressor]\n",
    "\n",
    "We perform training in 2 steps, (1) data transformation and (2) training because the sklearn pipeline does not use test data natively.\n",
    "To change with pinard update in the future.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### FAST GPU RESET ####\n",
    "from numba import cuda\n",
    "device = cuda.get_current_device()\n",
    "device.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Benchmarking 2 runs.\n",
      "========== ALPINE_CALPINE_424_MURGUZUR_RMSE1.36 Loading data... data/_RefSet\\ALPINE_Calpine_424_Murguzur_RMSE1.36\n"
     ]
    }
   ],
   "source": [
    "## Browse path and launch benchmark for every folders\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pathlib import Path\n",
    "from preprocessings import preprocessing_list\n",
    "\n",
    "from benchmark_loop import benchmark_dataset\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.get_logger().setLevel(\"ERROR\")\n",
    "tf.keras.mixed_precision.set_global_policy(\"mixed_float16\")\n",
    "\n",
    "rootdir = Path('data/regression')\n",
    "folder_list = [f for f in rootdir.glob('**/*') if f.is_dir()]\n",
    "\n",
    "SEED = ord('D') + 31373\n",
    "\n",
    "# tf.keras.utils.set_random_seed(SEED)\n",
    "# tf.config.experimental.enable_op_determinism()\n",
    "\n",
    "\n",
    "import preprocessings\n",
    "import regressors\n",
    "import pinard.preprocessing as pp\n",
    "from pinard import augmentation, model_selection\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from xgboost import XGBRegressor\n",
    "import sys\n",
    "import os.path\n",
    "\n",
    "def str_to_class(classname):\n",
    "    return getattr(sys.modules['pinard.preprocessing'], classname)\n",
    "\n",
    "# print(str_to_class('SavitzkyGolay'))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_dataset_list(path):\n",
    "    datasets = []\n",
    "    for r, d, _ in os.walk(path):\n",
    "        for folder in d:\n",
    "            # print(r, folder)\n",
    "            path = os.path.join(r, folder)\n",
    "            if os.path.isdir(path):\n",
    "                # if len(datasets) < 3:\n",
    "                datasets.append(str(path))\n",
    "                # break\n",
    "    return datasets\n",
    "\n",
    "split_configs = [\n",
    "    None,\n",
    "    # {'test_size':None, 'method':\"random\", 'random_state':SEED},\n",
    "    # {'test_size':None, 'method':\"stratified\", 'random_state':SEED, 'n_bins':5},\n",
    "    # {'test_size':0.25, 'method':\"spxy\", 'random_state':SEED, 'metric':\"euclidean\", 'pca_components':250},\n",
    "]\n",
    "\n",
    "augmentations = [\n",
    "    None,\n",
    "    # [(6, augmentation.Rotate_Translate())],\n",
    "    # [(3, augmentation.Rotate_Translate()),(2, augmentation.Random_X_Operation()),(1, augmentation.Random_Spline_Addition()),],\n",
    "    # [(3, augmentation.Rotate_Translate()),(2, augmentation.Random_X_Operation()),(2, augmentation.Random_Spline_Addition()),]\n",
    "]\n",
    "\n",
    "preprocessings_list = [\n",
    "    # None,\n",
    "    preprocessings.id_preprocessing(),\n",
    "    [(\"id\", pp.IdentityTransformer()), ('haar', pp.Haar()), ('savgol', pp.SavitzkyGolay())],\n",
    "    # preprocessings.decon_set(),\n",
    "    # preprocessings.bacon_set(),\n",
    "    # preprocessings.small_set(),\n",
    "    # preprocessings.transf_set(),\n",
    "    # preprocessings.optimal_set_2D(),\n",
    "    # preprocessings.fat_set(),\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "cv_configs = [\n",
    "    None,\n",
    "    # {'n_splits':5, 'n_repeats':4},\n",
    "    # {'n_splits':4, 'n_repeats':2},\n",
    "    # {'n_splits':3, 'n_repeats':1},\n",
    "]\n",
    "\n",
    "# import os\n",
    "# folder = \"data/regression\"\n",
    "# folder = \"data/Paprica_2D\"\n",
    "folder = \"data/_RefSet\"\n",
    "folders = get_dataset_list(folder)\n",
    "folders = folders[0:1]\n",
    "# folders = [\"data/regression/Cassava_TBC_3556_Davrieux_RMSE1.02\"]\n",
    "\n",
    "len_cv_configs = 0\n",
    "for c in cv_configs:\n",
    "    if c == None:\n",
    "        len_cv_configs += 1\n",
    "    else:\n",
    "        len_cv_configs += (c['n_splits'] * c['n_repeats'])\n",
    "\n",
    "models = [\n",
    "    # (regressors.ML_Regressor(XGBRegressor), {\"n_estimators\":200, \"max_depth\":50, \"seed\":SEED}),\n",
    "    # (regressors.ML_Regressor(PLSRegression), {\"n_components\":50}),\n",
    "    # (regressors.Transformer_NIRS(), {'batch_size':500, 'epoch':10000, 'verbose':0, 'patience':1000, 'optimizer':'adam', 'loss':'mse'}),\n",
    "    # (regressors.Decon_SepPo(), {'batch_size':50, 'epoch':10000, 'verbose':0, 'patience':1000, 'optimizer':'adam', 'loss':'mse'}),\n",
    "    # (regressors.FFT_Conv(), {'batch_size':500, 'epoch':20000, 'verbose':0, 'patience':1000, 'optimizer':'adam', 'loss':'mse'}),\n",
    "    # (regressors.Decon(), {'batch_size':100, 'epoch':20000, 'verbose':0, 'patience':400, 'optimizer':'adam', 'loss':'mse'}),\n",
    "    # (regressors.Decon_Sep(), {'batch_size':100, 'epoch':20, 'verbose':2, 'patience':500, 'optimizer':'adam', 'loss':'mse'}),\n",
    "    # (regressors.Decon_SepPo(), {'batch_size':100, 'epoch':20000, 'verbose':2, 'patience':500, 'optimizer':'adam', 'loss':'mse'}),\n",
    "    # (regressors.Decon_Sep_VG(), {'batch_size':100, 'epoch':20000, 'verbose':2, 'patience':500, 'optimizer':'adam', 'loss':'mse'}),\n",
    "    # (regressors.ResNetV2(), {'batch_size':200, 'epoch':20000, 'verbose':0, 'patience':300, 'optimizer':'adam', 'loss':'mse'}),\n",
    "    # (regressors.MLP(), {'batch_size':1000, 'epoch':20000, 'verbose':0, 'patience':2000, 'optimizer':'adam', 'loss':'mse'}),\n",
    "    # (regressors.CONV_LSTM(), {'batch_size':1000, 'epoch':20000, 'verbose':0, 'patience':2000, 'optimizer':'adam', 'loss':'mse'}),\n",
    "    (regressors.XCeption1D(), {'batch_size':500, 'epoch':10000, 'verbose':0, 'patience':1200, 'optimizer':'adam', 'loss':'mse'}),\n",
    "    # (regressors.Transformer(), {'batch_size':10, 'epoch':300, 'verbose':0, 'patience':30, 'optimizer':'Adam', 'loss':'mse'}),\n",
    "    # (regressors.Bacon(), {'batch_size':100, 'epoch':10, 'verbose':0, 'patience':10, 'optimizer':'adam', 'loss':'mse'}, True),\n",
    "]\n",
    "\n",
    "\n",
    "from lwpls import LWPLS\n",
    "from regressors import NonlinearPLSRegressor\n",
    "\n",
    "# for i in range(5,150,5):\n",
    "#     models. append(\n",
    "#         (regressors.ML_Regressor(NonlinearPLSRegressor, name=f\"NL_RBF_PLS_{i}\"), {\"n_components\":i, \"poly_degree\":2, \"gamma\":0.1})\n",
    "#     )\n",
    "#     models. append(\n",
    "#         (regressors.ML_Regressor(PLSRegression, name=f\"PLS_{i}\"), {\"n_components\":i})\n",
    "#     )\n",
    "\n",
    "# models.append(\n",
    "#     (regressors.ML_Regressor(LWPLS, name=f\"LWPLS_0-05_45\"), {\"max_component_number\":45, \"lambda_in_similarity\":0.05})\n",
    "# )\n",
    "\n",
    "# for i in range(10,100,50):\n",
    "#     models.append(\n",
    "#         (regressors.ML_Regressor(LWPLS, name=f\"LWPLS_0-1_{i}\"), {\"max_component_number\":i, \"lambda_in_similarity\":0.05})\n",
    "#     )\n",
    "    # models.append(\n",
    "    #     (regressors.ML_Regressor(LWPLS, name=f\"LWPLS_0-5_{i}\"), {\"max_component_number\":i, \"lambda_in_similarity\":0.5})\n",
    "    # )\n",
    "# for i in range(10,100,5):\n",
    "    # models. append(\n",
    "    #     (regressors.ML_Regressor(PLSRegression, name=f\"PLS_{i}\"), {\"n_components\":i})\n",
    "    # )\n",
    "\n",
    "benchmark_size = len(folders) * len(split_configs) * len_cv_configs * len(augmentations) * len(preprocessings_list) * len(models)\n",
    "print(\"Benchmarking\", benchmark_size, \"runs.\")\n",
    "\n",
    "\n",
    "benchmark_dataset(folders, split_configs, cv_configs, augmentations, preprocessings_list, models, SEED) #, resampling='resample', resample_size=2048) #bins=5)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# benchmark_dataset(folders, split_configs, cv_configs, augmentations, preprocessings_list, models, SEED, resampling='crop', resample_size=2150)\n",
    "\n",
    "\n",
    "# for folder in folder_list:\n",
    "    # # print(ord(str(folder)[17]), ord('A'), ord('M'))\n",
    "    # if ord(str(folder)[16]) < ord(\"L\") or ord(str(folder)[16]) > ord(\"M\"):\n",
    "    #     continue\n",
    "    # benchmark_dataset(folder, SEED, preprocessing_list(), 20, augment=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.preprocessing._data.MinMaxScaler'> MinMaxScaler(feature_range=(0, 0.5))\n"
     ]
    }
   ],
   "source": [
    "# import minmaxscaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import sklearn\n",
    "\n",
    "minMaxScaler = MinMaxScaler(feature_range=(0, 0.5))\n",
    "print(type(minMaxScaler), minMaxScaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('method_1', 'method_3'), ('method_1', 'method_4'), ('method_2', 'method_3'), ('method_2', 'method_4')]\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "\n",
    "\n",
    "def generate_combinations(steps):\n",
    "    methods = []\n",
    "    for step in steps.values():\n",
    "        methods.append(step['methods'])\n",
    "    return list(product(*methods))\n",
    "\n",
    "\n",
    "pre_indexation_steps = {\n",
    "    \"step_1\": {\n",
    "        \"type\": \"filter\",\n",
    "        \"methods\": ['method_1', 'method_2'],  # List[List[(TransformerMixin, Dict)]]\n",
    "    },\n",
    "    \"step_2\": {\n",
    "        \"type\": \"augmentation\",\n",
    "        \"methods\": ['method_3', 'method_4'],  # List[List[(TransformerMixin, Dict)]]\n",
    "    },\n",
    "}\n",
    "\n",
    "combinations = generate_combinations(pre_indexation_steps)\n",
    "print(combinations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"model\": \"sklearn.cross_decomposition._pls.PLSRegression--NoSpl-NoCV-Fold_1(1)-NoAug-PP_3-31441-23-05-31_17-23-18\",\n",
      "    \"dataset\": \"Paprica_XY2_eqGlu\",\n",
      "    \"seed\": \"31441\",\n",
      "    \"RMSE\": \"14.00861613825827\",\n",
      "    \"MAPE\": \"0.4850944984473116\",\n",
      "    \"R2\": \"0.49967301668101505\",\n",
      "    \"MAE\": \"10.189237773956993\",\n",
      "    \"MSE\": \"196.24132610907006\",\n",
      "    \"MedAE\": \"6.566170964445131\",\n",
      "    \"EVS\": \"0.5002228661042355\",\n",
      "    \"run\": \"2023-05-31  17:23:18\",\n",
      "    \"path\": \"data/regression\\\\Paprica_XY2_eqGlu\",\n",
      "    \"type\": \"ML\",\n",
      "    \"training_time\": \"00:00:00\"\n",
      "}\n",
      "(131, 2151) (131, 1) (59, 2151) (59, 1)\n",
      "results/Paprica_XY2_eqGlu/sklearn.cross_decomposition._pls.PLSRegression--NoSpl-NoCV-Fold_1(1)-NoAug-PP_3-31441-23-05-31_17-23-18\n",
      "{\n",
      "    \"model\": \"model_name\",\n",
      "    \"dataset\": \"dataset_name\",\n",
      "    \"seed\": \"31441\",\n",
      "    \"RMSE\": \"14.00861613825827\",\n",
      "    \"MAPE\": \"0.4850944984473116\",\n",
      "    \"R2\": \"0.49967301668101505\",\n",
      "    \"MAE\": \"10.189237773956993\",\n",
      "    \"MSE\": \"196.24132610907006\",\n",
      "    \"MedAE\": \"6.566170964445131\",\n",
      "    \"EVS\": \"0.5002228661042355\",\n",
      "    \"run\": \"2023-05-31  17:24:09\",\n",
      "    \"path\": \"path\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "from data import load_data\n",
    "from benchmark_loop import get_datasheet\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "import glob\n",
    "import json\n",
    "\n",
    "dataset = \"Paprica_XY2_eqGlu\"\n",
    "canon_folder = \"results/\" + dataset + \"/\"\n",
    "json_file = canon_folder[:-1] + \"_results.json\"\n",
    "# sort json_file dict by \"RMSE\" key\n",
    "with open(json_file, 'r') as f:\n",
    "    json_dict = json.load(f)\n",
    "    json_dict = {k: v for k, v in json_dict.items() if \"type\" in v}\n",
    "    json_dict = dict(\n",
    "        sorted(json_dict.items(), key=lambda item: float(item[1][\"RMSE\"])))\n",
    "\n",
    "    if len(json_dict) == 0:\n",
    "        print(\"no best model saved\")\n",
    "    else:\n",
    "        run = list(json_dict.values())[0]\n",
    "        print(json.dumps(run, indent=4))\n",
    "\n",
    "        X, y, X_valid, y_valid = load_data(run[\"path\"])\n",
    "        print(X.shape, y.shape, X_valid.shape, y_valid.shape)\n",
    "\n",
    "        canon_name = canon_folder + run[\"model\"]\n",
    "        print(canon_name)\n",
    "\n",
    "        transformer_pipeline = joblib.load(canon_name + \"_tf.pkl\")\n",
    "        y_scaler = joblib.load(canon_name + \"y_scaler.pkl\")\n",
    "        model = None\n",
    "\n",
    "        if run[\"type\"] == \"NN\":\n",
    "            regex = os.path.join(canon_name + '*' + '.h5')\n",
    "            weight_files = glob.glob(regex)\n",
    "            weight_file = max(weight_files, key=os.path.getctime)\n",
    "            model = tf.keras.models.load_model(weight_file)\n",
    "        else:\n",
    "            model = joblib.load(canon_name + \"_reg.pkl\")\n",
    "\n",
    "        X_valid = transformer_pipeline.transform(X_valid)\n",
    "        y_pred = model.predict(X_valid)\n",
    "        y_pred = y_scaler.inverse_transform(y_pred)\n",
    "\n",
    "        datasheet = get_datasheet(\n",
    "            \"dataset_name\", \"model_name\", \"path\", SEED, y_valid, y_pred)\n",
    "        print(json.dumps(datasheet, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from scipy import signal\n",
    "path = \"Data/Paprica_2D/Paprica_2D_XY_ag\"\n",
    "projdir = Path(path)\n",
    "# print(projdir.glob(\"*XCal*\"))\n",
    "# for t in projdir.glob(\"*XCal*\"):\n",
    "    # print(t)\n",
    "files = tuple(next(projdir.glob(n)) for n in [\"*Xcal*\", \"*Ycal*\"])\n",
    "print(files)\n",
    "\n",
    "# from data import load_multiple_data\n",
    "# load_multiple_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "import numpy as np\n",
    "y_train = np.array([0, 1, 2, 2.5, 3, 3.5, 6, 8, 20]).reshape(-1, 1)\n",
    "bins = 4\n",
    "discretizer = KBinsDiscretizer(\n",
    "    n_bins=bins, encode='onehot-dense', strategy='uniform')\n",
    "discretizer.fit(y_train)\n",
    "tt = discretizer.transform(y_train)\n",
    "print(tt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from numpy import genfromtxt\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import cross_validate, cross_val_predict\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def plot_regression_results(ax, y_true, y_pred, title, scores, elapsed_time):\n",
    "    \"\"\"Scatter plot of the predicted vs true targets.\"\"\"\n",
    "    ax.plot(\n",
    "        [y_true.min(), y_true.max()], [y_true.min(), y_true.max()], \"--r\", linewidth=2\n",
    "    )\n",
    "    ax.scatter(y_true, y_pred, alpha=0.2)\n",
    "\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "    ax.get_xaxis().tick_bottom()\n",
    "    ax.get_yaxis().tick_left()\n",
    "    ax.spines[\"left\"].set_position((\"outward\", 10))\n",
    "    ax.spines[\"bottom\"].set_position((\"outward\", 10))\n",
    "    ax.set_xlim([y_true.min(), y_true.max()])\n",
    "    ax.set_ylim([y_true.min(), y_true.max()])\n",
    "    ax.set_xlabel(\"Measured\")\n",
    "    ax.set_ylabel(\"Predicted\")\n",
    "    extra = plt.Rectangle(\n",
    "        (0, 0), 0, 0, fc=\"w\", fill=False, edgecolor=\"none\", linewidth=0\n",
    "    )\n",
    "    ax.legend([extra], [scores], loc=\"upper left\")\n",
    "    title = title + \"\\n Evaluation in {:.2f} seconds\".format(elapsed_time)\n",
    "    ax.set_title(title)\n",
    "\n",
    "\n",
    "def plot_data(d, filepath):\n",
    "    plt.scatter(d[:, 0], d[:, 1])\n",
    "    plt.xlabel('test')\n",
    "    plt.ylabel('predict')\n",
    "    plt.savefig(filepath + '.png')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "path = 'results'\n",
    "for root, dirs, files in os.walk(path):\n",
    "    for file in files:\n",
    "        if file.endswith('.csv'):\n",
    "            filepath = os.path.join(root, file)\n",
    "            df = pd.read_csv(filepath)\n",
    "            my_data = genfromtxt(filepath, delimiter=';')\n",
    "            # print(my_data)\n",
    "            plot_data(my_data, filepath.replace('csv', 'png'))\n",
    "        # if file.endswith('.json'):\n",
    "        #     print(file)\n",
    "        #     dataset = file.replace('.json','')\n",
    "        #     f = open(os.path.join(root, file))\n",
    "        #     data = json.load(f)\n",
    "        #     for key in data:\n",
    "        #         print(key)\n",
    "\n",
    "# returns JSON object as \n",
    "# a dictionary\n",
    "            # filepath = os.path.join(root, file)\n",
    "            # df = pd.read_csv(filepath)\n",
    "            # y_res = df.iloc[:,0]\n",
    "            # y_pred = df.iloc[:,1]\n",
    "            # fig, axs = plt.subplots(1,1, figsize=(10,10))\n",
    "            # axs = np.ravel(axs)\n",
    "\n",
    "            # plot_regression_results(\n",
    "            #     ax,\n",
    "            #     y,\n",
    "            #     y_pred,\n",
    "            #     name,\n",
    "            #     (r\"$R^2={:.2f} \\pm {:.2f}$\" + \"\\n\" + r\"$MAE={:.2f} \\pm {:.2f}$\").format(\n",
    "            #         np.mean(score[\"test_r2\"]),\n",
    "            #         np.std(score[\"test_r2\"]),\n",
    "            #         -np.mean(score[\"test_neg_mean_absolute_error\"]),\n",
    "            #         np.std(score[\"test_neg_mean_absolute_error\"]),\n",
    "            #     ),\n",
    "            #     elapsed_time,\n",
    "            # )\n",
    "\n",
    "\n",
    "# plt.suptitle(\"Single predictors versus stacked predictors\")\n",
    "# plt.tight_layout()\n",
    "# plt.subplots_adjust(top=0.9)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinard.utils import load_csv\n",
    "from benchmark_loop import transform_test_data\n",
    "import preprocessings\n",
    "import numpy as np\n",
    "\n",
    "dumb_set = preprocessings.dumb_and_dumber_set()\n",
    "Xfile = \"data/regression/ALPINE_Calpine_424_Murguzur_RMSE1.36/Xcal.csv.gz\"\n",
    "yfile = \"data/regression/ALPINE_Calpine_424_Murguzur_RMSE1.36/Ycal.csv.gz\"\n",
    "X_train, y_train = load_csv(Xfile, yfile, x_hdr=0, y_hdr=0, sep=\";\")\n",
    "X_train, y_train, X_test, y_test = X_train[0:\n",
    "                                           100], y_train[0:100], X_train[0:100], y_train[0:100]\n",
    "X_test_pp, y_test_pp, transformer_pipeline, y_scaler = transform_test_data(\n",
    "    dumb_set, X_train, y_train, X_test, y_test, type=\"augmentation\")\n",
    "\n",
    "print(X_test_pp.shape)\n",
    "sample = X_test_pp[0]\n",
    "print(len(dumb_set))\n",
    "ok = []\n",
    "for i in range(len(dumb_set)-1, -1, -1):\n",
    "    found = False\n",
    "    for j in range(i-1, -1, -1):\n",
    "        if np.allclose(sample[i], sample[j], rtol=10e-3, atol=10e-3):\n",
    "            found = True\n",
    "            break\n",
    "    if not found:\n",
    "        ok.append(dumb_set[i][0])\n",
    "\n",
    "print(len(ok))\n",
    "print(ok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "\n",
    "def count_columns(file):\n",
    "    with open(file, 'r') as f:\n",
    "        reader = csv.reader(f, delimiter=\";\")\n",
    "        header = next(reader)\n",
    "        header = next(reader)\n",
    "        return len(header), header\n",
    "\n",
    "\n",
    "def walk_directory(directory):\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith('.csv'):\n",
    "                path = os.path.join(root, file)\n",
    "                columns, header = count_columns(path)\n",
    "                if columns > 2100 and columns < 4000:\n",
    "                    print(header[0:5])\n",
    "                    print(f\"File: {path}, Columns: {columns}\")\n",
    "\n",
    "\n",
    "# Replace '.' with the directory path you want to traverse\n",
    "walk_directory('rawdata/regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "def json_to_csv(json_folder, csv_file):\n",
    "    csv_rows = []\n",
    "    for file in os.listdir(json_folder):\n",
    "        if file.endswith(\".json\"):\n",
    "            file_path = os.path.join(json_folder, file)\n",
    "            with open(file_path, \"r\") as f:\n",
    "                json_data = json.load(f)\n",
    "            for key, obj in json_data.items():\n",
    "                csv_row = {}\n",
    "                for key, value in obj.items():\n",
    "                    csv_row[key] = value\n",
    "                csv_rows.append(csv_row)\n",
    "\n",
    "    fieldnames = []\n",
    "    for row in csv_rows:\n",
    "        for key in row.keys():\n",
    "            if key not in fieldnames:\n",
    "                fieldnames.append(key)\n",
    "\n",
    "    with open(csv_file, \"w\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for row in csv_rows:\n",
    "            writer.writerow(row)\n",
    "\n",
    "\n",
    "json_folder = \"results\"\n",
    "csv_file = \"result.csv\"\n",
    "\n",
    "json_to_csv(json_folder, csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow version\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Check if TensorFlow can access the GPU\n",
    "print(\"Is GPU available:\", tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "# Run a simple computation on the GPU (if available) and time it\n",
    "with tf.device('/gpu:0'):\n",
    "    a = tf.random.normal([1000, 1000])\n",
    "    b = tf.random.normal([1000, 1000])\n",
    "    c = tf.matmul(a, b)\n",
    "\n",
    "# Print the result\n",
    "print(c)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyNIRS_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c3c3249a294db370905b327c25644ce18610bfebb370223cf3414a9c437db486"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
