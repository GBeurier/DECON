{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scikeras Keras regressor integration\n",
    "\n",
    "The integration of KerasRegressor in pipeline is limited. To use full keras capabilities, you may need to modify the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping rawdata\\regression\\Wood_Sustainability_511_Davrieux_RMSE0.092\\Xcal.csv\n",
      "skipping rawdata\\regression\\Wood_Sustainability_511_Davrieux_RMSE0.092\\Ycal.csv\n",
      "skipping rawdata\\regression\\Wood_Sustainability_511_Davrieux_RMSE0.092\\Xval.csv\n",
      "skipping rawdata\\regression\\Wood_Sustainability_511_Davrieux_RMSE0.092\\Yval.csv\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import glob\n",
    "import gzip\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "for folder in ['regression']:#, 'classif']:\n",
    "    rootdir = Path('rawdata/'+folder)\n",
    "    file_list = [f for f in rootdir.glob('**/*') if f.is_dir()]\n",
    "    files_pattern = [\"*Xcal*\",\"*Ycal*\",\"*Xval*\",\"*Yval*\"]\n",
    "    for dir in file_list:\n",
    "        training_name = str(dir)\n",
    "        projdir = Path(dir)\n",
    "        for f in files_pattern:\n",
    "            file = next(projdir.glob(f))\n",
    "            new_file = str(file).replace(\"rawdata\",\"data2\")\n",
    "            new_file += '.gz'\n",
    "            if os.path.exists(new_file):\n",
    "                print(\"skipping\", file)\n",
    "                continue\n",
    "            df = pd.read_csv(file, sep=\";\", header=0, skip_blank_lines=False)\n",
    "            new_folder = str(file.parent).replace(\"rawdata\",\"data2\")\n",
    "            if not os.path.exists(new_folder):\n",
    "                os.makedirs(new_folder)\n",
    "            df.to_csv(new_file, sep=\";\", index=False, compression=\"gzip\")\n",
    "            print(file, \"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WindowsPath('results/ALPINE_Calpine_424_Murguzur_results.json'), WindowsPath('results/ALPINE_C_424_Murguzur_results.json'), WindowsPath('results/ALPINE_Nalpine_552_Murguzur_results.json'), WindowsPath('results/ALPINE_N_552_Murguzur_results.json'), WindowsPath('results/ALPINE_Palpine_291_Murguzur_results.json'), WindowsPath('results/ALPINE_P_291_Murguzur_results.json'), WindowsPath('results/Cassava_TBC_3393_Sanchez_results.json'), WindowsPath('results/Cassava_TBC_3432_Shen_results.json'), WindowsPath('results/Cassava_TBC_3556_Davrieux_results.json'), WindowsPath('results/Cassava_TTC_3393_Sanchez_results.json'), WindowsPath('results/Cassava_TTC_3830_Davrieux_results.json'), WindowsPath('results/Eucalyptus_Density_1654_Chaix_results.json'), WindowsPath('results/LUCAS_SOCcropland_8731_Nocita_results.json'), WindowsPath('results/Meat_FatE1_215_Borggaard_results.json'), WindowsPath('results/Meat_FatE2_215_Borggaard_results.json'), WindowsPath('results/Meat_Fat_215_Borggaard_results.json'), WindowsPath('results/RICE_Redox_3693_Senseen_results.json'), WindowsPath('results/Sorghum_Starch_152_IAVAO_results.json')]\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import glob\n",
    "import gzip\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "## un fichier par dataset > run_idx + datasheet + link vers runs & link vers tensorboard\n",
    "## un fichier global json\n",
    "## un export csv ligne=run colonnes=[datasetx(RMSE,RÂ²)]\n",
    "## correct RMSE <-> MSE\n",
    "\n",
    "global_json = {}\n",
    "\n",
    "def fill_json(v, revert):\n",
    "    if revert:\n",
    "        tmp = v['RMSE']\n",
    "        v['RMSE'] = v['MSE']\n",
    "        v['MSE'] = tmp\n",
    "\n",
    "    d = v['dataset']\n",
    "    if d not in global_json:\n",
    "        global_json[d] = []\n",
    "    global_json[d].append(v)\n",
    "\n",
    "def parse(json_file, revert=False):\n",
    "    print(json_file)\n",
    "\n",
    "    with open(json_file) as json_file:\n",
    "        results = json.load(json_file)\n",
    "        if isinstance(results, list):\n",
    "            for ar in results:\n",
    "                v = ar[1]\n",
    "                tmp = v['RMSE']\n",
    "                fill_json(v, revert)\n",
    "        else:\n",
    "            for k, v in results.items():\n",
    "                # print(\"DICT>\", v['model'], v['dataset'], v['RMSE'])\n",
    "                fill_json(v, revert)\n",
    "\n",
    "# r=root, d=directories, f = files\n",
    "for r, d, f in os.walk(\"results\"):\n",
    "    for folder in d:\n",
    "        file_list = [f for f in Path(os.path.join(r, folder)).glob('*.json') if not f.is_dir()]\n",
    "        for file in file_list:\n",
    "            parse(file, True)\n",
    "    \n",
    "root_file_list = [f for f in Path(\"results\").glob('*.json') if not f.is_dir()]\n",
    "for file in file_list:\n",
    "    parse(file, False)\n",
    "\n",
    "### SORT runs per datasets\n",
    "# for k in global_json:\n",
    "    global_json[k] = sorted(global_json[k], key=lambda d: d['RMSE']) \n",
    "\n",
    "with open(\"GLOBAL.json\", 'w') as fp:\n",
    "    json.dump(global_json, fp, indent=4)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.9 ('pynirsENV')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7b09f6e5407ec4329146609a0cb08cbbe4720f97bb26598a93c421b663bd10d2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
