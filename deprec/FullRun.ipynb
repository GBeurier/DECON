{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies\n",
    "\n",
    "Install dependencies not available on Google Collab.\n",
    "Collab provides numpy, pandas, sklearn, tensorflow, scipy, etc. (see requirements.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pinard\n",
    "!pip install scikeras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark details\n",
    "\n",
    "The results aggregate the combination of the following trainings configurations:\n",
    "- estimation configuration: [regression, classification]\n",
    "- datasets configurations: [Single Train, Cross validation with 5 folds and 2 repeats, Augmented Single Train]\n",
    "- preprocessing configuration: [flat spectrum, savgol, haar, [small set], [big_set]]\n",
    "- models: \n",
    "   - for all configuration: BACON, BACON-VG, DECON, PLS(components from 1 to 100), XGBoost, LW-PLS\n",
    "   - for single train + small_set : Stack > [ BACON, BACON-VG, DECON, PLS(components from 1 to 100), XGBoost, LW-PLS,\n",
    "   f_PLSRegression,f_AdaBoostRegressor,f_BaggingRegressor,f_ExtraTreesRegressor, f_GradientBoostingRegressor,f_RandomForestRegressor,\n",
    "   f_ARDRegression,f_BayesianRidge,f_ElasticNet,f_ElasticNetCV,f_HuberRegressor, f_LarsCV,f_LassoCV,f_Lasso,f_LassoLars,f_LassoLarsCV,\n",
    "   f_LassoLarsIC,f_LinearRegression,f_OrthogonalMatchingPursuit,f_OrthogonalMatchingPursuitCV, f_PassiveAggressiveRegressor,f_RANSACRegressor,\n",
    "   f_Ridge,f_RidgeCV,f_SGDRegressor,f_TheilSenRegressor,f_GaussianProcessRegressor,f_KNeighborsRegressor, f_Pipeline,f_MLPRegressor,f_LinearSVR,\n",
    "   f_NuSVR,f_SVR,f_DecisionTreeRegressor,f_ExtraTreeRegressor,f_KernelRidge,f_XGBRegressor]\n",
    "\n",
    "We perform training in 2 steps, (1) data transformation and (2) training because the sklearn pipeline does not use test data natively.\n",
    "To change with pinard update in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded (175, 2152) (175,) (75, 2152) (75,)\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error, r2_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "from xgboost import XGBRegressor, XGBClassifier\n",
    "\n",
    "from lwpls import lwpls\n",
    "import pinard\n",
    "import models\n",
    "\n",
    "from pinard.utils import load_csv\n",
    "from pinard.sklearn import FeatureAugmentation\n",
    "from pinard import preprocessing as pp\n",
    "\n",
    "\n",
    "SEED = 163\n",
    "np.random.seed(SEED)\n",
    "# tf.random.set_seed(SEED)\n",
    "\n",
    "\n",
    "## Data configurations return a set of tuple (xtrain, ytrain, xtest, ytest, xvalid, yvalid)\n",
    "def data_raw_config(XCal, yCal, XVal, yVal):\n",
    "    return ([(XCal, yCal, XVal, yVal)], (XVal, yVal))\n",
    "\n",
    "def data_cv_config(XCal, yCal, XVal, yVal):\n",
    "    fold = RepeatedKFold(5, 2, random_state=SEED)\n",
    "    datasets = []\n",
    "    for train_index, test_index in fold.split(XCal):\n",
    "        datasets.append((XCal[train_index], yCal[train_index], XCal[test_index], yCal[test_index]))\n",
    "\n",
    "    return (datasets, (XVal, yVal))\n",
    "\n",
    "def data_augmented_config(XCal, yCal, XVal, yVal):\n",
    "    #TODO\n",
    "    pass\n",
    "\n",
    "def Id():\n",
    "    return [('id', pp.IdentityTransformer())]\n",
    "\n",
    "def savgol():\n",
    "    return [('id', pp.SavitzkyGolay())]\n",
    "\n",
    "def haar():\n",
    "    return [('id', pp.Wavelet('haar'))]\n",
    "\n",
    "def bacon_set():\n",
    "    preprocessing = [   \n",
    "        ('id', pp.IdentityTransformer()),\n",
    "        ('savgol1', pp.SavitzkyGolay( window_length=17, polyorder=2, deriv=2)),\n",
    "        ('savgol2', pp.SavitzkyGolay( window_length=5, polyorder=2)),\n",
    "        ('gaussian1', pp.Gaussian(order = 1, sigma = 2)),\n",
    "        ('gaussian2', pp.Gaussian(order = 2, sigma = 1)),\n",
    "        ('gaussian3', pp.Gaussian(order = 0, sigma = 2)),\n",
    "        ('baseline', pp.StandardNormalVariate()),\n",
    "        ('msc', pp.MultiplicativeScatterCorrection(scale=False)),\n",
    "        ('detrend', pp.Detrend()),\n",
    "        ('derivate', pp.Derivate(2,1)),\n",
    "        ('dv2', pp.Derivate(2,1)),\n",
    "        ('haar',pp.Wavelet('haar')),\n",
    "        ]\n",
    "    return preprocessing\n",
    "\n",
    "def decon_set():\n",
    "    preprocessing = [   \n",
    "        ('id', pp.IdentityTransformer()),\n",
    "        ('baseline', pp.StandardNormalVariate()),\n",
    "        ('savgol', pp.SavitzkyGolay()),\n",
    "        ('gaussian1', pp.Gaussian(order = 1, sigma = 2)),\n",
    "        ('gaussian2', pp.Gaussian(order = 2, sigma = 1)),\n",
    "        ('haar', pp.Wavelet('haar')),\n",
    "        ('coif3', pp.Wavelet('coif3')),\n",
    "        ('detrend', pp.Detrend()),\n",
    "        ('msc', pp.MultiplicativeScatterCorrection(scale=False)),\n",
    "        ('dv1', pp.Derivate(1,1)),\n",
    "        ('dv2', pp.Derivate(2,1)),\n",
    "        ('dv3', pp.Derivate(2,2)),\n",
    "        ('baseline*savgol', Pipeline([('_sg1',pp.StandardNormalVariate()),('_sg2',pp.SavitzkyGolay())])),\n",
    "        ('baseline*gaussian1', Pipeline([('_sg1',pp.StandardNormalVariate()),('g2', pp.Gaussian(order = 1, sigma = 2) )])),\n",
    "        ('baseline*gaussian2', Pipeline([('_sg1',pp.StandardNormalVariate()),('g2', pp.Gaussian(order = 2, sigma = 1) )])),\n",
    "        ('baseline*haar', Pipeline([('_sg1',pp.StandardNormalVariate()),('_sg2',pp.Wavelet('haar'))])),\n",
    "        ('savgol*gaussian1', Pipeline([('_sg1',pp.SavitzkyGolay()),('g2', pp.Gaussian(order = 1, sigma = 2) )])),\n",
    "        ('savgol*gaussian2', Pipeline([('_sg1',pp.SavitzkyGolay()),('g2', pp.Gaussian(order = 2, sigma = 1) )])),\n",
    "        ('gaussian2*savgol', Pipeline([('_g2',pp.Gaussian(order = 1, sigma = 2)),('_sg4',pp.SavitzkyGolay())])),\n",
    "        ('haar*gaussian2', Pipeline([('_haar2',pp.Wavelet('haar')), ('g2', pp.Gaussian(order = 2, sigma = 1)) ])),\n",
    "    ]\n",
    "    return preprocessing\n",
    "\n",
    "\n",
    "def PLS(estim_type, X_train, y_train, X_test, y_test):\n",
    "    return (PLSRegression(nc, max_iter=800), FeatureUnion, 'PLS_'+str(nc)+'nc')\n",
    "\n",
    "\n",
    "\n",
    "ALL_CONF_DATA = [data_raw_config, data_cv_config]#, data_augmented_config]\n",
    "ALL_CONF_PREPROCESSINGS = [Id, savgol, haar, bacon_set, decon_set]\n",
    "\n",
    "def preprocess_data(pp_generator, X_train, y_train, X_test, y_test, X_valid, y_valid):\n",
    "    y_scaler = MinMaxScaler()\n",
    "    y_train_s = y_scaler.fit_transform(y_train.reshape((-1,1)))\n",
    "    y_test_s = y_scaler.transform(y_test.reshape((-1,1)))\n",
    "    y_valid_s = y_scaler.transform(y_valid.reshape((-1,1)))\n",
    "\n",
    "    preprocessing = pp_generator()\n",
    "    \n",
    "    transformer_pipeline_aug = Pipeline([\n",
    "        ('scaler', MinMaxScaler()), \n",
    "        ('preprocessing', FeatureAugmentation(preprocessing)), \n",
    "    ])\n",
    "    X_train_s = transformer_pipeline_aug.fit_transform(X_train)\n",
    "    X_test_s = transformer_pipeline_aug.transform(X_test)\n",
    "    X_valid_s = transformer_pipeline_aug.transform(X_valid)\n",
    "    augmentation = (X_train_s, y_train_s, X_test_s, y_test_s, X_valid_s, y_valid_s, transformer_pipeline_aug)\n",
    "\n",
    "    transformer_pipeline_union = Pipeline([\n",
    "        ('scaler', MinMaxScaler()), \n",
    "        ('preprocessing', FeatureUnion(preprocessing)), \n",
    "    ])\n",
    "    X_train_s = transformer_pipeline_union.fit_transform(X_train)\n",
    "    X_test_s = transformer_pipeline_union.transform(X_test)\n",
    "    X_valid_s = transformer_pipeline_union.transform(X_valid)\n",
    "    union = (X_train_s, y_train_s, X_test_s, y_test_s, X_valid_s, y_valid_s, transformer_pipeline_union)\n",
    "\n",
    "    return union, augmentation, y_scaler\n",
    "\n",
    "\n",
    "def AUGMENTATION_MODELS():\n",
    "    m = {\n",
    "        'Bacon':(models.bacon, FeatureAugmentation),\n",
    "        'Bacon_vg': (models.bacon_vg, FeatureAugmentation),\n",
    "        'BigBacon_vg': (models.bacon_vg_big, FeatureAugmentation),\n",
    "        'Xception1D': (models.xception, FeatureAugmentation),\n",
    "        'Decon': (models.decon, FeatureAugmentation),\n",
    "    }\n",
    "    return m\n",
    "\n",
    "def UNION_MODELS():\n",
    "    m = {\n",
    "        'XGBoost_50_10': (XGBRegressor(n_estimators=50, max_depth=10), FeatureUnion),\n",
    "        'XGBoost_20_7': (XGBRegressor(n_estimators=20, max_depth=7), FeatureUnion),\n",
    "    }\n",
    "    for n in [3,5,7,10,15,20,40,70,100]:\n",
    "        m['PLS_'+str(n)+'nc'] = (PLSRegression(n, max_iter=800), FeatureUnion)\n",
    "        m['PLS_'+str(n)+'nc'] = (lwpls(n, max_iter=800), FeatureUnion)\n",
    "    return m\n",
    "\n",
    "\n",
    "class Auto_Save(Callback):\n",
    "    best_weights = []\n",
    "    def __init__(self):\n",
    "        super(Auto_Save, self).__init__()\n",
    "        self.best = np.Inf\n",
    "                \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        current_loss = logs.get('val_loss')\n",
    "        if np.less(current_loss, self.best):\n",
    "            self.best = current_loss            \n",
    "            Auto_Save.best_weights = self.model.get_weights()\n",
    "            print(\"Best so far >\", self.best)\n",
    "            \n",
    "    def on_train_end(self, logs=None):\n",
    "        if self.params['verbose'] == 2:\n",
    "            print('\\nSaved best {0:6.4f}\\n'.format(self.best))\n",
    "\n",
    "from tensorflow.keras.callbacks import Callback, EarlyStopping, LearningRateScheduler\n",
    "import math\n",
    "import datetime\n",
    "\n",
    "def train_nn(name, model, X_train, y_train, X_test, y_test):\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=400, verbose=0, mode='min') \n",
    "    log_dir = \"logs/fit/\"+name+\"/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "    cycle_params = {\n",
    "        'MIN_LR': 1e-5,\n",
    "        'MAX_LR': 1e-2,\n",
    "        'CYCLE_LENGTH': 256    \n",
    "    }\n",
    "\n",
    "\n",
    "    def scale_fn(x):\n",
    "        return 1 / (2.0 ** (x - 1))\n",
    "        # return 1. ** x\n",
    "        \n",
    "    MIN_LR, MAX_LR, CYCLE_LENGTH = cycle_params['MIN_LR'], cycle_params['MAX_LR'], cycle_params['CYCLE_LENGTH']\n",
    "    def clr(epoch, lr):\n",
    "        initial_learning_rate = MIN_LR\n",
    "        maximal_learning_rate = MAX_LR\n",
    "        step_size = CYCLE_LENGTH\n",
    "        step_as_dtype = float(epoch)\n",
    "        cycle = math.floor(1 + step_as_dtype / (2 * step_size))\n",
    "        x = abs(step_as_dtype / step_size - 2 * cycle + 1)\n",
    "        mode_step = cycle # if scale_mode == \"cycle\" else step\n",
    "        return initial_learning_rate + ( maximal_learning_rate - initial_learning_rate) * max(0, (1 - x)) * scale_fn(mode_step)\n",
    "\n",
    "   \n",
    "    lrScheduler = LearningRateScheduler(clr)\n",
    "    \n",
    "    model.fit(X_train, y_train,\n",
    "                epochs=15000, \n",
    "                batch_size=1500, \n",
    "                shuffle=True, \n",
    "                verbose=1,\n",
    "                validation_data = (X_test, y_test),\n",
    "                callbacks=[tensorboard_callback, early_stop, lrScheduler])\n",
    "    \n",
    "    model.set_weights(Auto_Save.best_weights)\n",
    "\n",
    "    Y_preds = model.predict(X_test)\n",
    "    RMSE = math.sqrt(mean_squared_error(y_test, Y_preds))\n",
    "    print(\"MAE\", mean_absolute_error(y_test, Y_preds))\n",
    "    print(\"MSE\", mean_squared_error(y_test, Y_preds))\n",
    "    print(\"RMSE\", RMSE)\n",
    "    print(\"MAPE\", mean_absolute_percentage_error(y_test, Y_preds))\n",
    "    print(\"R²\", r2_score(y_test, Y_preds))\n",
    "    \n",
    "    return y_test, Y_preds, RMSE, estimator.regressor_[1].model_\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def benchmark_dataset(path, estim_type=\"regression\"):\n",
    "    # Load data\n",
    "    projdir = Path(path)\n",
    "    files = tuple(next(projdir.glob(n)) for n in [\"*Xcal*\",\"*Ycal*\",\"*Xval*\",\"*Yval*\"])\n",
    "    XCal, yCal = load_csv(files[0], files[1], x_hdr=0, y_hdr=0, sep=';')\n",
    "    XVal, yVal = load_csv(files[2], files[3], x_hdr=0, y_hdr=0, sep=';')\n",
    "    print(\"Data loaded\", XCal.shape, yCal.shape, XVal.shape, yVal.shape)\n",
    "\n",
    "    ### RAW DATA\n",
    "    datasets, (X_valid, y_valid) = data_raw_config(XCal, yCal, XVal, yVal)\n",
    "    X_train, y_train, X_test, y_test = datasets[0]\n",
    "    for pp_generator in ALL_CONF_PREPROCESSINGS:\n",
    "        union, augmentation, y_scaler = preprocess_data(pp_generator, X_train, y_train, X_test, y_test, X_valid, y_valid)\n",
    "        for model_func in AUGMENTATION_MODELS:\n",
    "            model = model_func()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # for conf_data in ALL_CONF_DATA:\n",
    "    #     datasets, (X_valid, y_valid) = conf_data(XCal, yCal, XVal, yVal)\n",
    "    #     for i, dataset in enumerate(datasets):\n",
    "    #         X_train, y_train, X_test, y_test = dataset\n",
    "    #         for pp_generator in ALL_CONF_PREPROCESSINGS:\n",
    "    #             preprocessings = pp_generator()\n",
    "    #             for model_generator in ALL_CONF_MODELS:\n",
    "    #                 models = model_generator(estim_type, X_train, y_train, X_test, y_test)\n",
    "    #                 for model, feature_concatenation, name in models:\n",
    "    #                     pipeline = Pipeline([\n",
    "    #                         ('scaler', MinMaxScaler()), \n",
    "    #                         ('preprocessing', feature_concatenation(preprocessings)),\n",
    "    #                         ('model', model)\n",
    "    #                     ])\n",
    "    #                     estimator = TransformedTargetRegressor(regressor = pipeline, transformer = MinMaxScaler())\n",
    "    #                     estimator.fit(X_train, y_train, n_jobs=14)\n",
    "                        \n",
    "    #                     # Evaluate and save results\n",
    "                    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "benchmark_dataset('data/classif/Arabidopsis_Genotype10_250_CEFE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import cuda \n",
    "device = cuda.get_current_device()\n",
    "device.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scikeras Keras regressor integration\n",
    "\n",
    "The integration of KerasRegressor in pipeline is limited. To use full keras capabilities, you may need to modify the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard loading and preprocessing code\n",
    "\n",
    "from pinard import utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from pinard import preprocessing as pp\n",
    "from sklearn.pipeline import Pipeline\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Concatenate,GlobalAveragePooling1D, Conv1D, Activation, SpatialDropout1D,SeparableConv1D,Flatten, Dropout, Input, MaxPooling1D, DepthwiseConv1D, BatchNormalization, AveragePooling1D\n",
    "from typing import Dict, Iterable, Any\n",
    "from tensorflow.keras.optimizers import Adam, Adadelta, SGD\n",
    "import models\n",
    "from tensorflow_addons.optimizers import CyclicalLearningRate, LazyAdam\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error, r2_score\n",
    "import datetime\n",
    "from pinard.sklearn import FeatureAugmentation\n",
    "\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "# tf.keras.wrappers.scikit_learn.KerasRegressor\n",
    "\n",
    "import math\n",
    "\n",
    "\n",
    "# Init basic random\n",
    "rd_seed = 45678\n",
    "np.random.seed(rd_seed)\n",
    "tf.random.set_seed(rd_seed)\n",
    "random.seed(rd_seed)\n",
    "\n",
    "preprocessing = [   ('id', pp.IdentityTransformer()),\n",
    "                    ('baseline', pp.StandardNormalVariate()),\n",
    "                    ('savgol', pp.SavitzkyGolay()),\n",
    "                    ('gaussian1', pp.Gaussian(order = 1, sigma = 2)),\n",
    "                    ('gaussian2', pp.Gaussian(order = 2, sigma = 1)),\n",
    "                    ('haar', pp.Wavelet('haar')),\n",
    "                    ('coif3', pp.Wavelet('coif3')),\n",
    "                    ('detrend', pp.Detrend()),\n",
    "                    ('msc', pp.MultiplicativeScatterCorrection(scale=False)),\n",
    "                    ('dv1', pp.Derivate(1,1)),\n",
    "                    ('dv2', pp.Derivate(2,1)),\n",
    "                    ('dv3', pp.Derivate(2,2)),\n",
    "                    \n",
    "                    ('baseline*savgol', Pipeline([('_sg1',pp.StandardNormalVariate()),('_sg2',pp.SavitzkyGolay())])),\n",
    "                    ('baseline*gaussian1', Pipeline([('_sg1',pp.StandardNormalVariate()),('g2', pp.Gaussian(order = 1, sigma = 2) )])),\n",
    "                    ('baseline*gaussian2', Pipeline([('_sg1',pp.StandardNormalVariate()),('g2', pp.Gaussian(order = 2, sigma = 1) )])),\n",
    "                    ('baseline*haar', Pipeline([('_sg1',pp.StandardNormalVariate()),('_sg2',pp.Wavelet('haar'))])),\n",
    "                    \n",
    "                    # ('savgol*savgol', Pipeline([('_sg1',pp.SavitzkyGolay()),('_sg2',pp.SavitzkyGolay())])),\n",
    "                    # ('savgol*baseline', Pipeline([('_sg1',pp.SavitzkyGolay()),('_sg2',pp.StandardNormalVariate())])),\n",
    "                    ('savgol*gaussian1', Pipeline([('_sg1',pp.SavitzkyGolay()),('g2', pp.Gaussian(order = 1, sigma = 2) )])),\n",
    "                    ('savgol*gaussian2', Pipeline([('_sg1',pp.SavitzkyGolay()),('g2', pp.Gaussian(order = 2, sigma = 1) )])),\n",
    "                    # ('savgol*haar', Pipeline([('_sg1',pp.SavitzkyGolay()), ('haar', pp.Wavelet('haar'))])),\n",
    "                    # ('gaussian1*savgol', Pipeline([('_g1',pp.Gaussian(order = 1, sigma = 2)),('_sg3',pp.SavitzkyGolay())])),\n",
    "                    ('gaussian2*savgol', Pipeline([('_g2',pp.Gaussian(order = 1, sigma = 2)),('_sg4',pp.SavitzkyGolay())])),\n",
    "                    # ('haar*savgol', Pipeline([('_haar2',pp.Wavelet('haar')),('_sg5',pp.SavitzkyGolay())])),\n",
    "                    # ('haar*gaussian1', Pipeline([('_haar2',pp.Wavelet('haar')), ('g2', pp.Gaussian(order = 1, sigma = 2)) ])),\n",
    "                    ('haar*gaussian2', Pipeline([('_haar2',pp.Wavelet('haar')), ('g2', pp.Gaussian(order = 2, sigma = 1)) ])),\n",
    "                ]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# custom Keras Callback that store \n",
    "class Auto_Save(Callback):\n",
    "    best_weights = []\n",
    "    def __init__(self):\n",
    "        super(Auto_Save, self).__init__()\n",
    "        self.best = np.Inf\n",
    "                \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        current_loss = logs.get('val_loss')\n",
    "        if np.less(current_loss, self.best):\n",
    "            self.best = current_loss            \n",
    "            Auto_Save.best_weights = self.model.get_weights()\n",
    "            print(\"Best so far >\", self.best)\n",
    "            \n",
    "    def on_train_end(self, logs=None):\n",
    "        if self.params['verbose'] == 2:\n",
    "            print('\\nSaved best {0:6.4f}\\n'.format(self.best))\n",
    "\n",
    "\n",
    "class Print_LR(Callback):    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        iteration = self.model.optimizer.iterations.numpy()\n",
    "        # lr = clr(iteration).numpy()\n",
    "        lr = self.model.optimizer.learning_rate\n",
    "        if self.params['verbose'] == 2:\n",
    "            print(\"Iteration {} - Learning rate: {}\".format(iteration, lr) )\n",
    "\n",
    "# def cyclic_optimizer(X_train, BATCH_SIZE, p):\n",
    "#     MIN_LR, MAX_LR, CYCLE_LENGTH = p['MIN_LR'], p['MAX_LR'], p['CYCLE_LENGTH']\n",
    "#     steps_per_epoch = len(X_train) // BATCH_SIZE\n",
    "\n",
    "#     clr = CyclicalLearningRate(\n",
    "#         initial_learning_rate=MIN_LR,\n",
    "#         maximal_learning_rate=MAX_LR,\n",
    "#         scale_fn=lambda x: 1/(2.**(x-1)),\n",
    "#         step_size= CYCLE_LENGTH * steps_per_epoch\n",
    "#     )\n",
    "#     return Adam(clr)\n",
    "\n",
    "\n",
    "\n",
    "###DECON\n",
    "def keras_model(meta):\n",
    "    input_shape = meta[\"X_shape_\"][1:]\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=input_shape))\n",
    "    model.add(SpatialDropout1D(0.2))\n",
    "    model.add(DepthwiseConv1D(kernel_size=7, padding=\"same\",depth_multiplier=2, activation='relu'))\n",
    "    model.add(DepthwiseConv1D(kernel_size=7, padding=\"same\",depth_multiplier=2, activation='relu'))\n",
    "    model.add(AveragePooling1D(pool_size=2,strides=2))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(DepthwiseConv1D(kernel_size=5, padding=\"same\",depth_multiplier=2, activation='relu'))\n",
    "    model.add(DepthwiseConv1D(kernel_size=5, padding=\"same\",depth_multiplier=2, activation='relu'))\n",
    "    model.add(AveragePooling1D(pool_size=2,strides=2))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(DepthwiseConv1D(kernel_size=9, padding=\"same\",depth_multiplier=2, activation='relu'))\n",
    "    model.add(DepthwiseConv1D(kernel_size=9, padding=\"same\",depth_multiplier=2, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2,strides=2))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    model.add(SeparableConv1D(128, kernel_size=5, depth_multiplier=1, padding=\"same\", activation='relu'))\n",
    "    model.add(Conv1D(filters=32, kernel_size=3, padding=\"same\"))\n",
    "    model.add(MaxPooling1D(pool_size=5,strides=3))\n",
    "    model.add(SpatialDropout1D(0.2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(units=128, activation=\"sigmoid\"))\n",
    "    model.add(Dense(units=32, activation=\"sigmoid\"))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(units=1, activation=\"sigmoid\"))\n",
    "    # # optimizer = Adadelta(learning_rate=1.0, rho=0.95, epsilon=1e-07)\n",
    "    # model.compile(loss = 'mean_squared_error', metrics=['mse'], optimizer = optimizer)\n",
    "    # model.compile(loss = 'mean_squared_error', metrics=['mse'], optimizer = \"adam\")\n",
    "    # model.summary()\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_model(Xcal, Ycal, Xval, Yval, BATCH_SIZE, EPOCH, cycle_params):  \n",
    "    X_train, y_train = utils.load(Xcal, Ycal, y_cols=1)\n",
    "    X_test, y_test = utils.load(Xval, Yval, y_cols=1)\n",
    "    \n",
    "    y_scaler = MinMaxScaler()\n",
    "    y_scaler.fit(y_train.reshape((-1,1)))\n",
    "    y_valid = y_scaler.transform(y_test.reshape((-1,1)))\n",
    "  \n",
    "    transformer_pipeline = Pipeline([\n",
    "        ('scaler', MinMaxScaler()), \n",
    "        ('preprocessing', FeatureAugmentation(preprocessing)), \n",
    "    ])\n",
    "    \n",
    "    transformer_pipeline.fit(X_train)\n",
    "    X_valid = transformer_pipeline.transform(X_test)\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=400, verbose=0, mode='min') \n",
    "    log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "    # reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.75, patience=100, verbose=1, min_delta=0.5e-5, mode='min')\n",
    "\n",
    "    def scale_fn(x):\n",
    "        return 1 / (2.0 ** (x - 1))\n",
    "        # return 1. ** x\n",
    "        \n",
    "    MIN_LR, MAX_LR, CYCLE_LENGTH = cycle_params['MIN_LR'], cycle_params['MAX_LR'], cycle_params['CYCLE_LENGTH']\n",
    "    def clr(epoch, lr):\n",
    "        initial_learning_rate = MIN_LR\n",
    "        maximal_learning_rate = MAX_LR\n",
    "        step_size = CYCLE_LENGTH\n",
    "        step_as_dtype = float(epoch)\n",
    "        cycle = math.floor(1 + step_as_dtype / (2 * step_size))\n",
    "        x = abs(step_as_dtype / step_size - 2 * cycle + 1)\n",
    "        mode_step = cycle # if scale_mode == \"cycle\" else step\n",
    "        return initial_learning_rate + ( maximal_learning_rate - initial_learning_rate) * max(0, (1 - x)) * scale_fn(mode_step)\n",
    "\n",
    "    # clr = CyclicalLearningRate(\n",
    "    #     initial_learning_rate=MIN_LR,\n",
    "    #     maximal_learning_rate=MAX_LR,\n",
    "    #     scale_fn=scale_fn,\n",
    "    #     step_size= CYCLE_LENGTH * steps_per_epoch\n",
    "    # )\n",
    "    \n",
    "    # kerasModel = keras_model((len(X_train),len(preprocessing)))\n",
    "    # # print(\"---\", X_train.shape)\n",
    "    # kerasModel.compile(loss = 'mean_squared_error', metrics=['mse'], optimizer = LazyAdam(0.001))\n",
    "    \n",
    "    lrScheduler = tf.keras.callbacks.LearningRateScheduler(clr)\n",
    "    \n",
    "    k_regressor = KerasRegressor(model = keras_model,\n",
    "                                loss = 'mean_squared_error', metrics=['mse'], optimizer = \"adam\",\n",
    "                                callbacks=[Auto_Save(), early_stop, lrScheduler, tensorboard_callback],\n",
    "                                epochs=EPOCH,\n",
    "                                batch_size=BATCH_SIZE,\n",
    "                                # scale_fn=scale_fn,\n",
    "                                fit__validation_data = (X_valid, y_valid),\n",
    "                                verbose = 2)\n",
    "\n",
    "    # estimation pipeline\n",
    "    pipeline = Pipeline([\n",
    "        ('trans', transformer_pipeline), \n",
    "        ('KerasNN', k_regressor)\n",
    "    ])\n",
    "\n",
    "    estimator = TransformedTargetRegressor(regressor = pipeline, transformer = y_scaler)\n",
    "    estimator.fit(X_train, y_train)\n",
    "\n",
    "    estimator.regressor_[1].model_.set_weights(Auto_Save.best_weights)\n",
    "\n",
    "    Y_preds = estimator.predict(X_test)\n",
    "    RMSE = math.sqrt(mean_squared_error(y_test, Y_preds))\n",
    "    print(\"MAE\", mean_absolute_error(y_test, Y_preds))\n",
    "    print(\"MSE\", mean_squared_error(y_test, Y_preds))\n",
    "    print(\"RMSE\", RMSE)\n",
    "    print(\"MAPE\", mean_absolute_percentage_error(y_test, Y_preds))\n",
    "    print(\"R²\", r2_score(y_test, Y_preds))\n",
    "    \n",
    "    return y_test, Y_preds, RMSE, estimator.regressor_[1].model_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import glob\n",
    "rootdir = Path('data')\n",
    "file_list = [f for f in rootdir.glob('**/*') if f.is_dir()]\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "EPOCHS = 10000\n",
    "\n",
    "cycle_params = {\n",
    "    'MIN_LR': 1e-5,\n",
    "    'MAX_LR': 1e-2,\n",
    "    'CYCLE_LENGTH': 256    \n",
    "}\n",
    "\n",
    "for dir in file_list:\n",
    "    training_name = str(dir)\n",
    "    projdir = Path(dir)\n",
    "    Xcal = next(projdir.glob(\"*Xcal*\"))\n",
    "    Ycal = next(projdir.glob(\"*Ycal*\"))\n",
    "    Xval = next(projdir.glob(\"*Xval*\"))\n",
    "    Yval = next(projdir.glob(\"*Yval*\"))\n",
    "    \n",
    "    print(\"*\"*15)\n",
    "    print(str(dir))\n",
    "    print(\"-\"*15)    \n",
    "    \n",
    "    y_test, Y_preds, RMSE, model = train_model(Xcal, Ycal, Xval, Yval, BATCH_SIZE, EPOCHS, cycle_params)\n",
    "\n",
    "    np.savetxt(training_name + 'res'+str(RMSE)+'.csv', np.column_stack((y_test,Y_preds)))\n",
    "    model.save(training_name + '_model')\n",
    "    \n",
    "    np.savetxt(training_name +'/res'+str(RMSE)+'.csv', np.column_stack((y_test,Y_preds)))\n",
    "    model.save(training_name + '/model')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.9 ('pynirsENV')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7b09f6e5407ec4329146609a0cb08cbbe4720f97bb26598a93c421b663bd10d2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
