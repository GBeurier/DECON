{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'root - 2023-06-02 17:16:16,135 [INFO] Processing dataset ('data/_Raisin/Raisin_Tavernier_830_GFratio', [1, 2, 3])\n",
      "'root - 2023-06-02 17:16:16,136 [INFO] Registering dataset: ('data/_Raisin/Raisin_Tavernier_830_GFratio', [1, 2, 3])\n",
      "'root - 2023-06-02 17:16:16,138 [INFO] Loading file: data\\_Raisin\\Raisin_Tavernier_830_GFratio\\Xcal.csv\n",
      "'root - 2023-06-02 17:16:16,142 [INFO] Header inference: False\n",
      "'root - 2023-06-02 17:16:16,143 [INFO] Delimiter inference: ;\n",
      "'root - 2023-06-02 17:16:16,325 [INFO] Data shape: (664, 125)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'root - 2023-06-02 17:16:16,330 [INFO] Data shape after dropna(all) cols: (664, 125)\n",
      "'root - 2023-06-02 17:16:16,335 [INFO] Data shape after dropna(all) rows: (664, 125)\n",
      "'root - 2023-06-02 17:16:16,337 [INFO] Rows to remove after filtering: [432, 434, 428, 438]\n",
      "'root - 2023-06-02 17:16:16,338 [INFO] data_sample:\n",
      "[[0.2384144 0.2335801]\n",
      " [0.248429  0.2475333]]\n",
      "'root - 2023-06-02 17:16:16,341 [INFO] Loading file: data\\_Raisin\\Raisin_Tavernier_830_GFratio\\Xval.csv\n",
      "'root - 2023-06-02 17:16:16,344 [INFO] Header inference: False\n",
      "'root - 2023-06-02 17:16:16,345 [INFO] Delimiter inference: ;\n",
      "'root - 2023-06-02 17:16:16,410 [INFO] Data shape: (166, 125)\n",
      "'root - 2023-06-02 17:16:16,414 [INFO] Data shape after dropna(all) cols: (166, 125)\n",
      "'root - 2023-06-02 17:16:16,418 [INFO] Data shape after dropna(all) rows: (166, 125)\n",
      "'root - 2023-06-02 17:16:16,420 [INFO] data_sample:\n",
      "[[0.2509339 0.2475901]\n",
      " [0.2625283 0.2588682]]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Cannot initialize y columns ([1, 2, 3]). More than one X_train file found for Raisin_Tavernier_830_GFratio.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_20252\\1879677933.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_config\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"end\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_20252\\1879677933.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(config)\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Processing dataset {dataset_path}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m         \u001b[1;31m# 1. Load data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m         \u001b[0mdataset_uid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatacache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregister_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m         \u001b[1;31m# 2 .Filter data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Workspace\\ML\\DECON\\core\\datacache.py\u001b[0m in \u001b[0;36mregister_dataset\u001b[1;34m(dataset_config)\u001b[0m\n\u001b[0;32m    172\u001b[0m     \u001b[1;31m# Get y from a col of X\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset_config\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 174\u001b[1;33m         \u001b[1;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"X_train\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Cannot initialize y columns (%s). More than one X_train file found for %s.\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0my_cols\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    175\u001b[0m         \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Getting y cols %s from X for %s.\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_cols\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    176\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0my_key\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpattern\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_files\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: Cannot initialize y columns ([1, 2, 3]). More than one X_train file found for Raisin_Tavernier_830_GFratio."
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import json\n",
    "from itertools import product\n",
    "import logging\n",
    "\n",
    "import core.datacache as datacache\n",
    "import core.filters as filters\n",
    "\n",
    "# import pipeliner\n",
    "# import indexer\n",
    "# from core.trainingjob import TrainingJob\n",
    "# import core.indexer as indexer\n",
    "\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"'%(name)s - %(asctime)s [%(levelname)s] %(message)s\",\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"debug.log\"),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "def generate_combinations(steps):\n",
    "    methods = []\n",
    "    for step in steps.values():\n",
    "        methods.append([(m, step[\"type\"]) for m in step['method']])\n",
    "    return list(product(*methods))\n",
    "\n",
    "\n",
    "# TRAINING\n",
    "training_config = {\n",
    "    \"seed\": [42],  # int\n",
    "    \"random_scope\": \"dataset\",  # str - [\"dataset\", \"global\", \"models\"]\n",
    "    # List[str] - [path1, path2, ...]\n",
    "    \"paths\": [(\"data/test_data\", [1,2,3]), \"data/_Raisin/Raisin_Tavernier_830_GFratio\", \"data/_RefSet/ALPINE_C_424_Murguzur_RMSE1.16\"],\n",
    "    # List[(Splitter, Dict)]  - [splitter, params] -> to create dataset indexes trees\n",
    "    \"indexation\": [],\n",
    "    \"pre_indexation\": {\n",
    "        \"step_1\": {\n",
    "            \"type\": \"filter\",\n",
    "            \"method\": [(\"crop\", filters.Crop, {\"start\":100, \"end\":500}), (\"crop\", filters.Crop, {\"start\":0, \"end\":1000}), None],\n",
    "        },\n",
    "        \"step_2\": {\n",
    "            \"type\": \"filter\",\n",
    "            \"method\": [(\"resample\", filters.Uniform_FT_Resample, {\"resample_size\": 800}), None],\n",
    "        }\n",
    "    },\n",
    "    \"post_indexation\": {\n",
    "        \"step_1\": {\n",
    "            \"type\": \"augmentation\",\n",
    "            \"method\": [],  # List[List[(TransformerMixin, Dict)]]\n",
    "        },\n",
    "        \"step_2\": {\n",
    "            \"type\": \"preprocessing\",\n",
    "            \"method\": [],  # List[List[(TransformerMixin, Dict)]]\n",
    "        },\n",
    "    },\n",
    "    \"models\": [\n",
    "        # List[(Estimator, Dict)]  - [estimator, params] -> to create sklearn pipeline\n",
    "        [],\n",
    "    ],\n",
    "}\n",
    "\n",
    "# def\n",
    "\n",
    "\n",
    "def train(config):\n",
    "    for dataset_path in config[\"paths\"]:\n",
    "        logging.info(f\"Processing dataset {dataset_path}\")\n",
    "        # 1. Load data\n",
    "        dataset_uid, dataset_name, data = datacache.register_dataset(dataset_path)\n",
    "        \n",
    "        # 2 .Filter data\n",
    "        pre_indexation_steps = generate_combinations(config[\"pre_indexation\"])\n",
    "        logging.info(pre_indexation_steps)\n",
    "        \n",
    "        # 3. Indexation\n",
    "        # if not pre_indexation_steps:\n",
    "        #     pre_indexation_steps = [None]\n",
    "\n",
    "        # for pre_indexation_step in pre_indexation_steps:\n",
    "        #     indexations = indexer.get(dataset, config[\"indexation\"], pre_indexation_step)\n",
    "        #     post_indexation_steps = generate_combinations(config[\"post_indexation\"])\n",
    "        #     runs = generate_combinations(indexations, post_indexation_steps, config[\"models\"])\n",
    "        #     for run in runs:\n",
    "        #         scheduler.add(run)\n",
    "\n",
    "\n",
    "train(training_config)\n",
    "print(\"end\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import core.datacache as datacache\n",
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"'%(name)s - %(asctime)s [%(levelname)s] %(message)s\",\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"debug.log\"),\n",
    "        # logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "\n",
    "# data = datacache.load_csv(\"data/_RefSet/ALPINE_C_424_Murguzur_RMSE1.16/XCal.csv.gz\")\n",
    "# cache_hash, dataset_name, cache = datacache.register_dataset(\"data/test_data\")\n",
    "# print(cache_hash, dataset_name)\n",
    "# for k, v in cache.items():\n",
    "#     if isinstance(v, np.ndarray):\n",
    "#         print(k, v.shape)\n",
    "\n",
    "cache_hash, dataset_name, cache = datacache.register_dataset(\"data/test_data_2\")\n",
    "print(cache_hash, dataset_name)\n",
    "for k, v in cache.items():\n",
    "    if isinstance(v, np.ndarray):\n",
    "        print(k, v.shape)\n",
    "\n",
    "\n",
    "# data.isna().any(axis=0)\n",
    "\n",
    "# hex(hash(str(data)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def remove_nan_rows(X, y):\n",
    "    nan_indexes_X = np.isnan(X).any(axis=1)\n",
    "    nan_indexes_y = np.isnan(y).any(axis=0)\n",
    "\n",
    "    removed_indexes_X = np.where(nan_indexes_X)[0]\n",
    "    removed_indexes_y = np.where(nan_indexes_y)[0]\n",
    "    \n",
    "\n",
    "    matching_indexes_X = np.delete(np.arange(len(X)), removed_indexes_X)\n",
    "    matching_indexes_y = np.delete(np.arange(len(y)), removed_indexes_y)\n",
    "\n",
    "    X_matched = X[matching_indexes_X]\n",
    "    y_matched = y[matching_indexes_y]\n",
    "\n",
    "    return X_matched, y_matched\n",
    "\n",
    "\n",
    "# Example usage\n",
    "# Assuming you have X_train and y_train as the original arrays\n",
    "\n",
    "# Generate example arrays with NaN values\n",
    "X_train = np.array([[1, 2, np.nan], [4, 5, 6], [np.nan, 8, 9]])\n",
    "y_train = np.array([10, 11, np.nan])\n",
    "\n",
    "# Remove rows with NaN values and get matching arrays\n",
    "X_matched, y_matched = remove_nan_rows(X_train, y_train)\n",
    "\n",
    "print(\"X_matched:\\n\", X_matched)\n",
    "print(\"y_matched:\\n\", y_matched)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DECON_ENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
