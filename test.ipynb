{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'root - 2023-06-03 01:58:28,032 [WARNING] Unable to init test_data y_test, no X_test data found.\n",
      "'root - 2023-06-03 01:58:28,033 [WARNING] Removing rows: [432 434 428 438] in test_data X_train\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'core.datacache' has no attribute 'get'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_58872/516841846.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m \u001b[0mtrain_pool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_config\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"*\"\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"  Training done  \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"*\"\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_58872/516841846.py\u001b[0m in \u001b[0;36mtrain_pool\u001b[1;34m(config)\u001b[0m\n\u001b[0;32m     97\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mpre_indexation_step\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpre_indexation_steps\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m             \u001b[1;31m# 3. Indexation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m             \u001b[0mindexations\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"indexation\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpre_indexation_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset_uid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    100\u001b[0m             \u001b[0mpost_indexation_steps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerate_steps_combinations\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"post_indexation\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m             \u001b[0mruns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproduct\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpre_indexation_step\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexations\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpost_indexation_steps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"models\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Workspace\\ML\\DECON\\core\\indexer.py\u001b[0m in \u001b[0;36mindex\u001b[1;34m(configs, pre_indexation_step, dataset_uid, dataset_name)\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mpreprocessed_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_data_dependent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m             \u001b[0mpreprocessed_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatacache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset_uid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpre_indexation_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mindexes_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindexes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreprocessed_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'core.datacache' has no attribute 'get'"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import json\n",
    "from itertools import product\n",
    "import logging\n",
    "\n",
    "import core.datacache as datacache\n",
    "import core.filters as filters\n",
    "import core.indexer as indexer\n",
    "import preprocessings\n",
    "import pinard.preprocessing as pp\n",
    "from pinard import augmentation\n",
    "import regressors\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.WARNING,\n",
    "    format=\"'%(name)s - %(asctime)s [%(levelname)s] %(message)s\",\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"debug.log\"),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "\n",
    "def generate_steps_combinations(steps):\n",
    "    methods = []\n",
    "    for step in steps.values():\n",
    "        methods.append([(m, step[\"type\"]) for m in step['method']])\n",
    "    return list(product(*methods))\n",
    "\n",
    "\n",
    "# TRAINING\n",
    "training_config = {\n",
    "    \"seed\": [42],\n",
    "    \"random_scope\": \"dataset\", # TODO manage SEED\n",
    "    \"paths\": [(\"data/test_data\", [1,2,3]), \"data/_Raisin/Raisin_Tavernier_830_GFratio\", \"data/_RefSet/ALPINE_C_424_Murguzur_RMSE1.16\"],\n",
    "    \"pre_indexation\": {\n",
    "        \"step_1\": {\n",
    "            \"type\": \"filter\",\n",
    "            \"method\": [(\"crop\", filters.Crop, {\"start\":100, \"end\":500}), (\"crop\", filters.Crop, {\"start\":0, \"end\":1000})],# None],\n",
    "        },\n",
    "        \"step_2\": {\n",
    "            \"type\": \"filter\",\n",
    "            \"method\": [(\"resample\", filters.Uniform_FT_Resample, {\"resample_size\": 800})],# None],\n",
    "        }\n",
    "    },\n",
    "    \"indexation\": [\n",
    "        (\"random_split\", indexer.RandomSampling, {\"test_size\": 0.2}, {}),\n",
    "        (\"random_cv\", indexer.RandomSampling, {\"folds\": 4, \"repeat\": 1}, {}),\n",
    "        # None,\n",
    "        (\"random_cv\", indexer.SXPY, {\"folds\": 4, \"repeat\": 1}, {'metric':\"euclidean\", 'pca_components':250}),\n",
    "    ],\n",
    "    \"post_indexation\": {\n",
    "        \"step_1\": {\n",
    "            \"type\": \"augmentation\",\n",
    "            \"method\": [\n",
    "                # None,\n",
    "                [(6, augmentation.Rotate_Translate())],\n",
    "                [(3, augmentation.Rotate_Translate()),(2, augmentation.Random_X_Operation()),(1, augmentation.Random_Spline_Addition()),],\n",
    "                [(3, augmentation.Rotate_Translate()),(2, augmentation.Random_X_Operation()),(2, augmentation.Random_Spline_Addition()),]\n",
    "            ],\n",
    "        },\n",
    "        \"step_2\": {\n",
    "            \"type\": \"preprocessing\",\n",
    "            \"method\": [\n",
    "                # None,\n",
    "                preprocessings.id_preprocessing(),\n",
    "                [(\"id\", pp.IdentityTransformer()), ('haar', pp.Haar()), ('savgol', pp.SavitzkyGolay())],\n",
    "                preprocessings.decon_set(),\n",
    "            ]\n",
    "        },\n",
    "    },\n",
    "    \"models\": [\n",
    "        # ()\n",
    "        (regressors.Transformer_NIRS, {'batch_size':500, 'epoch':10000, 'verbose':0, 'patience':1000, 'optimizer':'adam', 'loss':'mse'}),\n",
    "        (XGBRegressor, {\"n_estimators\":200, \"max_depth\":50}),\n",
    "        (PLSRegression, {\"n_components\":50}),\n",
    "        (regressors.Decon_SepPo, {'batch_size':50, 'epoch':10000, 'verbose':0, 'patience':1000, 'optimizer':'adam', 'loss':'mse'}),\n",
    "    ],\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "def train_pool(config):\n",
    "    for dataset_path in config[\"paths\"]:\n",
    "        logging.info(f\"Processing dataset {dataset_path}\")\n",
    "        # 1. Load data\n",
    "        dataset_uid, dataset_name = datacache.register_dataset(dataset_path)\n",
    "        \n",
    "        # 2 .Filter data\n",
    "        pre_indexation_steps = generate_steps_combinations(config[\"pre_indexation\"])\n",
    "        logging.info(f\"Pre-indexation steps: {pre_indexation_steps}\")\n",
    "        \n",
    "        for pre_indexation_step in pre_indexation_steps:\n",
    "            # 3. Indexation\n",
    "            indexations = indexer.index(config[\"indexation\"], pre_indexation_step, dataset_uid, dataset_name)\n",
    "            post_indexation_steps = generate_steps_combinations(config[\"post_indexation\"])\n",
    "            runs = list(product([pre_indexation_step], indexations, post_indexation_steps, config[\"models\"]))\n",
    "            print(len(runs))\n",
    "            print(\"\\n\".join([str(r) for r in runs[0:10]]))\n",
    "            # for run in runs:\n",
    "                # scheduler.add(run)\n",
    "\n",
    "\n",
    "train_pool(training_config)\n",
    "print(\"*\"*50, \"  Training done  \", \"*\"*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DECON_ENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
